{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fcaa9-8ddb-491d-947b-f8db76be5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bokeh\n",
    "import time\n",
    "\n",
    "# Dimension reduction algorithms\n",
    "#from cuml.manifold import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "# Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import export_png\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc2ccb-c2c0-42b4-8b7b-8060507c3809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_125_1'\n",
    "checkpoint = 140000\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cpu'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "             \n",
    "model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "###########################\n",
    "# Loading the Tokenizer\n",
    "###########################\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print('Loadede Tokenizer from serialized instance!')    \n",
    "print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading Descriptions\n",
    "###########################\n",
    "with open(f'{dump_dir}/themes_descriptions.pk', 'rb') as f:\n",
    "    theme_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/viewpoints_descriptions.pk', 'rb') as f:\n",
    "    viewpoint_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/numbers_descriptions.pk', 'rb') as f:\n",
    "    number_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/full_descriptions.pk', 'rb') as f:\n",
    "        full_descriptions_dict = pk.load(f)\n",
    "\n",
    "\n",
    "###########################\n",
    "# Extracting the Embeddings\n",
    "###########################\n",
    "\n",
    "# Extracting the classification Head weights\n",
    "out_emb = model.get_output_embeddings()\n",
    "out_emb = next(out_emb.parameters()).detach().numpy()[2:]\n",
    "\n",
    "# Extracting the matching F_terms for the weights and creating lists with the defintions\n",
    "tokens = [tokenizer.decode(i) for i in range(len(tokenizer))]\n",
    "f_term_tokens = tokens[50002:]\n",
    "\n",
    "# Creating  a dict with f-Terms and their embedding vectors:\n",
    "\n",
    "emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, out_emb)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236df9de",
   "metadata": {},
   "source": [
    "# Detecting pairs of f-terms with supposed similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"data/f-terms.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41930941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset with \"material\" in the viewpoint description\n",
    "df2=df[df.viewpoint_label.str.contains(\"material\",case=False, na=False)].copy()\n",
    "df2[\"vp\"]=df2.theme+\"/\"+df2.viewpoint\n",
    "df2[\"fterm\"]=df2.theme+\"/\"+df2.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c563498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f-term descriptions are searched for the following materials\n",
    "materials_list=[\"metal\", \"wood\", \"concrete\", \"resin\", \"fiber\", \"wool\", \"sand\", \"cord\", \"copper\", \"iron\", \"silver\", \"gold\", \"lead\", \"glass\", \"stone\", \"titanium\", \"steel\", \"cement\", \"silicon\", \"polymer\", \"ceramics\"]\n",
    "materials_f_terms={}\n",
    "for material in materials_list:\n",
    "    materials_f_terms[material]=[df2[df2.label.str.contains(material,case=False, na=False)].vp.unique(), df2[df2.label.str.contains(material,case=False, na=False)].fterm.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs of two materials with fterms in same viewpoints are created\n",
    "material_combinations={}\n",
    "for mat1 in materials_list:\n",
    "    for mat2 in materials_list:\n",
    "        \n",
    "        if mat1==mat2: continue\n",
    "        if mat2+\"_\"+mat1 in materials_f_terms: continue\n",
    "        \n",
    "        #filtering shared f-terms with both materials in the label description\n",
    "        \n",
    "        unique_fterms_mat1 = list(set(materials_f_terms[mat1][1]) - set(materials_f_terms[mat2][1]))\n",
    "        unique_fterms_mat2 = list(set(materials_f_terms[mat2][1]) - set(materials_f_terms[mat1][1]))\n",
    "\n",
    "\n",
    "        vp_mat1= set([x[:8]for x in unique_fterms_mat1])\n",
    "        vp_mat2= set([x[:8]for x in unique_fterms_mat2])\n",
    "        shared_vp=[x for x in vp_mat1 if x in vp_mat2]\n",
    "        fterm_pairs=[]\n",
    "        for vp in shared_vp:\n",
    "            fterm_pairs.append([vp,\n",
    "                                [fterm for fterm in unique_fterms_mat1 if fterm.startswith(vp)],\n",
    "                                [fterm for fterm in unique_fterms_mat2 if fterm.startswith(vp)]])\n",
    "            \n",
    "            \n",
    "        material_combinations[mat1+\"_\"+mat2]=fterm_pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating vectors for all material combinations\n",
    "\n",
    "\n",
    "combination_desc = []\n",
    "vector_diffs = []\n",
    "vps = []\n",
    "vp_desc = []\n",
    "numbers = []\n",
    "desc = []\n",
    "color_ints = []\n",
    "\n",
    "for i, l  in enumerate(material_combinations.items()):\n",
    "    key, item = l\n",
    "    for viewpoint_comb in item:\n",
    "        viewpoint, mat_1, mat_2 = viewpoint_comb\n",
    "    \n",
    "        for f_term_1 in mat_1:\n",
    "            for f_term_2 in mat_2:\n",
    "                try: \n",
    "                    vec_1 = emb_dict[f_term_1]\n",
    "                    vec_2 = emb_dict[f_term_2]\n",
    "                    vp_desc.append(viewpoint_dict[viewpoint])\n",
    "                    desc. append(number_dict[f_term_2] + ' - ' + number_dict[f_term_1])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                    \n",
    "                diff = vec_2 - vec_1\n",
    "                combination_desc.append(key)\n",
    "                vector_diffs.append(diff)\n",
    "                vps.append(viewpoint)\n",
    "                numbers.append(f_term_2 + ' - ' + f_term_1)\n",
    "                color_ints.append(i)\n",
    "        \n",
    "len(combination_desc), len(vp_desc), len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50612085",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_diffs = np.stack(vector_diffs, 0)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69) \n",
    "tsne_rep = tsne.fit_transform(vector_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c04a1-a3c0-42a8-b25c-33f4b11b0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_palette = bokeh.palettes.Turbo256\n",
    "color_palette = bokeh_palette\n",
    "\n",
    "colors = [color_palette[c%256] for c in color_ints]\n",
    "\n",
    "datasource_diff = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = tsne_rep[:,0],\n",
    "            y = tsne_rep[:,1],\n",
    "            combination =  combination_desc,\n",
    "            viewpoints=vps,\n",
    "            vp_desc = vp_desc,\n",
    "            numbers = numbers,\n",
    "            desc = desc,\n",
    "            colors = colors\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>Combination: </b>  @combination<br><b>Viewpoint:</b> @viewpoints<br><b>Viewpoint Description:</b> @vp_desc<br><b>Numbers:</b> @numbers<br><b>Description:</b> @desc</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Material Combintation Differences')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=8, fill_color='colors', \n",
    "                     alpha=0.7, line_width=0, source=datasource_diff, name=\"Material Combination Differences\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd384e0b-cb36-47dd-bcc6-b97151701847",
   "metadata": {},
   "source": [
    "# Comparing the Cosine Similarities of all Vector Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd35fec-582f-49f0-abac-a52092c2b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating the 1 on 1 differences for a single material combination\n",
    "combination = material_combinations['metal_glass']\n",
    "descriptions = []\n",
    "diffs = []\n",
    "\n",
    "for viewpoint, fterms1, fterms2 in combination:\n",
    "    for fterm1 in fterms1:\n",
    "        for fterm2 in fterms2:\n",
    "            try: \n",
    "                vec1 = emb_dict[fterm1]\n",
    "                vec2 = emb_dict[fterm2]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            diffs.append(vec2- vec1)\n",
    "            descriptions.append([viewpoint, fterm1, fterm2])\n",
    "\n",
    "descriptions = np.array(descriptions)\n",
    "diffs = torch.tensor(diffs)\n",
    "\n",
    "# Generating the cosine similarities\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim =1)\n",
    "\n",
    "diffs = torch.nn.functional.normalize(diffs, p=2, dim=1)\n",
    "diffs1 = diffs.unsqueeze(0)\n",
    "diffs2 = diffs.unsqueeze(1)\n",
    "desc1 = np.expand_dims(descriptions, 0)\n",
    "desc2 = np.expand_dims(descriptions, 1)\n",
    "diffs1 = diffs1.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "diffs2 = diffs2.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "desc1 = np.broadcast_to(desc1, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc1.shape[-1])\n",
    "desc2 = np.broadcast_to(desc2, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc2.shape[-1])\n",
    "\n",
    "desc = np.stack([desc1, desc2],-1)\n",
    "\n",
    "co_simis = cos(diffs1, diffs2)\n",
    "\n",
    "diffs2.shape, diffs1.shape, desc.shape, co_simis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86fa46-48e5-471c-9ed6-2a8c5cd94573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the similarities\n",
    "sort_out = torch.argsort(co_simis)\n",
    "\n",
    "# Removing 'wrong' similarities\n",
    "clean_sort_simis = []\n",
    "clean_desc = []\n",
    "\n",
    "for idx in sort_out:\n",
    "    simi = co_simis[idx]\n",
    "    des = desc[idx]\n",
    "    vp_a, vp_b = des[0]\n",
    "    if vp_a == vp_b:\n",
    "        continue\n",
    "    clean_desc.append(des)\n",
    "    clean_sort_simis.append(simi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614a769-dc55-4b3c-8c23-95c323da0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_simis = []\n",
    "min_simis = []\n",
    "max_50_simis = []\n",
    "min_50_simis = []\n",
    "max_50_desc = []\n",
    "min_50_desc = []\n",
    "mean_simis = []\n",
    "max_simis_desc = []\n",
    "min_simis_desc = []\n",
    "n_comb = []\n",
    "\n",
    "for i, pair  in enumerate(material_combinations.items()):\n",
    "    name, combination = pair\n",
    "    descriptions = []\n",
    "    diffs = []\n",
    "    \n",
    "    for viewpoint, fterms1, fterms2 in combination:\n",
    "        for fterm1 in fterms1:\n",
    "            for fterm2 in fterms2:\n",
    "                try: \n",
    "                    vec1 = emb_dict[fterm1]\n",
    "                    vec2 = emb_dict[fterm2]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "    \n",
    "                diffs.append(vec2- vec1)\n",
    "                descriptions.append([viewpoint, fterm1, fterm2, name])\n",
    "    \n",
    "    descriptions = np.array(descriptions)\n",
    "    diffs = torch.tensor(diffs)\n",
    "    \n",
    "    # Generating the cosine similarities\n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity(dim =1)\n",
    "    try:\n",
    "        diffs = torch.nn.functional.normalize(diffs, p=2, dim=1)\n",
    "    except IndexError:\n",
    "        continue\n",
    "        \n",
    "    diffs1 = diffs.unsqueeze(0)\n",
    "    diffs2 = diffs.unsqueeze(1)\n",
    "    desc1 = np.expand_dims(descriptions, 0)\n",
    "    desc2 = np.expand_dims(descriptions, 1)\n",
    "    diffs1 = diffs1.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "    diffs2 = diffs2.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "    desc1 = np.broadcast_to(desc1, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc1.shape[-1])\n",
    "    desc2 = np.broadcast_to(desc2, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc2.shape[-1])\n",
    "    \n",
    "    desc = np.stack([desc1, desc2],-1)\n",
    "    \n",
    "    co_simis = cos(diffs1, diffs2)\n",
    "\n",
    "    # sorting the similarities\n",
    "    sort_out = torch.argsort(co_simis)\n",
    "    \n",
    "    # Removing 'wrong' similarities (cosine similarities where the theme is identical for both vector differences)\n",
    "    clean_sort_simis = []\n",
    "    clean_desc = []\n",
    "    \n",
    "    for idx in sort_out:\n",
    "        simi = co_simis[idx]\n",
    "        des = desc[idx]\n",
    "        vp_a, vp_b = des[0]\n",
    "        if vp_a == vp_b:\n",
    "            continue\n",
    "        clean_desc.append(des)\n",
    "        clean_sort_simis.append(simi)\n",
    "    try:\n",
    "        \n",
    "        max_simis.append(clean_sort_simis[-1])\n",
    "        min_simis.append(clean_sort_simis[0])\n",
    "        max_50_simis.extend(clean_sort_simis[-50:])\n",
    "        min_50_simis.extend(clean_sort_simis[:50])\n",
    "        max_50_desc.extend(clean_desc[-50:])\n",
    "        min_50_desc.extend(clean_desc[:50])\n",
    "\n",
    "        mean_simis.append(np.mean(clean_sort_simis))\n",
    "        max_simis_desc.append(clean_desc[-1])\n",
    "        min_simis_desc.append(clean_desc[0])\n",
    "        n_comb.append(len(sort_out))\n",
    "    except IndexError:\n",
    "        continue\n",
    "\n",
    "    print(f'{i}  {name} max: {max_simis[-1]}, min: {min_simis[-1]}, mean: {mean_simis[-1]}, mean_max: {np.mean(max_simis)}, mean_min: {np.mean(min_simis)} mean_mean: {np.sum(np.array(mean_simis)*np.array(n_comb))/np.sum(n_comb)}', end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc289ebb-c1ab-492f-8dec-a9912d490836",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_max_simis = np.argsort(max_simis)\n",
    "sort_max_simis = sort_max_simis[::-1]\n",
    "top_50 = []\n",
    "for i, idx in enumerate(sort_max_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = max_simis_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = max_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    top_50.append(output_dict)\n",
    "\n",
    "top_50 = pd.DataFrame(top_50)\n",
    "top_50.to_csv('top_50.csv')\n",
    "top_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609a101-18e1-4057-9a0e-4aede5557673",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_min_simis = np.argsort(min_simis)\n",
    "\n",
    "worst_50 = []\n",
    "for i, idx in enumerate(sort_min_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = min_simis_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = min_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    worst_50.append(output_dict)\n",
    "\n",
    "worst_50 = pd.DataFrame(worst_50)\n",
    "worst_50.to_csv('worst_50.csv')\n",
    "worst_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7f5c6-0282-4620-9697-c529c6848006",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_max_simis = np.argsort(max_50_simis)\n",
    "sort_max_simis = sort_max_simis[::-1]\n",
    "top_50_overall = []\n",
    "for i, idx in enumerate(sort_max_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = max_50_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = max_50_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    top_50_overall.append(output_dict)\n",
    "\n",
    "top_50_overall = pd.DataFrame(top_50_overall)\n",
    "top_50_overall.to_csv('top_50_overall.csv')\n",
    "top_50_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c395b9-59d7-424d-82d0-0d691cc1ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_max_simis = np.argsort(min_50_simis)\n",
    "sort_max_simis = sort_max_simis[::-1]\n",
    "top_50_overall = []\n",
    "for i, idx in enumerate(sort_max_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = max_50_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = max_50_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    top_50_overall.append(output_dict)\n",
    "\n",
    "top_50_overall = pd.DataFrame(top_50_overall)\n",
    "top_50_overall.to_csv('top_50_overall.csv')\n",
    "top_50_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e6aff-fb9c-409a-8322-6898a09edf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_min_simis = np.argsort(min_50_simis)\n",
    "worst_50_overall = []\n",
    "\n",
    "for i, idx in enumerate(sort_min_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = min_50_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = min_50_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    worst_50_overall.append(output_dict)\n",
    "\n",
    "worst_50_overall = pd.DataFrame(worst_50_overall)\n",
    "worst_50_overall.to_csv('worst_50_overall.csv')\n",
    "worst_50_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c8111-4dc8-4c75-8ea5-f17ee904b4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
