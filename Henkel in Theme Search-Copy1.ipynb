{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11acea55-4f6c-4fa4-944e-d36627f561b6",
   "metadata": {},
   "source": [
    "In this notebook I will analyze the patents about electrically debondable adhesives form Henkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af8aa1a-8fa5-4f2e-9829-619906b65754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 11:07:31.827619: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-09 11:07:31.846995: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-09 11:07:32.273358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"ab736b32-4e98-49e1-ae5b-0d39bf31cc67\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"ab736b32-4e98-49e1-ae5b-0d39bf31cc67\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"ab736b32-4e98-49e1-ae5b-0d39bf31cc67\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"ab736b32-4e98-49e1-ae5b-0d39bf31cc67\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"ab736b32-4e98-49e1-ae5b-0d39bf31cc67\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports \n",
    "\n",
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils_agg import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "import psutil\n",
    "from collections import Counter\n",
    "import itertools\n",
    "# Dimension reduction algorithms\n",
    "#from cuml.manifold import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from scipy.fft import fft, fftfreq\n",
    "# Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import export_png\n",
    "from bokeh.palettes import Viridis256, Category20\n",
    "from bokeh.transform import linear_cmap, factor_cmap\n",
    "from bokeh.colors import RGB\n",
    "\n",
    "# Huggingface\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54613b8-2a8e-4828-84a7-a76765492dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 unique patents in the Henkel dataset, only 15 of them contain F-Terms.\n",
      "Loaded Tokenizer from serialized instance!\n",
      "There are 195,617 different F-Terms in the whole Dataset!\n",
      "Embedding Dimension:  768\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# Loading the Henkel Patents\n",
    "###########################################################\n",
    "\n",
    "# Directories in which data important for the notebook is stored\n",
    "dump_dir = 'PK_DUMP'\n",
    "data_dir = 'data'\n",
    "\n",
    "# Loading the dataframes with the patents deemed most important for electrically debondable adhesives from Henkel\n",
    "henkel_patents = pd.read_csv(f'{data_dir}/Henkel_patente_patstat_docdb_families_abstract.csv', delimiter=',').reset_index(drop=True)\n",
    "henkel_orbit = pd.read_csv(f'{data_dir}/Henkel_Orbit_Suche_Patstat_Export.csv', delimiter=',')\n",
    "\n",
    "# Filtering the Samples which contain F-Terms\n",
    "henkel_filtered = henkel_patents[henkel_patents['fterms'].notna()]\n",
    "henkel_filtered = henkel_filtered.reset_index(drop=True)\n",
    "\n",
    "orbit_filtered = henkel_orbit[henkel_orbit['fterms'].notna()]\n",
    "orbit_filtered = orbit_filtered.reset_index(drop=True)\n",
    "\n",
    "print(f\"There are {len(henkel_patents['doc_db_family_id'].unique())} unique patents in the Henkel dataset, only {len(henkel_filtered['doc_db_family_id'].unique())} of them contain F-Terms.\")\n",
    "\n",
    "# extracting all f-terms form the datasets\n",
    "fterms_henkel = [fterm[:10] for fterms in henkel_filtered['fterms'] for fterm in fterms.split(',')]\n",
    "fterms_orbit = [fterm[:10] for fterms in orbit_filtered['fterms'] for fterm in fterms.split(',')]\n",
    "\n",
    "# Aggreagting the F-Terms\n",
    "with open(f'{dump_dir}/aggregation_dict_new.pk', 'rb') as f:\n",
    "    aggregation_dict = pk.load(f)\n",
    "\n",
    "def aggregate(f_term):\n",
    "    try:\n",
    "        return aggregation_dict[f_term]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "fterms_henkel_agg = [aggregate(fterm) for fterm in fterms_henkel if aggregate(fterm) is not None]\n",
    "fterms_orbit_agg = [aggregate(fterm) for fterm in fterms_orbit if aggregate(fterm) is not None]\n",
    "\n",
    "# Counting the occurrences of the henkel and orbit fterms\n",
    "counter_henkel = Counter(fterms_henkel_agg)\n",
    "counter_orbit = Counter(fterms_orbit_agg)\n",
    "\n",
    "# Structuring the henkel F-Terms\n",
    "henkel_dict = {}\n",
    "for fterm in counter_henkel.keys():\n",
    "    theme = fterm[:5]\n",
    "    try:\n",
    "        _ = henkel_dict[theme]\n",
    "    except KeyError:\n",
    "        henkel_dict[theme] = {}\n",
    "\n",
    "    vp = fterm[:8]\n",
    "    try: \n",
    "        henkel_dict[theme][vp].append(fterm)\n",
    "    except KeyError:\n",
    "        henkel_dict[theme][vp] = [fterm]\n",
    "\n",
    "# Extracting the themes form the F-Terms\n",
    "\n",
    "henkel_themes = list(set([fterm[:5] for fterm in fterms_henkel_agg]))\n",
    "orbit_themes = list(set([fterm[:5] for fterm in fterms_orbit_agg]))\n",
    "################################################################\n",
    "# Loading the Model\n",
    "################################################################\n",
    "\n",
    "model_name = 'gal_125_new_1'\n",
    "checkpoint = int(2*86515)\n",
    "# If True normalization is applied to the embeddings\n",
    "norm = True\n",
    "context_less = False\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "if model_name.split('_')[1] == '125':\n",
    "    base_model_name = 'mini'\n",
    "elif model_name.split('_')[1] == '1300':\n",
    "    base_model_name = 'base'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cpu'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "###########################\n",
    "# Loading the Tokenizer\n",
    "###########################\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print('Loaded Tokenizer from serialized instance!')    \n",
    "print(f'There are {n_f_terms:,} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading Descriptions\n",
    "###########################\n",
    "with open(f'{dump_dir}/agg_themes_descriptions_new.pk', 'rb') as f:\n",
    "    theme_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_viewpoints_descriptions_new.pk', 'rb') as f:\n",
    "    viewpoint_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_numbers_descriptions_new.pk', 'rb') as f:\n",
    "    number_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_full_descriptions_new.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)\n",
    "\n",
    "\n",
    "###########################\n",
    "# Extracting the Embeddings\n",
    "###########################\n",
    "\n",
    "# Extracting the classification Head weights\n",
    "inp_emb = model.get_input_embeddings()\n",
    "\n",
    "\n",
    "#Embeddings if the model is not a sequence classification model\n",
    "out_emb = model.get_output_embeddings()\n",
    "out_emb = next(out_emb.parameters()).to('cpu').detach().numpy()[2:]\n",
    "inp_emb = inp_emb(torch.arange(len(tokenizer))).to('cpu').detach().numpy()[50002:]\n",
    "\n",
    "if context_less:\n",
    "    # Extracting context less embeddings\n",
    "    if not os.path.isfile(f'{model_folder}/context_less_emb{checkpoint}.pk'):\n",
    "        print('Calculating context less embeddings!')\n",
    "        context_less_emb = [[] for _ in range(len([1 for _ in model.parameters()]))]\n",
    "        for i in range(len(tokenizer)):\n",
    "            print(i, end='\\r')\n",
    "            out = model(input_ids= torch.tensor([[i]]), attention_mask = torch.tensor([[1]]), output_hidden_states=True)\n",
    "                \n",
    "            out = out.hidden_states\n",
    "            for i, k in enumerate(out):\n",
    "                context_less_emb[i].append(k.to('cpu').detach().numpy())\n",
    "        with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'wb') as f:\n",
    "            pk.dump(context_less_emb, f)\n",
    "    else:\n",
    "        print('Loading context less embeddings from disk')\n",
    "        with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'rb') as f:\n",
    "            context_less_emb = pk.load(f)\n",
    "        \n",
    "    # Combining context less embeddings of a layer to a single tensor\n",
    "    for i, layer in enumerate(context_less_emb):\n",
    "        layer = [e[0] for e in layer]\n",
    "        layer = np.concatenate(layer, 0)\n",
    "        context_less_emb[i] = layer\n",
    "\n",
    "# Normalizing the embeddings \n",
    "def normalize(tensor):\n",
    "    if norm:\n",
    "        return torch.nn.functional.normalize(torch.tensor(tensor), p=2).numpy()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "out_emb = normalize(out_emb)\n",
    "inp_emb = normalize(inp_emb)\n",
    "\n",
    "if context_less:\n",
    "    context_less_emb = [normalize(layer) for layer in context_less_emb]\n",
    "\n",
    "# Extracting the matching F_terms for the weights and creating lists with the defintions\n",
    "tokens = [tokenizer.decode(i) for i in range(len(tokenizer))]\n",
    "f_term_tokens = tokens[50002:]\n",
    "\n",
    "# Creating  a dict with f-Terms and their embedding vectors:\n",
    "out_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, out_emb)}\n",
    "ft_emb_dict = {key: np.abs(fft(value)) for key, value in out_emb_dict.items()}\n",
    "inp_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, inp_emb)}\n",
    "    \n",
    "# Creating Context Less Embedding Dicts\n",
    "if context_less:\n",
    "    context_less_dicts = []\n",
    "    for layer in context_less_emb:\n",
    "        context_less_dicts.append({token[:-1]: vec for token, vec in zip(tokens, layer)})\n",
    "\n",
    "# Extracting the emb_dim\n",
    "for e in out_emb_dict.values():\n",
    "    break\n",
    "emb_dim = e.shape[-1]\n",
    "print('Embedding Dimension: ', emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148b3e2-564b-4a0f-a47c-a7c98cbb8dd6",
   "metadata": {},
   "source": [
    "# Search in Theme Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4939424e-4aa4-40df-95d4-d4cdf4c7d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fterm_dict():\n",
    "    \"\"\"\n",
    "    Creates a hirachical dict with all F-Terms ordered by Theme -> Viewpoint > F-Terms\n",
    "    \"\"\"\n",
    "    f_term_dict = {}\n",
    "    for f_term in f_term_tokens:\n",
    "        theme = f_term.split('/')[0]\n",
    "        vp = f_term[:8]\n",
    "        # Creating a dict entry for the theme\n",
    "        try: \n",
    "            _ = f_term_dict[theme]\n",
    "        except KeyError:\n",
    "            f_term_dict[theme] = {}\n",
    "    \n",
    "        # Creating a dict entry for the viewpoint\n",
    "    \n",
    "        try:\n",
    "            # The first dict call will def. work the second may work if the vp-dict entry \n",
    "            # was already made. If it works the theme is appended to the viewpoint dict\n",
    "            f_term_dict[theme][vp].append(f_term)\n",
    "        except KeyError:\n",
    "            f_term_dict[theme][vp] = []\n",
    "\n",
    "    return f_term_dict\n",
    "\n",
    "\n",
    "def extract_theme_fterms(t_dict):\n",
    "    fterms = []\n",
    "    for vp_list in t_dict.values():\n",
    "        fterms.extend(vp_list)\n",
    "    return fterms        \n",
    "\n",
    "\n",
    "def create_all_diffs(emb=out_emb_dict):\n",
    "    \"\"\"\n",
    "    Creates all possible in viewpoint combinations and returns them a s a \n",
    "    \"\"\"\n",
    "    all_diffs = {}\n",
    "    # Calculating the needed combinations\n",
    "    f_term_dict = create_fterm_dict()\n",
    "    for i, (theme, t_dict) in enumerate(f_term_dict.items()):\n",
    "        print(i, theme, end='\\r')\n",
    "        all_diffs[theme] = {}\n",
    "        fterms = extract_theme_fterms(t_dict)\n",
    "        combinations = itertools.combinations(fterms, 2)\n",
    "        for fterm1, fterm2 in combinations:\n",
    "            viewpoint = fterm1[:8]\n",
    "            diff = emb[fterm2[:10]] - emb[fterm1[:10]]\n",
    "            diff = normalize(np.array([diff]))\n",
    "            try:\n",
    "                all_diffs[theme][viewpoint][(fterm1, fterm2)] = diff\n",
    "            except KeyError:\n",
    "                all_diffs[theme][viewpoint] = {}\n",
    "                all_diffs[theme][viewpoint][(fterm1, fterm2)] = diff\n",
    "    return all_diffs\n",
    "    \n",
    "\n",
    "def create_diffs_tensor(block_theme, all_diffs):\n",
    "    \"\"\"\n",
    "    Creates a tensor with all diffs, which do not contain the block theme.\n",
    "    Additionaly also returns a list with all comination descriptions\n",
    "    \"\"\"\n",
    "    # Filtering out the unwanted theme\n",
    "    diffs = {theme: t_dict for theme, t_dict in all_diffs.items() if theme != block_theme}\n",
    "    out_diffs = []\n",
    "    out_desc = []\n",
    "    for i, (theme, t_dict) in enumerate(diffs.items()):\n",
    "        #print(i, theme, end='\\r')\n",
    "        for vp_dict in t_dict.values():\n",
    "            for comb, diff in vp_dict.items():\n",
    "                out_desc.append(comb)\n",
    "                out_diffs.append(diff)\n",
    "\n",
    "    out_diffs = np.array(out_diffs)\n",
    "    out_diffs = out_diffs.squeeze(1)\n",
    "    return out_diffs, out_desc\n",
    "\n",
    "\n",
    "def search_cos(query_vec, vecs):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarities between all_vecs and the query_vec \n",
    "    \"\"\"\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    # Creating an array of query vectors, with the same number of vectors as the all_vecs array.\n",
    "    query = np.concatenate([query_vec for _ in vecs], 0)\n",
    "    vecs = torch.tensor(vecs, requires_grad=False)\n",
    "    query = torch.tensor(query, requires_grad=False)\n",
    "    simis = cos(vecs, query)\n",
    "    return simis\n",
    "    \n",
    "\n",
    "def search_in_all(fterm1, fterm2, all_diffs, step=100):\n",
    "    \"\"\"\n",
    "    This function computes the most similar combinations in steps of 'step' themes at a time\n",
    "    \"\"\"\n",
    "    theme = fterm1[:5]\n",
    "    query = out_emb_dict[fterm2[:10]] - out_emb_dict[fterm1[:10]]\n",
    "    query = normalize(np.array([query]))\n",
    "    iterations = -(len(all_diffs)//-step)    # Ceiling devision\n",
    "    simis = []                               # Stores all computed cosine similarities\n",
    "    descs = []                               # Stores all combination descriptions (fterm1, fterm2)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        diffs_chunk = dict([d for d in all_diffs.items()][i*step: (i+1)*step])\n",
    "        search_diffs, search_descs = create_diffs_tensor(theme, diffs_chunk)\n",
    "        simis_chunk = search_cos(query, search_diffs)\n",
    "        simis.extend(simis_chunk)\n",
    "        descs.extend(search_descs)\n",
    "\n",
    "    # Sorting for highest similarity\n",
    "    idx = np.argsort(simis)[::-1]\n",
    "    simis = [simis[i].item() for i in idx]\n",
    "    descs = [descs[i] for i in idx]\n",
    "    return simis,  descs,  idx\n",
    "\n",
    "\n",
    "def generate_combinations():\n",
    "    # Extracting the most important Themes\n",
    "    theme_occurrences = {}\n",
    "    for fterm, occ in counter_henkel.items():\n",
    "        theme = fterm[:5]\n",
    "        try:\n",
    "            theme_occurrences[theme] += occ\n",
    "        except KeyError:\n",
    "            theme_occurrences[theme] = occ\n",
    "    \n",
    "    main_themes = ['4J040', '4F100', '4J004']\n",
    "    \n",
    "    # Extracting the most important F-Terms\n",
    "    main_fterms = list(set([fterm for fterm in fterms_henkel_agg if fterm[:5] in main_themes]))\n",
    "    \n",
    "    # Creating all in Theme Combinations\n",
    "    combinations = []\n",
    "    for theme in main_themes:\n",
    "        fterms = [fterm for fterm in main_fterms if fterm[:5] == theme]\n",
    "        # Permutations instead of combinations because we want both directions\n",
    "        combs = list(itertools.permutations(fterms, 2))\n",
    "        combinations.append(combs)\n",
    "    return combinations\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f132dc5-0f4b-4624-acbb-9128b08f3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(f'{model_folder}/all_theme_diffs.pk'):\n",
    "    with open(f'{model_folder}/all_theme_diffs.pk', 'rb') as f:\n",
    "        all_diffs = pk.load(f)\n",
    "\n",
    "else:\n",
    "    all_diffs = create_all_diffs()\n",
    "    with open(f'{model_folder}/all_theme_diffs.pk', 'wb') as f:\n",
    "        pk.dump(all_diffs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8f5ded-2bd7-4818-9654-4beee7010b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Generating in Theme combinations of the three most frequent Henkel Themes\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "combinations = generate_combinations()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc91d6a-f545-47d2-b4a3-5506df000b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4J040/HC01 4J040/JB02\n"
     ]
    }
   ],
   "source": [
    "# Searching with all combinations\n",
    "# !!! This takes more than 24 hours, all results above the threshold similarity will be saved.\n",
    "threshold = 0.15\n",
    "os.makedirs(f'{model_folder}/comb_sims', exist_ok=True)\n",
    "for c in combinations:\n",
    "    for i, (fterm1, fterm2) in enumerate(c):\n",
    "        print(i, fterm1, fterm2)\n",
    "        simis, desc, idx = search_in_all(fterm1, fterm2, all_diffs, step=50)\n",
    "        print('calculated')\n",
    "        desc = [d for d, s in zip(desc, simis) if s > threshold]\n",
    "        simis = [s for s in  simis if s > threshold]\n",
    "        if len(simis) == 0: continue\n",
    "        print(len(desc), len(simis))\n",
    "        with open(f'{model_folder}/comb_sims/{fterm1.replace(\"/\", \"\")}_{fterm2.replace(\"/\",\"\")}_simis.pk', 'wb') as f:\n",
    "            pk.dump(simis, f)\n",
    "        with open(f'{model_folder}/comb_sims/{fterm1.replace(\"/\", \"\")}_{fterm2.replace(\"/\",\"\")}_desc.pk', 'wb') as f:\n",
    "            pk.dump(desc, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ae545-e9ab-4f4e-ad51-b68847cf5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Searching for the combinations with the highest resulting Similarities\n",
    "#########################################################################\n",
    "\n",
    "def generate_search_results(thresh = 0.25):\n",
    "    top_search_res = {}\n",
    "    for c in generate_combinations():\n",
    "        theme = c[0][0][:5]\n",
    "        top_search_res[theme] = {}\n",
    "        for i, (fterm1, fterm2) in enumerate(c):\n",
    "            print(i, end='\\r')\n",
    "            try: \n",
    "                with open(f'{model_folder}/comb_sims/{fterm1.replace(\"/\", \"\")}_{fterm2.replace(\"/\",\"\")}_simis.pk', 'rb') as f:\n",
    "                    simis = pk.load(f)\n",
    "                with open(f'{model_folder}/comb_sims/{fterm1.replace(\"/\", \"\")}_{fterm2.replace(\"/\",\"\")}_desc.pk', 'rb') as f:\n",
    "                    desc = pk.load(f)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            \n",
    "            idx = np.argsort(simis)[::-1]\n",
    "            desc = [desc[i] for i in idx if simis[i] > thresh]\n",
    "            simis = [simis[i] for i in idx if simis[i] > thresh]\n",
    "            \n",
    "            if len(desc) > 0:\n",
    "                top_search_res[theme][(fterm1, fterm2)] = [desc, simis] \n",
    "    return top_search_res\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Searching for viable F-Term Chains\n",
    "#####################################\n",
    "def add_crossconnections(chain, combs):\n",
    "    '''\n",
    "    For a chain consisting of fterms it adds all possible connections between the fterms to it, which have search results above the threshhold.\n",
    "    combs = all fterm connections that have results above the threshold\n",
    "    '''\n",
    "    chain_elements = [c for el in chain for c in el]\n",
    "    cross_links = itertools.combinations(chain_elements, 2)\n",
    "    for cross_link in cross_links:\n",
    "        if cross_link in combs:\n",
    "            chain.append(cross_link)\n",
    "    return chain\n",
    "\n",
    "\n",
    "def create_possible_connections(comb, all_combs):\n",
    "    \"\"\"\n",
    "    Basically the same as add_crossconnections, but for a set of F-Terms and not a chain of connections\n",
    "    \"\"\"\n",
    "    cross_links = itertools.combinations(comb, 2)\n",
    "    valid_connections = []\n",
    "    for cross_link in cross_links:\n",
    "        if cross_link in all_combs:\n",
    "            valid_connections.append(cross_link)\n",
    "    return valid_connections\n",
    "\n",
    "\n",
    "def find_chains(combs, n=3):\n",
    "    elements = []\n",
    "    [elements.extend(comb) for comb in combs]\n",
    "    elements = list(set(elements))\n",
    "    chain_items = itertools.combinations(elements, n)\n",
    "    chains = []\n",
    "    for items in chain_items:\n",
    "        chain = []\n",
    "        for i in range(len(items) - 1):\n",
    "            chain_element = items[i: i+2]\n",
    "            if chain_element in combs:\n",
    "                chain.append(chain_element)\n",
    "        # Only appends chain if it is valid\n",
    "        if len(chain) == n-1:\n",
    "            chains.append(chain)\n",
    "\n",
    "    # Adding cross links to the chains if there are viable cross links\n",
    "    for chain in chains:\n",
    "        chain = add_crossconnections(chain, combs)\n",
    "    return chains\n",
    "\n",
    "    \n",
    "def find_overlap(chain, top_search_res, n=5, chain_n=3):\n",
    "    \"\"\"\n",
    "    n indicates the indices of the f_term string which will be used to measure the search result overlap \n",
    "    (5 = theme, 8 = viewpoint and 10 = whole fterm)\n",
    "    \"\"\"\n",
    "    theme = chain[0][0][:5]\n",
    "    # dict that contains the search resuts for each chain element\n",
    "    combs = {element: top_search_res[theme][element] for element in chain}\n",
    "    # all fterms found in any search\n",
    "    unique_values = list(set([fterm[:n] for desc, simis in combs.values() for pair in desc for fterm in pair]))\n",
    "    found_by = {u_v:[] for u_v in unique_values}\n",
    "    score = {u_v: [] for u_v in unique_values}\n",
    "    for (q1, q2), (desc, simis) in combs.items(): #q1, q2 = query f-terms\n",
    "        for (ft1, ft2), simi in zip(desc, simis): #ft1, ft2 = Fterm found by search with q1, q2\n",
    "            found_by[ft1[:n]].extend([q1, q2])\n",
    "            found_by[ft2[:n]].extend([q1, q2])\n",
    "            score[ft1[:n]].extend([simi, simi])\n",
    "            score[ft2[:n]].extend([simi, simi])\n",
    "\n",
    "    # Now the score will be manipulated, so that the score = maximum threshold at which the u_v can be found\n",
    "    score_idx = {u_v: np.argsort(s)[::-1] for u_v, s in score.items()}\n",
    "    found = []\n",
    "    new_score = []\n",
    "    for u_v, idx in score_idx.items():\n",
    "        queries = found_by[u_v]\n",
    "        scores_for_u_v = score[u_v]\n",
    "        queries = [queries[i] for i in idx]\n",
    "        scores_for_u_v = [scores_for_u_v[i] for i in idx]\n",
    "        if len(set(queries)) == chain_n: # The theme/viewpoint/fterm was found in atleast one search containing each of the unique chain fterms\n",
    "            i = 0\n",
    "            while len(set(queries[:i])) < chain_n:\n",
    "                i += 1\n",
    "                \n",
    "            found.append(u_v)\n",
    "            new_score.append( scores_for_u_v[i])    # Maximal threshold\n",
    "    \n",
    "    # sorting\n",
    "    score = new_score\n",
    "    idx = np.argsort([s for s in score])[::-1]\n",
    "    found = [found[i] for i in idx]\n",
    "    score = [score[i] for i in idx]\n",
    "    return found, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de5391-728b-4b38-b0ae-461cb26a7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promising Search F-Terms\n",
    "\n",
    "search_fterms = {\n",
    "    '4J040': ['4J040/KA09', '4J040/PA41', '4J040/MA01', '4J040/JB09', '4J040/MB01', '4J040/PA21', '4J040/JB10', '4J040/LA09', '4J040/LA06', '4J040/MB08', '4J040/MA07', '4J040/JB02'],\n",
    "    '4F100': ['4F100/BA10', '4F100/JG01', '4F100/JL11', '4F100/AB01', '4F100/DB01', '4F100/JK01', '4F100/BA03', '4F100/BA02', '4F100/GB41', '4F100/BA07', '4F100/BA04', '4F100/EH11', '4F100/EJ61', '4F100/BA05', '4F100/JG05', '4F100/EJ16', '4F100/CA21', '4F100/BA06', '4F100/AB33', '4F100/DC30']\n",
    "}\n",
    "\n",
    "\n",
    "search_fterms_clustered = {\n",
    "'4J040': {\n",
    "    'adhesive':   ['4J040/PA21', '4J040/PA41', '4J040/JB09',  '4J040/JB02'],\n",
    "    'conductive': ['4J040/JB10', '4J040/LA09'],\n",
    "    'others':     ['4J040/KA09' , '4J040/MA01', '4J040/MB01', '4J040/LA06', '4J040/MB08', '4J040/MA07']\n",
    "                 \n",
    "},\n",
    "\n",
    "'4F100': {\n",
    "    'adhesive':   ['4F100/JL11', '4F100/JK01',],\n",
    "    'conductive': ['4F100/JG01', '4F100/GB41', '4F100/EJ61', '4F100/JG05', '4F100/CA21'], \n",
    "    'others':     ['4F100/BA10', '4F100/AB01', '4F100/DB01', '4F100/BA03', '4F100/BA02', '4F100/BA07', '4F100/BA04', '4F100/EH11',  '4F100/BA05',  '4F100/EJ16', '4F100/BA06', '4F100/AB33', '4F100/DC30']\n",
    "    } \n",
    "}\n",
    "\n",
    "search_fields = {\n",
    "    'adhesion electric structure': [\n",
    "                                    '4J040/PA41', # Treatment after adhesion\n",
    "                                    '4J040/JB09', # Pressure sensitive adhesive type\n",
    "                                    '4J040/PA21', \n",
    "                                    '4J040/JB10',\n",
    "                                    '4J040/LA09', \n",
    "                                    '4J040/LA06'\n",
    "                                   ,'4J040/JB02'\n",
    "                                   ], \n",
    "    'function material adhesion': [\n",
    "                                   '4J040/KA09', \n",
    "                                   #'4J040/JB09', \n",
    "                                   '4J040/PA41', \n",
    "                                   '4J040/MA01', \n",
    "                                   '4J040/JB10'\n",
    "                                  ], \n",
    "    'conductivity adhesion metal rigidity': ['4F100/JG01', '4F100/AB01', '4F100/JL11', '4F100/JK01']\n",
    "\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47450783-0274-40d8-9f1d-0919dd9418a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Searching with combinations where some F-Terms must be in a certain category\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "def search_clustered_fterms(query_theme, threshold, comb_n=3, overlap=5):\n",
    "    \n",
    "    results = []\n",
    "    results_scores = []\n",
    "\n",
    "    search_results = generate_search_results(threshold)\n",
    "    fields = search_fterms_clustered[query_theme]\n",
    "    try:\n",
    "        theme_search_res = search_results[query_theme]\n",
    "    except KeyError:\n",
    "        raise KeyError (f'No results for theme {query_theme}')\n",
    "    fterm_fields = [fterms for fterms in fields.values()]\n",
    "    \n",
    "    while len(fterm_fields) < comb_n:  # Adding the F-Terms from the category others multiple times until the wanted number of combinations is reached\n",
    "        fterm_fields.append(fields['others'])\n",
    "\n",
    "    combinations = list(itertools.product(*fterm_fields))\n",
    "    # Dropping F-Terms which have duplicates due to the multiple adding of the 'others' field\n",
    "    combinations = [c for c in combinations if len(set(c)) == comb_n]\n",
    "    combinations = [create_possible_connections(c, theme_search_res) for c in combinations]\n",
    "    combinations = [c for c in combinations if len(set([x for x in c])) == comb_n]\n",
    "    for c in combinations:\n",
    "        overlaps, scores = find_overlap(c, search_results, overlap_n, comb_n)  \n",
    "        results.extend(overlaps)\n",
    "        results_scores.extend(scores)\n",
    "        \n",
    "    # Creating a unique set of results and scores\n",
    "    unique_results = []\n",
    "    unique_scores = []     # if a Theme/ Viewpoint of Fterm iis found multiple times the result with the highes average score is choosen\n",
    "    for result, score in zip(results, results_scores):\n",
    "        if result not in unique_results:\n",
    "\n",
    "            unique_results.append(result)\n",
    "            unique_scores.append(score)\n",
    "        if result in unique_results:\n",
    "            i = unique_results.index(result)\n",
    "            if score > unique_scores[i]:\n",
    "                unique_scores[i] = score\n",
    "\n",
    "    # Sorting by score\n",
    "    idx = np.argsort([score for score in unique_scores])[::-1]\n",
    "    unique_scores = [unique_scores[i] for i in idx]\n",
    "    unique_results = [unique_results[i] for i in idx]\n",
    "\n",
    "    return unique_results, unique_scores\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729cb11-d040-4426-8972-90a03d7efd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fr nchste Woche! Anzeigen fr welche Qery F-Terms die Themes gefunden werden\n",
    "\n",
    "thresh_4J040 = 0.15\n",
    "thresh_4F100 = 0.28\n",
    "\n",
    "ncomb = 4\n",
    "n_overlap = 5 # Theme overlaps\n",
    "\n",
    "themes_4J040, scores_4J040 = search_clustered_fterms('4J040', thresh_4J040, ncomb, n_overlap)\n",
    "themes_4F100, scores_4F100 = search_clustered_fterms('4F100', thresh_4F100, ncomb, n_overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e8a3a-2e45-405c-afdb-a9ecff847a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.21367013454437256\n",
    "compositions of macromolecular compounds\n",
    "In Orbit\n",
    "\n",
    "0.21141356229782104\n",
    "forming of porous articles\n",
    "In Orbit\n",
    "\n",
    "0.20656512677669525\n",
    "container, transfer, fixing, positioning, etc. of wafers, etc.\n",
    "In Orbit\n",
    "\n",
    "0.2052198350429535\n",
    "credit cards or the like\n",
    "In Orbit\n",
    "\n",
    "0.20516209304332733\n",
    "moulding techniques not otherwise provided for, e.g. moulding plastics; combinations of mouldings (no alteration)\n",
    "In Orbit\n",
    "\n",
    "0.19889184832572937\n",
    "polymerisation methods in general\n",
    "In Orbit\n",
    "\n",
    "0.19878213107585907\n",
    "manuscript preparation and masking in photoengraving\n",
    "In Orbit\n",
    "\n",
    "0.19873034954071045\n",
    "polyesters or polycarbonates\n",
    "In Orbit\n",
    "\n",
    "0.1978987157344818\n",
    "laminated bodies (2)\n",
    "In Orbit\n",
    "\n",
    "0.1977296620607376\n",
    "processes specially adapted for manufacturing cables\n",
    "In Orbit\n",
    "\n",
    "0.19709782302379608\n",
    "protection, testing and repair of underground structures and foundations\n",
    "NEW!\n",
    "\n",
    "0.19605137407779694\n",
    "large containers\n",
    "In Orbit\n",
    "\n",
    "0.1957583725452423\n",
    "air conditioning control equipment\n",
    "In Orbit\n",
    "\n",
    "0.1957554817199707\n",
    "electromechanical clocks\n",
    "In Orbit\n",
    "\n",
    "0.19573277235031128\n",
    "brushes\n",
    "NEW!\n",
    "\n",
    "0.19486621022224426\n",
    "treatments of macromolecular shaped articles\n",
    "In Orbit\n",
    "\n",
    "0.193396657705307\n",
    "building environments\n",
    "NEW!\n",
    "\n",
    "0.19339494407176971\n",
    "thermal printer structures\n",
    "In Orbit\n",
    "\n",
    "0.19308800995349884\n",
    "processing and handling of plastics and other materials for molding in general\n",
    "In Orbit\n",
    "\n",
    "0.19230255484580994\n",
    "mating device and connection to printed circuit\n",
    "In Orbit\n",
    "\n",
    "0.19095058739185333\n",
    "large containers\n",
    "In Orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc6597-bbdb-48c2-8abb-853415fbaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for theme, score in zip(themes_4J040, scores_4J040):\n",
    "    print(score)\n",
    "    print(theme_dict[theme])\n",
    "    print('In Orbit' if theme in orbit_themes else 'NEW!')\n",
    "    print('')\n",
    "\n",
    "for theme, score in zip(themes_4F100, scores_4F100):\n",
    "    print(score)\n",
    "    print(theme_dict[theme])\n",
    "    print('In Orbit' if theme in orbit_themes else 'NEW!')\n",
    "    print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dad533-1ec2-469e-98c0-27b455d5a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_'*20)\n",
    "print('not in orbit')\n",
    "print('_'*20)\n",
    "for theme in not_in_orbit:\n",
    "    print(theme, theme_dict[theme[:5]])\n",
    "    try: \n",
    "        print(viewpoint_dict[theme[:8]])\n",
    "    except Exception:\n",
    "        print('vp not found')\n",
    "        pass\n",
    "    try: \n",
    "        print(number_dict[theme[:10]])\n",
    "    except Exception:\n",
    "        pass\n",
    "    print('')\n",
    "\n",
    "print('_'*20)\n",
    "print('in orbit')\n",
    "print('_'*20)\n",
    "\n",
    "for theme in in_orbit:\n",
    "    print(theme, theme_dict[theme[:5]])\n",
    "    try: \n",
    "        print(viewpoint_dict[theme[:8]])\n",
    "    except Exception:\n",
    "        print('vp not found')\n",
    "        pass\n",
    "    try: \n",
    "        print(number_dict[theme[:10]])\n",
    "    except Exception:\n",
    "        pass\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2261f-4f50-4d0c-8f99-e79755b5ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_search_res = generate_search_results(0.15)\n",
    "df = {}\n",
    "found_fterms = []\n",
    "found_by = []\n",
    "all_found = True\n",
    "\n",
    "for field, query_fterms in search_fields.items():\n",
    "    theme = query_fterms[0][:5]\n",
    "    chain = list(itertools.combinations(query_fterms, 2))\n",
    "    chain = [c for c in chain if c in top_search_res[theme].keys()]\n",
    "    if len(chain) == 0:\n",
    "        print('No chain for', field)\n",
    "        all_found = False\n",
    "        continue\n",
    "        \n",
    "    themes, scores = find_overlap(chain, top_search_res, n=5, chain_n=len(query_fterms))\n",
    "    chain_elements = [x for a in chain for x in a]\n",
    "    chain = field + ':   '\n",
    "    for fterm in list(set(chain_elements)):\n",
    "        label = number_dict[fterm]\n",
    "        chain += f'{fterm}:'\n",
    "        chain += label\n",
    "        chain += '| '\n",
    "\n",
    "    in_orbit = ['Yes' if theme in orbit_themes else 'No' for theme in themes]\n",
    "    themes = [f'({theme}):{theme_dict[theme]}| Score: {score}' for theme, score in zip(themes, scores)]\n",
    "    #themes = [f'({theme}):{full_descriptions_dict[theme]}| Score: {score}' for theme, score in zip(themes, scores)]\n",
    "    if len(themes) == 0:\n",
    "        print('No themes found',  field)\n",
    "        all_found = False\n",
    "        continue\n",
    "    found_fterms.extend(themes)\n",
    "    found_by.extend([chain for _ in themes])\n",
    "    df[chain] = pd.Series(themes)\n",
    "    df[f'In Orbit {field}'] = pd.Series(in_orbit)\n",
    "\n",
    "all_found = True\n",
    "if all_found:\n",
    "    df = pd.DataFrame(df)          \n",
    "    df = df.reset_index()\n",
    "    df.to_excel('Field Search Themes 0_15 comp orbit.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1088317-d5ce-4ada-9618-83c7cdd5370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orbit_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be82a27-49bf-43be-a68b-b5588b7340e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the overlaps for all chains\n",
    "thresh = 0.3\n",
    "search = 5\n",
    "search_theme = '4F100'\n",
    "top_search_res = generate_search_results(thresh)\n",
    "df = {}\n",
    "found_fterms = []\n",
    "found_by = []\n",
    "n=4\n",
    "for searches in search_fterms.values():\n",
    "    theme = searches[0][:5]\n",
    "    if not theme == search_theme:\n",
    "        continue\n",
    "    chains = list(itertools.combinations(searches, n))\n",
    "    chains = [list(itertools.combinations(chain, 2)) for chain in chains]\n",
    "    chains = [[e for e in chain if e in top_search_res[theme].keys()] for chain in chains]\n",
    "    chains = [chain for chain in chains if len(set([e for e in chain])) == n]\n",
    "    print('Len Chains', len(chains))\n",
    "    for chain in chains:\n",
    "        themes, scores = find_overlap(chain, top_search_res, search)\n",
    "        chain_elements = [x for a in chain for x in a]\n",
    "        chain = 'QUERY'\n",
    "        for fterm in list(set(chain_elements)):\n",
    "            label = number_dict[fterm]\n",
    "            chain += f'{fterm}:'\n",
    "            chain += label\n",
    "            chain += '|'\n",
    "    \n",
    "        #themes = [f'({theme}):{theme_dict[theme]}| Score: {score}' for theme, score in zip(themes, scores)]\n",
    "        #themes = [f'({theme}):{full_descriptions_dict[theme]}| Score: {score}' for theme, score in zip(themes, scores)]\n",
    "        if len(themes) == 0:\n",
    "            continue\n",
    "        found_fterms.extend(themes)\n",
    "        found_by.extend([chain for _ in themes])\n",
    "        df[chain] = pd.Series(themes)\n",
    "\n",
    "df = pd.DataFrame(df)          \n",
    "df = df.reset_index()\n",
    "\n",
    "#res_1 = [f[1:11] for f in found_fterms]\n",
    "res_1 = found_fterms\n",
    "res_1_found_by = found_by\n",
    "\n",
    "# generating the overlaps for all chains\n",
    "thresh = 0.2\n",
    "search = 5\n",
    "search_theme = '4J040'\n",
    "top_search_res = generate_search_results(thresh)\n",
    "df = {}\n",
    "found_fterms = []\n",
    "found_by = []\n",
    "n=4\n",
    "for searches in search_fterms.values():\n",
    "    theme = searches[0][:5]\n",
    "    if not theme == search_theme:\n",
    "        continue\n",
    "    chains = list(itertools.combinations(searches, n))\n",
    "    chains = [list(itertools.combinations(chain, 2)) for chain in chains]\n",
    "    chains = [[e for e in chain if e in top_search_res[theme].keys()] for chain in chains]\n",
    "    chains = [chain for chain in chains if len(set([e for e in chain])) == n]\n",
    "    print('Len Chains', len(chains))\n",
    "    for chain in chains:\n",
    "        themes, scores = find_overlap(chain, top_search_res, search)\n",
    "        chain_elements = [x for a in chain for x in a]\n",
    "        chain = 'QUERY'\n",
    "        for fterm in list(set(chain_elements)):\n",
    "            label = number_dict[fterm]\n",
    "            chain += f'{fterm}:'\n",
    "            chain += label\n",
    "            chain += '|'\n",
    "    \n",
    "        #themes = [f'({theme}):{theme_dict[theme]}| Score: {score}' for theme, score in zip(themes, scores)]\n",
    "        #themes = [f'({theme}):{full_descriptions_dict[theme]}| Score: {score}' for theme, score in zip(themes, scores)]\n",
    "        if len(themes) == 0:\n",
    "            continue\n",
    "        found_fterms.extend(themes)\n",
    "        found_by.extend([chain for _ in themes])\n",
    "        df[chain] = pd.Series(themes)\n",
    "\n",
    "#res_2 = [f[1:11] for f in found_fterms]\n",
    "res_2 = found_fterms\n",
    "res_2_found_by = found_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fe546-838d-4078-aff4-982828a5a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all = [*res_1, *res_2]\n",
    "len(orbit_themes), len(set(res_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c6d80-ee53-4def-8b46-bace1f3a7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the found themes by their appearence in the orbit search\n",
    "\n",
    "new_4F100_themes = list(set([res for res in res_1 if res[:5] not in orbit_themes]))\n",
    "new_4J040_themes = list(set([res for res in res_2 if res[:5] not in orbit_themes]))\n",
    "\n",
    "search_res = 'Found in search with 4F100 chains of length 4\\n--------------------------------------------------------------------------\\n'\n",
    "\n",
    "for res in list(set(res_1)):\n",
    "    \n",
    "    line = f'In Orbit: {\"TRUE\" if res[:5] in orbit_themes else \"FALSE\"}; {res}: {theme_dict[res[:5]]} \\n'\n",
    "    search_res += line\n",
    "\n",
    "search_res += 'Found in Search with 4J040 chains of length 4\\n--------------------------------------------------------------------------\\n'\n",
    "    \n",
    "for res in list(set(res_2)):\n",
    "    \n",
    "    line = f'In Orbit: {\"TRUE\" if res[:5] in orbit_themes else \"FALSE\"}; {res}: {theme_dict[res[:5]]} \\n'\n",
    "    search_res += line\n",
    "\n",
    "with open('all_chains_len4_search.txt', 'w') as f:\n",
    "    f.writelines(search_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10405c85-f6eb-4a2b-97ac-85aaf95c6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding the query F-Terms\n",
    "query_1_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in search_fterms['4J040']]))\n",
    "query_2_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in search_fterms['4F100']]))\n",
    "\n",
    "# Embedding the found F-Terms\n",
    "found_1_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in res_1_fterms]))\n",
    "found_2_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in res_2_fterms]))\n",
    "\n",
    "\n",
    "# Embedding the orbit fterms\n",
    "orbit_emb = []\n",
    "o_ft = []\n",
    "for fterm in counter_orbit.keys(): \n",
    "    try:\n",
    "        orbit_emb.append(out_emb_dict[fterm])\n",
    "        o_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "orbit_emb = np.array(orbit_emb)\n",
    "\n",
    "all_emb = np.concatenate([query_1_emb, query_2_emb, found_1_emb, found_2_emb, orbit_emb], 0)\n",
    "\n",
    "# Calculating the TSNE Representation\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69)\n",
    "#rep = tsne.fit_transform(all_emb)\n",
    "\n",
    "\n",
    "datasource_query1 = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[0:query_1_emb.shape[0],0],\n",
    "            y = rep[0:query_1_emb.shape[0],1],\n",
    "            fterms = search_fterms['4J040'],\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in search_fterms['4J040']],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in search_fterms['4J040']],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in search_fterms['4J040']]))\n",
    "\n",
    "datasource_query2 = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[query_1_emb.shape[0]:query_1_emb.shape[0] + query_2_emb.shape[0],0],\n",
    "            y = rep[query_1_emb.shape[0]:query_1_emb.shape[0] + query_2_emb.shape[0],1],\n",
    "            fterms = search_fterms['4F100'],\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in search_fterms['4F100']],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in search_fterms['4F100']],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in search_fterms['4F100']]))\n",
    "\n",
    "datasource_found1 = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[query_1_emb.shape[0] + query_2_emb.shape[0]: query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0],0],\n",
    "            y = rep[query_1_emb.shape[0] + query_2_emb.shape[0]: query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0],1],\n",
    "            fterms = res_1_fterms,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in res_1_fterms],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in res_1_fterms],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in res_1_fterms]\n",
    "        ))\n",
    "\n",
    "datasource_found2 = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0]: query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0] + found_2_emb.shape[0],0],\n",
    "            y = rep[query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0]: query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0] + found_2_emb.shape[0],1],\n",
    "            fterms = res_2_fterms,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in res_2_fterms],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in res_2_fterms],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in res_2_fterms]\n",
    "        ))\n",
    "\n",
    "datasource_orbit = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[ query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0] + found_2_emb.shape[0]:, 0],\n",
    "            y = rep[ query_1_emb.shape[0] + query_2_emb.shape[0] + found_1_emb.shape[0] + found_2_emb.shape[0]:, 1],\n",
    "            fterms = o_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in o_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in o_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in o_ft]))\n",
    "\n",
    "\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>F-Term:</b> @fterms<br><b>Theme:</b> @themes<br><b>Viewpoint:</b> @viewpoints<br><b>Number:</b> @numbers<br><b>Query:</b> @found_by</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='TSNE Chain Search')\n",
    "    \n",
    "\n",
    "plot_tsne.triangle('x', 'y', size=7, fill_color=RGB(200, 30, 30), alpha=0.5, line_width=0, source=datasource_found1, name=\"Found F-Terms 1\")\n",
    "plot_tsne.triangle('x', 'y', size=7, fill_color=RGB(0, 50, 250), alpha=0.5, line_width=0, source=datasource_found2, name=\"Found F-Terms 2\")\n",
    "plot_tsne.square('x', 'y', size=7, fill_color=RGB(50, 250, 50), alpha=0.5, line_width=0, source=datasource_orbit, name=\"Orbit F-Terms\")\n",
    "plot_tsne.circle('x', 'y', size=10, fill_color=RGB(200, 150, 50), alpha=1, line_width=0, source=datasource_query1, name=\"Query F-Terms 1\")\n",
    "plot_tsne.circle('x', 'y', size=10, fill_color=RGB(200, 50, 150), alpha=1, line_width=0, source=datasource_query2, name=\"Query F-Terms 2\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62b839-6706-4add-810f-794a5c128282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n search results for evaluation\n",
    "n = 5\n",
    "for theme, combs in top_search_res.items():\n",
    "    for (q1, q2), (desc, simis) in combs.items():\n",
    "        print('')\n",
    "        print('QUERY:')\n",
    "        print( full_descriptions_dict[q1], '|||', full_descriptions_dict[q2])\n",
    "        print('______________________')\n",
    "        \n",
    "        for (ft1, ft2), s in zip(desc[:n], simis[:n]):\n",
    "            print(s, full_descriptions_dict[ft1[:10]], '|||', full_descriptions_dict[ft2[:10]])\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8e9cc-f239-462a-8b07-f42a1b740722",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Searching for promising Henkel Combinations  (In Theme)\n",
    "###############################################################\n",
    "\n",
    "\n",
    "search_combination = ['4F100/JL11', '4F100/JG01']  # (adhesiveness, conductivity being properties or funcitons)\n",
    "#search_combination = ['4J040/JB09', '4J040/PA21']  # (pressure sensitive adhesive or adhesive types, use of adhesive characterised by specific shapess of functions)\n",
    "#search_combination = ['4J004/CC02', '4J004/CA07']  # (foil like, inorganic materials)\n",
    "\n",
    "simis, descs, idx = search_in_all(*search_combination, all_diffs, step=500)\n",
    "results = descs[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd343ad-46c0-4336-9389-a7b87a0ad97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(adhesiveness, conductivity being properties or funcitons)')\n",
    "for fterm1, fterm2 in results:\n",
    "    theme = theme_dict[fterm1[:5]]\n",
    "    vp1 = viewpoint_dict[fterm1[:8]]\n",
    "    vp2 = viewpoint_dict[fterm2[:8]]\n",
    "    n1 = number_dict[fterm1[:10]]\n",
    "    n2 = number_dict[fterm2[:10]]\n",
    "\n",
    "    print(f'''    \n",
    "Theme: {theme}\n",
    "vp1: {vp1}     vp2: {vp2}\n",
    "n1: {n1}       n2:{n2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d443af-47ab-4ddd-88d3-cf4867a604d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting uniqe F-terms from the results\n",
    "found_f_terms = []\n",
    "[found_f_terms.extend(comb) for comb in results]\n",
    "found_f_terms = list(set(found_f_terms))\n",
    "\n",
    "# Embedding the found F-Terms\n",
    "hits_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in found_f_terms]))\n",
    "\n",
    "orbit_emb = []\n",
    "o_ft = []\n",
    "for fterm in counter_orbit.keys(): \n",
    "    try:\n",
    "        orbit_emb.append(out_emb_dict[fterm])\n",
    "        o_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "orbit_emb = np.array(orbit_emb)\n",
    "\n",
    "henkel_emb = []\n",
    "h_ft = []\n",
    "for fterm in search_combination:\n",
    "    try:\n",
    "        henkel_emb.append(out_emb_dict[fterm])\n",
    "        h_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "henkel_emb = np.array(henkel_emb)\n",
    "\n",
    "all_emb = np.concatenate([orbit_emb, henkel_emb, hits_emb], 0)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69)\n",
    "rep = tsne.fit_transform(all_emb)\n",
    "\n",
    "datasource_henkel = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft):len(o_ft) + len(h_ft),0],\n",
    "            y = rep[len(o_ft):len(o_ft) + len(h_ft),1],\n",
    "            fterms = h_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in h_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in h_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in h_ft]))\n",
    "\n",
    "datasource_orbit = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[:len(o_ft), 0],\n",
    "            y = rep[:len(o_ft), 1],\n",
    "            fterms = o_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in o_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in o_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in o_ft]))\n",
    "\n",
    "datasource_emb_search = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft) + len(h_ft):, 0],\n",
    "            y = rep[len(o_ft) + len(h_ft):, 1],\n",
    "            fterms = found_f_terms,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in found_f_terms],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in found_f_terms],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in found_f_terms]))\n",
    "\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>F-Term:</b> @fterms<br><b>Theme:</b> @themes<br><b>Viewpoint:</b> @viewpoints<br><b>Number:</b> @numbers<br><b>Query:</b> @found_by</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Henkel and Orbit Embeddings')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=10, fill_color=RGB(250, 50, 75), alpha=1, line_width=0, source=datasource_henkel, name=\"Henkel Embeddings\")\n",
    "plot_tsne.square('x', 'y', size=7, fill_color=RGB(50, 75, 250), alpha=0.2, line_width=0, source=datasource_orbit, name=\"Orbit Embeddings\")\n",
    "plot_tsne.triangle('x', 'y', size=7, fill_color=RGB(75, 250, 50), alpha=1, line_width=0, source=datasource_emb_search, name=\"Cos Similar Embeddings\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be44417-e902-4b9d-9d17-3dd403901fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
