{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14d1105-9dc4-45ba-a080-1f033a34ea05",
   "metadata": {},
   "source": [
    "This is a rewrite of the original Dataset Analysis notebook. \n",
    "\n",
    "In this notebook f-terms which are sub F-Terms of different f-Terms are aggregated to the main F-Term which is. Main F-Terms are indicates by . <name> sub F-Terms are indicates by .. <name> etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae941e2c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a0e73-93ed-4105-bdef-22e3f324c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from dask import dataframe as dd\n",
    "import dask\n",
    "import time\n",
    "import re\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a554c3",
   "metadata": {},
   "source": [
    "# Loading the Dataset to a Dask DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50aa51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/JPO_patents_abstracts_fterms'\n",
    "f_term_def_file = r'data/f-terms.csv'\n",
    "\n",
    "# Using dask because the file is way to big for memory.\n",
    "data = dask.dataframe.read_parquet(file, delimiter='\\t')\n",
    "l_data = len(data)\n",
    "\n",
    "pk_dump_dir = r'PK_DUMP'\n",
    "\n",
    "n_load = l_data\n",
    "\n",
    "print(f'There are {l_data} patents listed in the dataset')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59fdd9-3402-402f-aded-5d84418f1624",
   "metadata": {},
   "source": [
    "# Aggregating F-Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af0741-9a65-4f02-941c-15da2e8af5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the defenitions of all fine grained F-Terms\n",
    "f_term_def = pd.read_csv(f_term_def_file)\n",
    "print(f'There are {len(f_term_def)} entrys in the F-Terms CSV file')\n",
    "f_term_def.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcda5f8-4cd7-420f-a7ce-8da73cffdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a making the viewpoints in the dataframe unique by adding the theme to it\n",
    "# Only run this cell once otherwise the theme is added multiple times\n",
    "f_term_def['viewpoint'] = f_term_def['theme'] + '/' + f_term_def['viewpoint'] \n",
    "f_term_def.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c4bb4-b1c0-4ac9-ba67-0b28d0dadbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating over all viewpoints and aggregating the f-terms with more than one . infront of them\n",
    "import time\n",
    "# Unique Viewpoints\n",
    "unique_viewpoints = f_term_def['viewpoint'].dropna(inplace=False).unique()\n",
    "\n",
    "# This dict contains an entry for all original f-terms, which are mapped to the f-terms the aggregate to.\n",
    "# This algorithm is based on the assumption, that sub-f-terms follow after directly after theier main f-term in the \n",
    "# f_term_def dataframe\n",
    "\n",
    "f_term_aggregation_dict = {}\n",
    "for i, viewpoint in enumerate(unique_viewpoints):\n",
    "    print(f'{i:,}', 'Viewpoint', viewpoint, end='\\r')\n",
    "    f_terms = f_term_def.query(\"viewpoint == @viewpoint\").dropna()\n",
    "\n",
    "    current_main_f_term = ''\n",
    "    for number, theme, label in zip(f_terms['number'], f_terms['theme'], f_terms['label']):\n",
    "        f_term = str(theme) + '/' + str(number)\n",
    "        if not label[:3] == '. .':\n",
    "            current_main_f_term = f_term\n",
    "        f_term_aggregation_dict[f_term] = current_main_f_term\n",
    "\n",
    "unique_agg_f_terms = [f_term for f_term in f_term_aggregation_dict.values()]\n",
    "unique_agg_f_terms = np.unique(unique_agg_f_terms)\n",
    "\n",
    "print(f'After Aggregation to their main F-Term there are {len(unique_agg_f_terms)} F-Terms left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a937f9-028a-4a4e-ae89-a5fc1b75b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pk_dump_dir}/aggregation_dict.pk', 'wb') as f:\n",
    "    pk.dump(f_term_aggregation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530597a7-d9f8-4693-9daf-9f04f3e22b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating F-Terms in the dataset\n",
    "def aggregate_f_terms(f_terms_string):\n",
    "    f_terms = f_terms_string.split(',')\n",
    "    agg_f_terms = []\n",
    "    for f_term in f_terms:\n",
    "        try: \n",
    "            agg_f_terms.append(f_term_aggregation_dict[f_term])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    f_terms = np.unique(agg_f_terms)\n",
    "    f_terms_string = ''.join([f_term + ',' for f_term in f_terms])[:-1] # [:-1] to remove last comma\n",
    "    return f_terms_string\n",
    "\n",
    "data['jp_class_symbol'] = data['jp_class_symbol'].apply(aggregate_f_terms, meta=('jp_class_symbol', 'str'))\n",
    "data.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672bc07",
   "metadata": {},
   "source": [
    "# Deleting Previous Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba3224-0166-4396-a9e4-df712e084406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you want to recalculate the number of labels, the text_lengths and the label_embedding\n",
    "\n",
    "i = input('''Warning! You are about to delete all previously computed files. If you want to continue write \"y\": ''')\n",
    "\n",
    "if i == \"y\":\n",
    "    with open(pk_dump_dir + r'/agg_n_labels', 'wb') as f:\n",
    "        pass\n",
    "    with open(pk_dump_dir + r'/agg_text_lengths', 'wb') as f:\n",
    "        pass\n",
    "    with open(pk_dump_dir + r'/agg_label_embedding', 'wb') as f:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb28ec9",
   "metadata": {},
   "source": [
    "# Extracting the abstract text-lengths and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d94ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelEmbedding():\n",
    "    \"\"\"\n",
    "    A class to count the occurrence of each individual label.\n",
    "    It also creates a dict, which contains each label and matches it to a number\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict = {}\n",
    "        self.r_dict = {}\n",
    "        self.occurrence = []\n",
    "        \n",
    "        \n",
    "    def __call__(self, label):\n",
    "        try: \n",
    "            emb = self.dict[label]\n",
    "            self.occurrence[emb] += 1\n",
    "        except KeyError:\n",
    "            emb = len(self.dict)\n",
    "            self.dict[label] = emb\n",
    "            self.r_dict[emb] = label\n",
    "            self.occurrence.append(1)\n",
    "        \n",
    "        return emb\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "    \n",
    "    def reverse(self, emb):\n",
    "        return self.r_dict[emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c60f1-b175-48f1-a53d-3349e8c6f29f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell only on your first run of this notebook, it takes really long.\n",
    "# All outputs will be saved and can be loaded from disk in \n",
    "# all following runs of this notebook.\n",
    "\n",
    "\n",
    "i = input('''Warning! You are about to recalculate all metric files. This will take a wile!\n",
    "If you want to continue write \"y\": ''')\n",
    "\n",
    "\n",
    "n_labels = []\n",
    "text_lengths = []\n",
    "    \n",
    "def get_text_lengths(line):\n",
    "    \"\"\"\n",
    "    Returns the length of the patent abstract.\n",
    "    \"\"\"\n",
    "    text = line['appln_abstract']\n",
    "    text = text.split()\n",
    "    return len(text)\n",
    "    \n",
    "    \n",
    "def get_labels(line):\n",
    "    \"\"\"\n",
    "    Returns the f_term labels of a patent as a list of strings.\n",
    "    \"\"\"\n",
    "    f_terms = line['jp_class_symbol']\n",
    "    f_terms = f_terms.split(',')\n",
    "    return f_terms\n",
    "    \n",
    "    \n",
    "class LabelEmbedding():\n",
    "    def __init__(self):\n",
    "        self.dict = {}\n",
    "        self.r_dict = {}\n",
    "        self.occurrence = []\n",
    "            \n",
    "            \n",
    "    def __call__(self, label):\n",
    "        try: \n",
    "            emb = self.dict[label]\n",
    "            self.occurrence[emb] += 1\n",
    "        except KeyError:\n",
    "            emb = len(self.dict)\n",
    "            self.dict[label] = emb\n",
    "            self.r_dict[emb] = label\n",
    "            self.occurrence.append(1)\n",
    "            \n",
    "        return emb\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "        \n",
    "    def reverse(self, emb):\n",
    "        return self.r_dict[emb]\n",
    "    \n",
    "if i == \"y\":\n",
    "    # Iterating over the whole dataset and extracting the text_lengths and the labels\n",
    "    label_embedding = LabelEmbedding()\n",
    "    for i, line in enumerate(data.iterrows()):\n",
    "        \n",
    "        # Processing the data\n",
    "        line = line[1]\n",
    "        labels_split = get_labels(line)\n",
    "        labels_split = [label_embedding(label) for label in labels_split]\n",
    "        \n",
    "        # storing in lists\n",
    "        n_labels.append(len(labels_split))\n",
    "        text_lengths.append(get_text_lengths(line))\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print(f'Processed {i} samples', end='\\r')\n",
    "            \n",
    "        if i == n_load:\n",
    "            # Stopping when finnished\n",
    "            with open(pk_dump_dir + r'/agg_n_labels', 'ab') as f:\n",
    "                pk.dump(n_labels, f)\n",
    "                n_labels = []\n",
    "            with open(pk_dump_dir + r'/agg_text_lengths', 'ab') as f:\n",
    "                pk.dump(text_lengths, f)\n",
    "                text_lengths = []\n",
    "            break\n",
    "            \n",
    "        if i%100000 == 0 and i != 0:\n",
    "            # Saving chunks of processed data to not overflow the memory\n",
    "            with open(pk_dump_dir + r'/agg_n_labels', 'ab') as f:\n",
    "                pk.dump(n_labels, f)\n",
    "                n_labels = []\n",
    "            with open(pk_dump_dir + r'/agg_text_lengths', 'ab') as f:\n",
    "                pk.dump(text_lengths, f)\n",
    "                text_lengths = []\n",
    "    \n",
    "    # Saving the label_embedding to access them faster in the next runs of this notebook\n",
    "    with open(pk_dump_dir + r'/agg_label_embedding', 'ab') as f:\n",
    "                pk.dump(label_embedding, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff640f0d",
   "metadata": {},
   "source": [
    "# Number of Labels in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a91398",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pk_dump_dir + r'/agg_label_embedding', 'rb') as f: \n",
    "    label_embedding = pk.load(f)\n",
    "\n",
    "print(f'Number of F-Term Labels in Dataset = {len(label_embedding)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc1875",
   "metadata": {},
   "source": [
    "# Plotting the Word Counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the list from Memory\n",
    "\n",
    "with open(pk_dump_dir + r'/agg_text_lengths', 'rb') as f:\n",
    "    text_lengths = []\n",
    "    while True:\n",
    "        try: \n",
    "            text_lengths.extend(pk.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "       \n",
    "hist_wc = plt.hist(text_lengths, bins=[i for i in range(400)])\n",
    "plt.xlabel('Text Length in Words')\n",
    "plt.ylabel('Occurrence in Dataset-Slice')\n",
    "plt.title(f'Patent Abstract Word Counts in {n_load} Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60969875",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Plotting the Labels per Patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pk_dump_dir + r'/agg_n_labels', 'rb') as f:\n",
    "    n_labels = []\n",
    "    while True:\n",
    "        try: \n",
    "            n_labels.extend(pk.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "hist_lpp = plt.hist(n_labels, bins=[i for i in range(260)])\n",
    "plt.xlabel(f'Labels per Patent')\n",
    "plt.ylabel('Occurrence in Dataset-Slice')\n",
    "plt.title('Histogram of Labels per Patent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156b2aa-1031-4b35-ab1d-e6a593fb5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'On average each patent has {np.mean(n_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of patents with just one label\n",
    "l_single = hist_lpp[0][1]\n",
    "\n",
    "print(f'''\n",
    "There are {l_single} patents that only have one label! ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d6e68",
   "metadata": {},
   "source": [
    "# Counting the Occurrence of Each Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a62034",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = label_embedding.occurrence\n",
    "\n",
    "\n",
    "print(f'The maximum occurrence of a label is {max(occurrences)} times in the {n_load} samples dataset-slice.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1f178",
   "metadata": {},
   "source": [
    "# Plotting the Label Occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8360a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(occurrences, bins=np.arange(500))\n",
    "plt.title(f'Occurrences of Labels in {n_load} Samples')\n",
    "plt.xlabel(f'Occurrence of Label in Dataset')\n",
    "plt.ylabel(f'Labels With this Occurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argwhere(np.array(occurrences) == 1)\n",
    "l_one_time = len(np.array(occurrences)[ind])\n",
    "print(f'There are {l_one_time} labels that only occur once in the dataset-slice.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63327123",
   "metadata": {},
   "source": [
    "# All F-Terms in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea90c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_terms = [k for k, v in label_embedding.dict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8fea5",
   "metadata": {},
   "source": [
    "# Splitting the F-Terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_f_term(f_term):\n",
    "    \"\"\"\n",
    "    This function splits an f_term into a hirachical order of classes.\n",
    "    \n",
    "    :f_term:   string: f_term as a string\n",
    "    \n",
    "    :return:   list of strings: f_term classes as a list of strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        theme_code, term_code = f_term.split('/')\n",
    "    except Exception:\n",
    "        return f_term, '', '', ''\n",
    "    view_point = term_code[:2]\n",
    "    digit = term_code[2:4]\n",
    "    additional_code = term_code[4:]\n",
    "    #print(f'theme-code: {theme_code}, term-code: {term_code}, view_point: {view_point}, digit: {digit}, additional-code: {additional_code}')\n",
    "    return theme_code, view_point, digit, additional_code\n",
    "    \n",
    "\n",
    "classes_list = [split_f_term(f_term) for f_term in f_terms]\n",
    "classes = pd.DataFrame(classes_list, columns=[\"theme\", \"viewpoint\", \"number\", \"additional code\"])\n",
    "classes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c04645",
   "metadata": {},
   "source": [
    "# Loading the F-Term Definitions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef08d8-fee6-4ff2-a118-e999f3925d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the defenitions of all fine grained F-Terms\n",
    "f_term_def = pd.read_csv(f_term_def_file)\n",
    "print(f'There are {len(f_term_def)} entrys in the F-Terms CSV file')\n",
    "f_term_def.head()\n",
    "\n",
    "# Dropping the aggregated f-terms \n",
    "idx = [i for i, (theme, number) in enumerate(zip(f_term_def['theme'], f_term_def['number'])) if str(theme) + '/' + str(number) in unique_agg_f_terms]\n",
    "f_term_def = f_term_def.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of themes.\n",
    "themes = f_term_def['theme']\n",
    "n_themes = len(set(themes))\n",
    "theme_labels = f_term_def['theme_label']\n",
    "n_theme_labels = len(set(theme_labels))\n",
    "print(f'There are {n_themes} unique themes and number {n_theme_labels} theme-labels in the f-terms CSV-file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for contradicting theme - theme_label definitions\n",
    "current_theme = ''\n",
    "current_label = ''\n",
    "n_missmatches = 0\n",
    "for i, row in f_term_def[['theme', 'theme_label']].iterrows():\n",
    "    theme, theme_label = row\n",
    "    if theme != current_theme and theme_label != current_label:\n",
    "        current_theme = theme\n",
    "        current_label = theme_label\n",
    "    elif theme != current_theme or theme_label != current_label:\n",
    "        if theme == current_theme and theme_label!=theme_label:\n",
    "            continue\n",
    "        n_missmatches += 1\n",
    "        print(f'''Double Match Found:\n",
    "        \n",
    "Theme= {theme}, expected theme= {current_theme}\n",
    "Label= {theme_label}, expected label = {current_label}\n",
    "''')\n",
    "        current_theme = theme\n",
    "        current_label = theme_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3ccc1",
   "metadata": {},
   "source": [
    "### There are several theme_labels attributed to more than one theme. \n",
    "### There are also theme_labels which are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all duplicate theme rows, but keeping duplicate theme_labels\n",
    "themes_and_labels = f_term_def[['theme', 'theme_label']]\n",
    "themes_and_labels = themes_and_labels.drop_duplicates(subset=['theme'])\n",
    "l_d = len(themes_and_labels)\n",
    "\n",
    "print(f'''Number of themes after dropping duplicates: {l_d}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567552d4",
   "metadata": {},
   "source": [
    "# Theme-Label Issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several small issues concerning the theme-labels\n",
    "\n",
    "# Inconsistent usage of large and lower case\n",
    "themes_and_labels.iloc[44:46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untranslated words:\n",
    "\n",
    "themes_and_labels.iloc[1874:1877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad theme descriptions with little differentiation\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "themes_and_labels.iloc[1952: 1956]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9370868",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = classes['theme']\n",
    "\n",
    "print(f'Problem: There are {len(set(t_df))} unique themes in the dataset-slice, but only {len(themes_and_labels)} themes in the dataset_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea395428",
   "metadata": {},
   "source": [
    "# Create (incomplete) F-Terms Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(raw_number):\n",
    "    if raw_number != raw_number:\n",
    "        return ''\n",
    "    return re.findall(r'\\d+', raw_number)[0]\n",
    "\n",
    "def extract_information_from_line(line):\n",
    "    theme = line[\"theme\"]\n",
    "    viewpoint = line[\"viewpoint\"]\n",
    "    number = line[\"number\"]\n",
    "        \n",
    "    # some numbers are nan, droppin these, removing viewpoint from number\n",
    "    exact_number = extract_number(number)\n",
    "        \n",
    "    # checking for nan in viewpoint\n",
    "    if viewpoint != viewpoint:\n",
    "        # number also contains view-point\n",
    "        if number != number:\n",
    "            number = ''\n",
    "        viewpoint = number\n",
    "        exact_number = ''\n",
    "            \n",
    "    theme_txt = str(line['theme_label']).lower()\n",
    "    viewpoint_txt = str(line['viewpoint_label']).lower()\n",
    "    number_txt = str(line['label']).lower()\n",
    "    \n",
    "    return [theme, theme_txt, viewpoint, viewpoint_txt, exact_number, number_txt]\n",
    "\n",
    "\n",
    "def clean_data(f_term_definitions):\n",
    "    \"\"\"\n",
    "    :f_term_definitions: pd.DataFrame: Loaded CSV file\n",
    "    \n",
    "    :return: Dataframe with cleand data\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_data = [extract_information_from_line(line) for i, line in f_term_definitions.iterrows()]\n",
    "    extracted_data = pd.DataFrame(extracted_data, columns=['theme', 'theme_label', 'viewpoint', 'viewpoint_label', 'number', 'label'])\n",
    "    return extracted_data\n",
    "    \n",
    "    \n",
    "clean_data = clean_data(f_term_def)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(clean_f_terms):\n",
    "    \"\"\"\n",
    "    Creates dictionarys to look up the description of themes, viewpoints and numbers for all f-terms in the f-term-definitions\n",
    "    file.\n",
    "    \"\"\"\n",
    "    theme_df = clean_f_terms[['theme', 'theme_label']]\n",
    "    theme_df = theme_df.drop_duplicates(subset=['theme'])\n",
    "    theme_dict = {line[0]: line[1] for i, line in theme_df.iterrows()}\n",
    "    \n",
    "    viewpoint_df = clean_f_terms[['theme', 'viewpoint', 'viewpoint_label' ]]\n",
    "    viewpoint_df['theme_viewpoint'] = viewpoint_df['theme'] + '/' + viewpoint_df['viewpoint']\n",
    "    viewpoint_df = viewpoint_df[['theme_viewpoint', 'viewpoint_label']]\n",
    "    viewpoint_dict = {line[0]: line[1] for i, line in viewpoint_df.iterrows()}\n",
    "       \n",
    "    number_df = clean_f_terms[['theme', 'viewpoint', 'number', 'label']]\n",
    "    number_df['key'] = number_df['theme'] + '/' + number_df['viewpoint'] + number_df['number']\n",
    "    number_df = number_df[['key', 'label']]\n",
    "    number_dict = {line[0]: line[1] for i, line in number_df.iterrows()}\n",
    "\n",
    "    \n",
    "    full_definitions_df = clean_f_terms[['theme', 'viewpoint', 'number', 'theme_label', 'viewpoint_label', 'label']]\n",
    "    full_definitions_df['key'] = full_definitions_df['theme'] + '/' + full_definitions_df['viewpoint'] + full_definitions_df['number']\n",
    "    full_definitions_df['description'] = full_definitions_df['theme_label'] + full_definitions_df['viewpoint_label'] + full_definitions_df['label']\n",
    "    full_definitions_df = full_definitions_df[['key', 'description']]\n",
    "    full_definitions_dict = {line[0]: line[1] for i, line in full_definitions_df.iterrows()}\n",
    "    \n",
    "    return {'theme_dict': theme_dict, \n",
    "            'viewpoint_dict': viewpoint_dict, \n",
    "            'number_dict': number_dict,\n",
    "            'full_definitions_dict': full_definitions_dict}\n",
    "    \n",
    "f_term_dicts = create_dicts(clean_data)\n",
    "\n",
    "theme_dict = f_term_dicts['theme_dict']\n",
    "viewpoint_dict = f_term_dicts['viewpoint_dict']\n",
    "number_dict = f_term_dicts['number_dict']\n",
    "full_description_dict = f_term_dicts['full_definitions_dict']\n",
    "\n",
    "\n",
    "with open(f'{pk_dump_dir}/agg_themes_descriptions.pk', 'wb') as f:\n",
    "    pk.dump(theme_dict, f)\n",
    "with open(f'{pk_dump_dir}/agg_viewpoints_descriptions.pk', 'wb') as f:\n",
    "    pk.dump(viewpoint_dict, f)\n",
    "with open(f'{pk_dump_dir}/agg_numbers_descriptions.pk', 'wb') as f:\n",
    "    pk.dump(number_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699cb47f-73d2-4457-87e6-6881bde7c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pk_dump_dir}/agg_full_descriptions.pk', 'wb') as f:\n",
    "    pk.dump(full_description_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pk_dump_dir}/agg_f_term_dict.pk', 'wb') as f:\n",
    "    pk.dump(number_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy(list_of_classes):\n",
    "    \"\"\"\n",
    "    This function converts a list of string-classes created by calling the split_f_term-function on all samples in the \n",
    "    dataset-splice to a numpyarray and also returns a dict of usefull dicts for recreating the original labels.\n",
    "    \n",
    "    :list_of_classes:  list of strings: List of classes produced by the split_f_term-function.\n",
    "    \n",
    "    :returns:          Numpy array: List of classes imbedded in an numpy array.\n",
    "    :returns:          dict of dicts: Dict with all dicts needed to recreate the orignial classes.\n",
    "    \"\"\"\n",
    "    theme_codes = set([c[0] for c in list_of_classes])\n",
    "    viewpoints = set([c[1] for c in list_of_classes])\n",
    "    digits = set([c[2] for c in list_of_classes])\n",
    "    additional_code = set([c[3] for c in list_of_classes])\n",
    "\n",
    "    theme_codes_dict = {x: i for i, x in enumerate(theme_codes)}\n",
    "    viewpoints_dict = {x: i for i, x in enumerate(viewpoints)}\n",
    "    digits_dict = {x: i for i, x in enumerate(digits)}\n",
    "    additional_code_dict = {x: i for i, x in enumerate(additional_code)}\n",
    "    \n",
    "    list_of_classes = [[theme_codes_dict[t], viewpoints_dict[v], digits_dict[d], additional_code_dict[a]] \n",
    "                       for t, v, d, a in list_of_classes]\n",
    "    \n",
    "    list_of_classes = np.array(list_of_classes)\n",
    "    \n",
    "    reversed_theme_codes_dict = {v: k for k, v in theme_codes_dict.items()}\n",
    "    reversed_viewpoints_dict = {v: k for k, v in viewpoints_dict.items()}\n",
    "    reversed_digits_dict = {v: k for k, v in digits_dict.items()}\n",
    "    reversed_additional_code_dict = {v: k for k, v in additional_code_dict.items()}\n",
    "    \n",
    "    dicts_dict = {'reversed_theme_codes_dict': reversed_theme_codes_dict,\n",
    "                  'reversed_viewpoints_dict': reversed_viewpoints_dict,\n",
    "                  'reversed_digits_dict': reversed_digits_dict,\n",
    "                  'reversed_additional_code_dict': reversed_additional_code_dict, \n",
    "                  \n",
    "                  'theme_codes_dict': theme_codes_dict,\n",
    "                  'viewpoints_dict': viewpoints_dict,\n",
    "                  'digits_dict': digits_dict,\n",
    "                  'additional_code_dict': additional_code_dict}\n",
    "    return list_of_classes, dicts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2555f-f2b6-4e4e-a3ea-fdf3b701bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_classes, dictofdicts = convert_to_numpy(classes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906d2a2",
   "metadata": {},
   "source": [
    "# Counting the Occurrence of Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme classes\n",
    "theme_keys, theme_occ = np.unique(np_classes[:,0], return_counts=True)\n",
    "ind = np.argsort(theme_occ)\n",
    "theme_keys, theme_occ = theme_keys[ind], theme_occ[ind]\n",
    "\n",
    "# Viewpoints classes\n",
    "view_keys, view_occ = np.unique(np_classes[:,1], return_counts=True)\n",
    "ind = np.argsort(view_occ)\n",
    "view_keys, view_occ = view_keys[ind], view_occ[ind]\n",
    "\n",
    "# Digits classes\n",
    "digits_keys, digits_occ = np.unique(np_classes[:, 2], return_counts=True)\n",
    "ind = np.argsort(digits_occ)\n",
    "digits_keys, digits_occ = digits_keys[ind], digits_occ[ind]\n",
    "\n",
    "# Additional Code classes\n",
    "code_keys, code_occ = np.unique(np_classes[:, 3], return_counts=True)\n",
    "ind = np.argsort(code_occ)\n",
    "code_keys, code_occ = code_keys[ind], code_occ[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87602cab-9469-4af1-8307-652a68fef2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
