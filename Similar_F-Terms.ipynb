{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1fcaa9-8ddb-491d-947b-f8db76be5706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 09:40:49.983463: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-28 09:40:50.003112: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 09:40:50.415087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"f35f976d-682a-4d79-9c6c-ba87f7c46709\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"f35f976d-682a-4d79-9c6c-ba87f7c46709\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"f35f976d-682a-4d79-9c6c-ba87f7c46709\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"f35f976d-682a-4d79-9c6c-ba87f7c46709\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"f35f976d-682a-4d79-9c6c-ba87f7c46709\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bokeh\n",
    "import time\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Dimension reduction algorithms\n",
    "#from cuml.manifold import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import distance\n",
    "# Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import export_png\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bbc2ccb-c2c0-42b4-8b7b-8060507c3809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at data/models/gal_125_seq_3/checkpoint-3 were not used when initializing OPTForCausalLM: ['score.weight']\n",
      "- This IS expected if you are initializing OPTForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OPTForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenizer from serialized instance!\n",
      "There are 378166 different F-Terms in the whole Dataset!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of Embedding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 141\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m    140\u001b[0m out_emb \u001b[38;5;241m=\u001b[39m normalize(out_emb)\n\u001b[0;32m--> 141\u001b[0m inp_emb \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m seq_class:\n\u001b[1;32m    143\u001b[0m     context_less_emb \u001b[38;5;241m=\u001b[39m [normalize(layer) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m context_less_emb]\n",
      "Cell \u001b[0;32mIn[8], line 136\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize\u001b[39m(tensor):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of Embedding"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_125_seq_3'\n",
    "checkpoint = 3\n",
    "# If True normalization is applied to the embeddings\n",
    "norm = True\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cuda:0'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "             \n",
    "model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "###########################\n",
    "# Loading the Tokenizer\n",
    "###########################\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print('Loaded Tokenizer from serialized instance!')    \n",
    "print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading Descriptions\n",
    "###########################\n",
    "with open(f'{dump_dir}/themes_descriptions.pk', 'rb') as f:\n",
    "    theme_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/viewpoints_descriptions.pk', 'rb') as f:\n",
    "    viewpoint_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/numbers_descriptions.pk', 'rb') as f:\n",
    "    number_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/full_descriptions.pk', 'rb') as f:\n",
    "        full_descriptions_dict = pk.load(f)\n",
    "\n",
    "\n",
    "###########################\n",
    "# Extracting the Embeddings\n",
    "###########################\n",
    "\n",
    "# Extracting the classification Head weights\n",
    "out_emb = model.get_output_embeddings()\n",
    "out_emb = next(out_emb.parameters()).to('cpu').detach().numpy()[2:]\n",
    "inp_emb = model.get_input_embeddings()\n",
    "if len(inp_emb.weight) < len(tokenizer):\n",
    "    seq_class = True\n",
    "else:\n",
    "    seq_class = False\n",
    "\n",
    "if not seq_class:\n",
    "    inp_emb = inp_emb(torch.arange(len(tokenizer))).to('cpu').detach().numpy()[50002:]\n",
    "    \n",
    "    # Extracting context less embeddings\n",
    "    if not os.path.isfile(f'{model_folder}/context_less_emb.pk'):\n",
    "        print('Calculating context less embeddings!')\n",
    "        context_less_emb = [[] for _ in range(13)]\n",
    "        for i in range(len(tokenizer)):\n",
    "            print(i, end='\\r')\n",
    "            out = model(input_ids= torch.tensor([[i]]), attention_mask = torch.tensor([[1]]), output_hidden_states=True)\n",
    "            \n",
    "            out = out.hidden_states\n",
    "            for i, k in enumerate(out):\n",
    "                context_less_emb[i].append(k.to('cpu').detach().numpy())\n",
    "        with open(f'{model_folder}/context_less_emb.pk', 'wb') as f:\n",
    "            pk.dump(context_less_emb, f)\n",
    "    else:\n",
    "        print('Loading context less embeddings from disk')\n",
    "        with open(f'{model_folder}/context_less_emb.pk', 'rb') as f:\n",
    "            context_less_emb = pk.load(f)\n",
    "    \n",
    "    # Combining context less embeddings of a layer to a single tensor\n",
    "    for i, layer in enumerate(context_less_emb):\n",
    "        layer = [e[0] for e in layer]\n",
    "        layer = np.concatenate(layer, 0)\n",
    "        context_less_emb[i] = layer\n",
    "\n",
    "\n",
    "## Normalizing the embeddings \n",
    "def normalize(tensor):\n",
    "    if norm:\n",
    "        return torch.nn.functional.normalize(torch.tensor(tensor), p=2).numpy()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "out_emb = normalize(out_emb)\n",
    "inp_emb = normalize(inp_emb)\n",
    "if not seq_class:\n",
    "    context_less_emb = [normalize(layer) for layer in context_less_emb]\n",
    "\n",
    "# Extracting the matching F_terms for the weights and creating lists with the defintions\n",
    "tokens = [tokenizer.decode(i) for i in range(len(tokenizer))]\n",
    "f_term_tokens = tokens[50002:]\n",
    "\n",
    "# Creating  a dict with f-Terms and their embedding vectors:\n",
    "\n",
    "out_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, out_emb)}\n",
    "if seq_class:\n",
    "    inp_emb_dict = {token[:-1]: vec for token, vec in zip(tokens[:50000], inp_emb)}\n",
    "else: \n",
    "    inp_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, inp_emb)}\n",
    "    \n",
    "# Creating Context Less Embedding Dicts\n",
    "if not seq_class:\n",
    "    context_less_dicts = []\n",
    "    for layer in context_less_emb:\n",
    "        context_less_dicts.append({token[:-1]: vec for token, vec in zip(tokens, layer)})\n",
    "\n",
    "\n",
    "####################################################\n",
    "# Detecting F-Term Pairs with Supposed Similarities\n",
    "####################################################\n",
    "df=pd.read_csv(\"data/f-terms.csv\", index_col=0)\n",
    "\n",
    "#subset with \"material\" in the viewpoint description\n",
    "df2=df[df.viewpoint_label.str.contains(\"material\",case=False, na=False)].copy()\n",
    "df2[\"vp\"]=df2.theme+\"/\"+df2.viewpoint\n",
    "df2[\"fterm\"]=df2.theme+\"/\"+df2.number\n",
    "\n",
    "#f-term descriptions are searched for the following materials\n",
    "materials_list=[\"metal\", \"wood\", \"concrete\", \"resin\", \"fiber\", \"wool\", \"sand\", \"cord\", \"copper\", \"iron\", \"silver\", \"gold\", \"lead\", \"glass\", \"stone\", \"titanium\", \"steel\", \"cement\", \"silicon\", \"polymer\", \"ceramics\"]\n",
    "materials_f_terms={}\n",
    "for material in materials_list:\n",
    "    materials_f_terms[material]=[df2[df2.label.str.contains(material,case=False, na=False)].vp.unique(), df2[df2.label.str.contains(material,case=False, na=False)].fterm.values]\n",
    "\n",
    "#pairs of two materials with fterms in same viewpoints are created\n",
    "material_combinations={}\n",
    "for mat1 in materials_list:\n",
    "    for mat2 in materials_list:\n",
    "        \n",
    "        if mat1==mat2: continue\n",
    "        if mat2+\"_\"+mat1 in materials_f_terms: continue\n",
    "        \n",
    "        #filtering shared f-terms with both materials in the label description\n",
    "        \n",
    "        unique_fterms_mat1 = list(set(materials_f_terms[mat1][1]) - set(materials_f_terms[mat2][1]))\n",
    "        unique_fterms_mat2 = list(set(materials_f_terms[mat2][1]) - set(materials_f_terms[mat1][1]))\n",
    "\n",
    "\n",
    "        vp_mat1= set([x[:8]for x in unique_fterms_mat1])\n",
    "        vp_mat2= set([x[:8]for x in unique_fterms_mat2])\n",
    "        shared_vp=[x for x in vp_mat1 if x in vp_mat2]\n",
    "        fterm_pairs=[]\n",
    "        for vp in shared_vp:\n",
    "            fterm_pairs.append([vp,\n",
    "                                [fterm for fterm in unique_fterms_mat1 if fterm.startswith(vp)],\n",
    "                                [fterm for fterm in unique_fterms_mat2 if fterm.startswith(vp)]])\n",
    "            \n",
    "            \n",
    "        material_combinations[mat1+\"_\"+mat2]=fterm_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7afe8-bd9a-4b82-a9a2-b7136eb2ba12",
   "metadata": {},
   "source": [
    "# Plotting Context Less Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6065d8c5-e19a-4b85-93ab-5c9fbb2fc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating or loading the TSNE Representations for all Tokens (Text and F-Terms)\n",
    "\n",
    "layer_tsne_reps = []\n",
    "for i, emb in enumerate(context_less_emb):\n",
    "    if os.path.isfile(f'{model_folder}/output_tsne_rep_{checkpoint}_layer{i}.pk'):\n",
    "        print('Loading TSNE representation for layer ', i)\n",
    "        with open(f'{model_folder}/output_tsne_rep_{checkpoint}_layer{i}.pk', 'rb') as f:\n",
    "            layer_tsne_reps.append(pk.load(f))\n",
    "    else:\n",
    "        print('Calculating TSNE representation for layer ', i)\n",
    "        tsne = TSNE(n_components=2, verbose=0, random_state=69) \n",
    "        tsne_rep = tsne.fit_transform(emb)\n",
    "        with open(f'{model_folder}/output_tsne_rep_{checkpoint}_layer{i}.pk', 'wb') as f:\n",
    "                pk.dump(tsne_rep, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb32b7-9858-424d-86f5-d7d5a3c86384",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 12\n",
    "colors = []\n",
    "for i in range(len(tokenizer)):\n",
    "    if i < 50000:\n",
    "        colors.append('blue')\n",
    "    else: \n",
    "        colors.append('red')\n",
    "        \n",
    "datasource = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = layer_tsne_reps[layer][:,0],\n",
    "            y = layer_tsne_reps[layer][:,1],\n",
    "            tokens = [tokenizer.decode(i) for i in range(len(tokenizer))],\n",
    "            colors = colors\n",
    "        )\n",
    "    )\n",
    "    \n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>Token:</b> @tokens</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title=f'Context less Embeddings Layer {layer}')\n",
    "        \n",
    "plot_tsne.circle('x', 'y', size=8, fill_color='colors', \n",
    "                 alpha=0.7, line_width=0, source=datasource, name=\"Tokens\")\n",
    "    \n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6076942-9784-4588-95c2-9d12968fa0dd",
   "metadata": {},
   "source": [
    "# Plotting the Material Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating vectors for all material combinations\n",
    "\n",
    "combination_desc = []\n",
    "vector_diffs = []\n",
    "vps = []\n",
    "vp_desc = []\n",
    "numbers = []\n",
    "desc = []\n",
    "color_ints = []\n",
    "\n",
    "for i, l  in enumerate(material_combinations.items()):\n",
    "    key, item = l\n",
    "    for viewpoint_comb in item:\n",
    "        viewpoint, mat_1, mat_2 = viewpoint_comb\n",
    "        for f_term_1 in mat_1:\n",
    "            for f_term_2 in mat_2:\n",
    "                try: \n",
    "                    vec_1 = inp_emb_dict[f_term_1]\n",
    "                    vec_2 = inp_emb_dict[f_term_2]\n",
    "                    vp_desc.append(viewpoint_dict[viewpoint])\n",
    "                    desc. append(number_dict[f_term_2] + ' - ' + number_dict[f_term_1])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                    \n",
    "                diff = vec_2 - vec_1\n",
    "                combination_desc.append(key)\n",
    "                vector_diffs.append(diff)\n",
    "                vps.append(viewpoint)\n",
    "                numbers.append(f_term_2 + ' - ' + f_term_1)\n",
    "                color_ints.append(i)\n",
    "        \n",
    "len(combination_desc), len(vp_desc), len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50612085",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_diffs = np.stack(vector_diffs, 0)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69) \n",
    "tsne_rep = tsne.fit_transform(vector_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c04a1-a3c0-42a8-b25c-33f4b11b0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_palette = bokeh.palettes.Turbo256\n",
    "color_palette = bokeh_palette\n",
    "\n",
    "colors = [color_palette[c%256] for c in color_ints]\n",
    "\n",
    "datasource_diff = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = tsne_rep[:,0],\n",
    "            y = tsne_rep[:,1],\n",
    "            combination =  combination_desc,\n",
    "            viewpoints=vps,\n",
    "            vp_desc = vp_desc,\n",
    "            numbers = numbers,\n",
    "            desc = desc,\n",
    "            colors = colors\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>Combination: </b>  @combination<br><b>Viewpoint:</b> @viewpoints<br><b>Viewpoint Description:</b> @vp_desc<br><b>Numbers:</b> @numbers<br><b>Description:</b> @desc</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Material Combintation Differences')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=8, fill_color='colors', \n",
    "                     alpha=0.7, line_width=0, source=datasource_diff, name=\"Material Combination Differences\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a1704-52a8-4ab0-b92e-08a3443ea570",
   "metadata": {},
   "source": [
    "# Comparing the Similarities of all Difference Vectors Within a Material Combination with the Cosine Similariteis when Compared to a Different Material Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9346ea7-eacc-4d1e-81f0-4a70a1940cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m layer_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m13\u001b[39m\n\u001b[1;32m      2\u001b[0m emb \u001b[38;5;241m=\u001b[39m out_emb_dict\n\u001b[0;32m----> 3\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_less_dicts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_n\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Dictionary that contains all differences and additional information for each material combination\u001b[39;00m\n\u001b[1;32m      5\u001b[0m differences \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "layer_n = 13\n",
    "emb = out_emb_dict\n",
    "emb = context_less_dicts[layer_n]\n",
    "# Dictionary that contains all differences and additional information for each material combination\n",
    "differences = {}\n",
    "# Huge dict, that contains the difference vectors as keys and \n",
    "diffs_dict = {}\n",
    "\n",
    "for i, combination in enumerate(material_combinations.items()):\n",
    "    key, item = combination\n",
    "    print(i, key, end='\\r')\n",
    "    # Checking if the reverse combination was already calculated\n",
    "    current_material_keys = []\n",
    "    for current_key in differences.keys():\n",
    "      \n",
    "        current_material_keys.append(current_key.split('_'))\n",
    "    combination_keys = key.split('_')\n",
    "    combination_keys.reverse()\n",
    "    if combination_keys in current_material_keys:\n",
    "        continue\n",
    "\n",
    "    # Calculating the differences and storing them in a list of dict\n",
    "    combinations_list = []\n",
    "    for viewpoint_comb in item:\n",
    "        viewpoint, mat_1, mat_2 = viewpoint_comb\n",
    "        for f_term_1 in mat_1:\n",
    "            for f_term_2 in mat_2:\n",
    "                try: \n",
    "                    vec_1 = emb[f_term_1]\n",
    "                    vec_2 = emb[f_term_2]\n",
    "                    diff = vec_2 - vec_1\n",
    "                    diff = normalize(np.array([diff]))[0]\n",
    "                    sample_dict = {}\n",
    "                    sample_dict['Vector'] = diff\n",
    "                    sample_dict['Viewpoint'] = viewpoint\n",
    "                    sample_dict['F-Term 1'] = f_term_1\n",
    "                    sample_dict['F-Term 2'] = f_term_2\n",
    "                    diffs_dict[sys.intern(str(np.sum(diff)))] = [viewpoint, f_term_1, f_term_2]\n",
    "                    combinations_list.append(sample_dict)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "    # Now the differences are computed and stored in a list of dicts\n",
    "            \n",
    "    differences[key] = combinations_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf7828-581c-4428-8bf5-64d85f29e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairing the vectors to compare\n",
    "\n",
    "max_comb = 100000000\n",
    "sqr_max_comb = int(max_comb**0.5)\n",
    "in_combination_comparisons = {}\n",
    "\n",
    "for i, [keys, samples] in enumerate(differences.items()):\n",
    "    # Adding the comparison of the material combination with itself to the comparisons\n",
    "    in_combination_comparisons[keys] = {'Vectors A': [], \n",
    "                                        'Vectors B': [],\n",
    "                                        'Viewpoints': [],\n",
    "                                        'F-Terms A': [],\n",
    "                                        'F-Terms B': []}\n",
    "    print(i, keys, len(samples), end = '\\r')\n",
    "    samples_2 = list(samples)\n",
    "    random.shuffle(samples_2)\n",
    "    s1 = 0 \n",
    "    while len(samples_2) > 0:\n",
    "        s1 += 1\n",
    "        s2 = 0\n",
    "        sample_a = samples_2.pop(0)\n",
    "        random.shuffle(samples_2)\n",
    "        for sample_b in samples_2:\n",
    "\n",
    "            if s2 == sqr_max_comb:\n",
    "                break\n",
    "            # Ignoring combinations with matching Viewpoints\n",
    "            if sample_a['Viewpoint'] == sample_b['Viewpoint']:\n",
    "                continue\n",
    "            in_combination_comparisons[keys]['Vectors A'].append(torch.tensor(np.array([sample_a['Vector']])))\n",
    "            in_combination_comparisons[keys]['Vectors B'].append(torch.tensor(np.array([sample_b['Vector']])))\n",
    "            in_combination_comparisons[keys]['Viewpoints'].append([sample_a['Viewpoint'], sample_a['Viewpoint']])\n",
    "            in_combination_comparisons[keys]['F-Terms A'].append([sample_a['F-Term 1'], sample_a['F-Term 2']])\n",
    "            in_combination_comparisons[keys]['F-Terms B'].append([sample_b['F-Term 1'], sample_b['F-Term 2']])\n",
    "            s2 += 1\n",
    "        if s1 == sqr_max_comb:\n",
    "            break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28435d-be6f-4b6c-9b1b-f3b2eaa3a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Cosine Similarities within all combinations\n",
    "cos = torch.nn.CosineSimilarity(dim =1)\n",
    "bad_keys = []\n",
    "min_comb = 2000\n",
    "\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    print(' '*1000, end='\\r')\n",
    "    print(i, key, end = '\\r')\n",
    "    try:\n",
    "        similarities = cos(torch.cat(combination_dict['Vectors A'], 0), torch.cat(combination_dict['Vectors B'], 0))\n",
    "        # Dropping combinations with low sample counts\n",
    "        if len(similarities) < min_comb:\n",
    "            print('Low number of samples in:', key, len(similarities), end='\\r')\n",
    "            bad_keys.append(key)\n",
    "            continue\n",
    "        combination_dict['Cosine Similarities'] = np.array(similarities)\n",
    "        # Creating a sorted index of the similariteis to compare the best ones with other combinations\n",
    "        combination_dict['Sort idx'] = np.argsort(np.array(similarities))[::-1]\n",
    "        \n",
    "        \n",
    "    except RuntimeError:\n",
    "        print(' '*1000, end='\\r')\n",
    "        print('Empty combination dictionary found, dropping  it!', key, end='\\r')\n",
    "        bad_keys.append(key)\n",
    "        continue\n",
    "    \n",
    "\n",
    "for key in bad_keys:\n",
    "    in_combination_comparisons.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd848b-3efd-4e49-acfb-eaca0e553ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best vectors \n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    top_100 = np.array(combination_dict['Cosine Similarities'])\n",
    "    idx = combination_dict['Sort idx'][:100]\n",
    "    top_100 = top_100[idx]\n",
    "    # Top 100 vectors a\n",
    "    top_vectors = np.concatenate(combination_dict['Vectors A'], 0)[idx]\n",
    "    top_vectors = np.concatenate([np.concatenate(combination_dict['Vectors B'], 0)[idx], top_vectors])\n",
    "    # dropping duplicates\n",
    "    top_vectors = np.unique(top_vectors, axis = 0)    \n",
    "    combination_dict['Best Vectors']  = top_vectors\n",
    "    print(' '*1000, end='\\r')\n",
    "    print(i, key, len(top_vectors), end='\\r')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afee99-93db-4384-b475-517adfc0bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the cosine similarities for out of combination comparisons\n",
    "\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    print(' '*1000, end='\\r')\n",
    "    print(i, key, end=' \\r')\n",
    "    \n",
    "    # Extracting all unique vectors from other material combinations, which do not share a material with the material \n",
    "    # Combination to be checked\n",
    "    all_vectors = []\n",
    "    material_1, material_2 = key.split('_')\n",
    "    for key2, samples in differences.items():\n",
    "        \n",
    "        materials2 = key2.split('_')\n",
    "        # Ignoring material combinations with matching materials \n",
    "        if material_1 in materials2 or material_2 in materials2:\n",
    "            continue\n",
    "        # Ignoring material combinations which do not appear in the in_combination_comparisons keys\n",
    "        if not key2 in [k for k in in_combination_comparisons.keys()]:\n",
    "            continue\n",
    "        \n",
    "        for sample_dict in samples:\n",
    "            all_vectors.append(sample_dict['Vector'])\n",
    "            \n",
    "    \n",
    "    all_vectors = np.stack(all_vectors, 0)\n",
    "    all_vectors = np.unique(all_vectors, axis=0)\n",
    "    \n",
    "    # vectors_a = vectors from top_vectors, vectors_b = vectors from other material combinations to compare the top_vectors with.\n",
    "    vectors_a , vectors_b = [], []\n",
    "    top_vectors = combination_dict['Best Vectors'][:50]\n",
    "\n",
    "    for top_vector in top_vectors:\n",
    "        vp, _ ,_ = diffs_dict[str(np.sum(top_vector))]\n",
    "        for vector in all_vectors:\n",
    "            vp2, _, _ = diffs_dict[str(np.sum(vector))]\n",
    "            # Skipping vectors with the same viewpoints\n",
    "            if vp2 == vp:\n",
    "                continue\n",
    "            vectors_a.append(top_vector)\n",
    "            vectors_b.append(vector)\n",
    "    \n",
    "    vectors_a = np.stack(vectors_a, 0)\n",
    "    vectors_b = np.stack(vectors_b, 0)\n",
    "   \n",
    "    similarities = cos(torch.tensor(vectors_a), torch.tensor(vectors_b)).numpy()\n",
    "    combination_dict['Out of Comb Simis'] = similarities\n",
    "    # Just to generate the Matrices of the top combinations\n",
    "    combination_dict['Out of Comb Vec A'] = vectors_a\n",
    "    combination_dict['Out of Comb Vec B'] = vectors_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299c040-d172-4952-a624-d8bbd3b96295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all out of comb similarities vs all in comb similarities (combined)\n",
    "all_in_comb = []\n",
    "all_out_comb = []\n",
    "for comb_dict in in_combination_comparisons.values():\n",
    "    all_in_comb.extend(comb_dict['Cosine Similarities'])\n",
    "    all_out_comb.extend(comb_dict['Out of Comb Simis'])\n",
    "\n",
    "all_in_comb = np.array(all_in_comb)\n",
    "all_out_comb = np.array(all_out_comb)\n",
    "mean_in = np.mean(all_in_comb)\n",
    "mean_out = np.mean(all_out_comb)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=[10, 10])\n",
    "\n",
    "\n",
    "axs.hist(all_in_comb, bins=500, alpha=0.75)\n",
    "axs.hist(all_out_comb, bins=500, alpha=0.75)\n",
    "axs.axvline(mean_in , color='red', linestyle='dashed', linewidth=1)\n",
    "axs.axvline(mean_out, color='magenta', linestyle='dashed', linewidth=1)\n",
    "axs.text(mean_in + 0.01, axs.get_ylim()[1] * 0.9, f'Mean: {mean_in:.6f}', color='grey')\n",
    "axs.text(mean_in + 0.01, axs.get_ylim()[1] * 0.925, f'Out of Comb Mean: {mean_out:.6f}', color='grey')\n",
    "axs.set_title(f'Layer {layer_n} Embedding 1 one 1 Similarities {model_name} {checkpoint}')\n",
    "plt.show()\n",
    "fig.savefig(f'{model_folder}/Layer {layer_n} Embeddings Similarities {model_name} {checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc7750-f413-4304-8d7a-8533b05bee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cosine similarities are  plotted\n",
    "\n",
    "fig, axs = plt.subplots(nrows = len(in_combination_comparisons), ncols=1, figsize=[10, 500])\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    print(i, end='\\r')\n",
    "    simis = np.array(combination_dict['Cosine Similarities'])\n",
    "    \n",
    "    axs[i].hist(simis, bins=100)\n",
    "    axs[i].set_title(key)\n",
    "    mean =  np.mean(simis)\n",
    "  \n",
    "    axs[i].axvline(mean, color='red', linestyle='dashed', linewidth=1)\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.9, f'Mean: {mean:.6f}', color='red')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.85, f'Number of Comparisons: {len(simis)}', color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec1f8d-f593-4b45-a72e-bb5b92f73ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the out of comb similarities\n",
    "\n",
    "fig, axs = plt.subplots(nrows = len(in_combination_comparisons), ncols=1, figsize=[10, 500])\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    print(i, end='\\r')\n",
    "    simis = np.array(combination_dict['Cosine Similarities'])\n",
    "    out_simis = np.array(combination_dict['Out of Comb Simis'])\n",
    "    # Pruning the out of comb similarities to the same length as the similarities\n",
    "    out_simis = out_simis[:len(simis)]\n",
    "    \n",
    "    mean_top_100 = np.mean(np.sort(simis)[-100:])\n",
    "    out_mean_top_100 = np.mean(np.sort(out_simis)[-100:])\n",
    "    \n",
    "    axs[i].hist(simis, bins=100, alpha=.75)\n",
    "    axs[i].hist(out_simis, bins=100, alpha=.75)\n",
    "    axs[i].set_title(key)\n",
    "    mean =  np.mean(simis)\n",
    "    mean_out = np.mean(out_simis)\n",
    "  \n",
    "    axs[i].axvline(mean, color='red', linestyle='dashed', linewidth=1)\n",
    "    axs[i].axvline(mean_out, color='magenta', linestyle='dashed', linewidth=1)\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.9, f'Mean: {mean:.6f}', color='grey')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.925, f'Out of Comb Mean: {mean_out:.6f}', \n",
    "                color='green' if mean > mean_out else 'red')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.875, f'Number of Comparisons: {len(simis)}', color='grey')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.85, f'Mean of Top 100: {mean_top_100:.6f}', color='grey')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.825, f'Mean of Top 100 Out of Comb: {out_mean_top_100:.6f}', \n",
    "                color='green' if mean_top_100 > out_mean_top_100 else 'red')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aae807-8090-4abd-a234-c2d92c055722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the best out of combination Similarities for all material combinations\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "\n",
    "    # Generating the Best Match List\n",
    "    out_simis = combination_dict['Out of Comb Simis']\n",
    "    top_vecs = combination_dict['Out of Comb Vec A']\n",
    "    out_vecs = combination_dict['Out of Comb Vec B']\n",
    "    \n",
    "    # Taking the highest n out of Combination Similarities\n",
    "    n = 20\n",
    "    idx = np.argsort(out_simis)[-n:]\n",
    "    \n",
    "    for id in idx:\n",
    "        top_vec = top_vecs[id]\n",
    "        out_vec = out_vecs[id]\n",
    "        \n",
    "        viewpoint_top, f_term1_top, f_term2_top = diffs_dict[str(np.sum(top_vec))]\n",
    "        viewpoint_out, f_term1_out, f_term2_out = diffs_dict[str(np.sum(out_vec))]\n",
    "\n",
    "        vp_top_desc = viewpoint_dict[viewpoint_top]\n",
    "        vp_out_desc = viewpoint_dict[viewpoint_out]\n",
    "\n",
    "        theme_top = theme_dict[viewpoint_top.split('/')[0]]\n",
    "        theme_out = theme_dict[viewpoint_out.split('/')[0]]\n",
    "\n",
    "        f_terms_top = [f_term1_top, f_term2_top]\n",
    "        f_terms_out = [f_term1_out, f_term2_out]\n",
    "\n",
    "        f_terms_top_desc = [number_dict[f_term1_top], number_dict[f_term2_top]]\n",
    "        f_terms_out_desc = [number_dict[f_term1_out], number_dict[f_term2_out]]\n",
    "        \n",
    "        row_dict = {}\n",
    "        row_dict['Material Combination'] = key.replace('_', '-')\n",
    "        row_dict['Cosine Similarity'] = out_simis[id]\n",
    "        row_dict['Viewpoint Top Vec'] = viewpoint_top\n",
    "        row_dict['Description Viewpoint Top Vec'] = vp_top_desc\n",
    "        row_dict['Theme Top Vec'] = theme_top\n",
    "        row_dict['Viewpoint Out'] = viewpoint_out\n",
    "        row_dict['Description Viewpoint Out'] = vp_out_desc\n",
    "        row_dict['F-Terms Top A-B'] = f_terms_top\n",
    "        row_dict['F-Terms Top A-B Description'] = f_terms_top_desc\n",
    "        row_dict['F-Terms Out A-B'] = f_terms_out\n",
    "        row_dict['F-Terms Out A-B Description'] = f_terms_out_desc\n",
    "        results.append(row_dict)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.to_excel(f'{model_folder}/Best out of combination similarities.xlsx')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffdac0-828d-46d4-9059-198dd2710b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the best out of combination Similarities for all material combinations\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "\n",
    "    # Generating the Best Match List\n",
    "    out_simis = combination_dict['Out of Comb Simis']\n",
    "    top_vecs = combination_dict['Out of Comb Vec A']\n",
    "    out_vecs = combination_dict['Out of Comb Vec B']\n",
    "    \n",
    "    # Taking the highest n out of Combination Similarities\n",
    "    n = 20\n",
    "    idx = np.argsort(out_simis)[:n]\n",
    "    \n",
    "    for id in idx:\n",
    "        top_vec = top_vecs[id]\n",
    "        out_vec = out_vecs[id]\n",
    "        \n",
    "        viewpoint_top, f_term1_top, f_term2_top = diffs_dict[str(np.sum(top_vec))]\n",
    "        viewpoint_out, f_term1_out, f_term2_out = diffs_dict[str(np.sum(out_vec))]\n",
    "\n",
    "        vp_top_desc = viewpoint_dict[viewpoint_top]\n",
    "        vp_out_desc = viewpoint_dict[viewpoint_out]\n",
    "\n",
    "        theme_top = theme_dict[viewpoint_top.split('/')[0]]\n",
    "        theme_out = theme_dict[viewpoint_out.split('/')[0]]\n",
    "\n",
    "        f_terms_top = [f_term1_top, f_term2_top]\n",
    "        f_terms_out = [f_term1_out, f_term2_out]\n",
    "\n",
    "        f_terms_top_desc = [number_dict[f_term1_top], number_dict[f_term2_top]]\n",
    "        f_terms_out_desc = [number_dict[f_term1_out], number_dict[f_term2_out]]\n",
    "        \n",
    "        row_dict = {}\n",
    "        row_dict['Material Combination'] = key.replace('_', '-')\n",
    "        row_dict['Cosine Similarity'] = out_simis[id]\n",
    "        row_dict['Viewpoint Top Vec'] = viewpoint_top\n",
    "        row_dict['Description Viewpoint Top Vec'] = vp_top_desc\n",
    "        row_dict['Theme Top Vec'] = theme_top\n",
    "        row_dict['Viewpoint Out'] = viewpoint_out\n",
    "        row_dict['Description Viewpoint Out'] = vp_out_desc\n",
    "        row_dict['F-Terms Top A-B'] = f_terms_top\n",
    "        row_dict['F-Terms Top A-B Description'] = f_terms_top_desc\n",
    "        row_dict['F-Terms Out A-B'] = f_terms_out\n",
    "        row_dict['F-Terms Out A-B Description'] = f_terms_out_desc\n",
    "        results.append(row_dict)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.to_excel(f'{model_folder}/Worst out of combination similarities.xlsx')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc85a4-744c-473f-bb46-ed38533a8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_max_simis = np.argsort(max_simis)\n",
    "sort_max_simis = sort_max_simis[::-1]\n",
    "top_50 = []\n",
    "for i, idx in enumerate(sort_max_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = max_simis_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = max_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    top_50.append(output_dict)\n",
    "\n",
    "top_50 = pd.DataFrame(top_50)\n",
    "top_50.to_csv('top_50.csv')\n",
    "top_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024de6e1-ea22-4504-a6ad-ef788402ac17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2cb227a-724e-41b5-9223-235c48dace58",
   "metadata": {},
   "source": [
    "# Averaging the Material Vectors to Cancel out Theme and Viewpoint Specific Attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6839871-83cb-4733-8bc7-840e8ba552c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = False\n",
    "em = out_emb_dict\n",
    "\n",
    "all_cos_base = []\n",
    "all_cos_add = []\n",
    "all_cos_diff = []\n",
    "all_dist_base = []\n",
    "all_dist_add = []\n",
    "all_dist_diff = []\n",
    "\n",
    "\n",
    "for i, (comb_name, samples) in enumerate(material_combinations.items()):\n",
    "    # Creating tensors with all embeddings fo a certain combination\n",
    "    emb_mat_1 = {}\n",
    "    emb_mat_2 = {}\n",
    "    viewpoints = []\n",
    "    for viewpoint, samples_mat_1, samples_mat_2 in samples:\n",
    "        emb_1_vp = []\n",
    "        emb_2_vp = []\n",
    "        # Extracting the vector embeddings for the first material\n",
    "        for f_term in samples_mat_1:\n",
    "            try:\n",
    "                vec = em[f_term]\n",
    "                emb_1_vp.append(vec)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        # Dropping the viewpoint if for the first material not a single vector is found\n",
    "        if len(emb_1_vp) == 0:\n",
    "            continue\n",
    "\n",
    "        # Extracting the vector embeddings for the second material \n",
    "        for f_term in samples_mat_2:\n",
    "            try: \n",
    "                vec = em[f_term]\n",
    "                emb_2_vp.append(vec)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        # Dropping the viewpoint if for the second material not a single vector is found\n",
    "        if len(emb_2_vp) == 0:\n",
    "            continue\n",
    "\n",
    "        # Adding the vector embeddings to the dicts\n",
    "        emb_mat_1[viewpoint] = emb_1_vp\n",
    "        emb_mat_2[viewpoint] = emb_2_vp \n",
    "        viewpoints.append(viewpoint)\n",
    "        \n",
    "    if len(emb_mat_1) < 2 or len(emb_mat_2) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Testing the predictive capabilities of diff mean vec (excluding the tested vector) + test_vector mat1 = test_vector mat2\n",
    "    for viewpoint in viewpoints:\n",
    "        # Creating copys of the dictionaries to be able to iterate over multiple viewpoints.\n",
    "        emb_mat_1c = emb_mat_1.copy()\n",
    "        emb_mat_2c = emb_mat_2.copy()\n",
    "        # Removing the test viepoint embeddigns from the dict copy and storing them seperately\n",
    "        test_vec_1 = emb_mat_1c.pop(viewpoint)[0]\n",
    "        test_vec_2 = emb_mat_2c.pop(viewpoint)[0]\n",
    "        # Creating the mean-vectors for material-1 and material-2\n",
    "        mean_emb1 = []\n",
    "        mean_emb2 = []\n",
    "        for vecs1 in emb_mat_1c.values():\n",
    "            mean_emb1.extend(vecs1)\n",
    "        for vecs2 in emb_mat_2c.values():\n",
    "            mean_emb2.extend(vecs2)\n",
    "        \n",
    "        mean_emb1 = np.stack(mean_emb1)\n",
    "        mean_emb2 = np.stack(mean_emb2)\n",
    "        mean_emb1 = np.mean(mean_emb1, 0)\n",
    "        mean_emb2 = np.mean(mean_emb2, 0)\n",
    "        diff = mean_emb2 - mean_emb1\n",
    "        #diff = torch.nn.functional.normalize(torch.tensor([diff]), p=2).numpy().flatten()\n",
    "\n",
    "        \n",
    "        test_vec_1_2 = np.array(test_vec_1) + diff\n",
    "        #test_vec_1_2 = torch.nn.functional.normalize(torch.tensor([test_vec_1_2]), p=2).numpy().flatten()\n",
    "\n",
    "        cosim_1_2 = np.dot(test_vec_1, test_vec_2) / (np.linalg.norm(test_vec_1) * np.linalg.norm(test_vec_2))\n",
    "        cosim_1_2_2 = np.dot(test_vec_1_2, test_vec_2) / (np.linalg.norm(test_vec_1_2) * np.linalg.norm(test_vec_2))\n",
    "        \n",
    "        dist_1_2 = distance.euclidean(test_vec_1, test_vec_2)\n",
    "        dist_1_2_2 = distance.euclidean(test_vec_1_2, test_vec_2)\n",
    "\n",
    "        all_cos_diff.append(cosim_1_2_2 - cosim_1_2)\n",
    "        all_dist_diff.append(dist_1_2_2 - dist_1_2)\n",
    "        all_cos_base.append(cosim_1_2)\n",
    "        all_cos_add.append(cosim_1_2_2)\n",
    "        all_dist_base.append(dist_1_2)\n",
    "        all_dist_add.append(dist_1_2_2)\n",
    "        if p:\n",
    "            print(f'''\n",
    "{comb_name}\n",
    "Test-Viewpoint: {viewpoint}\n",
    "        Cos (base):  {cosim_1_2:.5f}\n",
    "        Cos (add):   {cosim_1_2_2:.5f}\n",
    "        !!Cos (diff):  {cosim_1_2_2 - cosim_1_2:.5f}\n",
    "\n",
    "        Dist (base): {dist_1_2:.5f}\n",
    "        Dist (add):  {dist_1_2_2:.5f}\n",
    "        !!Dist (diff): {dist_1_2_2 - dist_1_2:.5f}\n",
    "\n",
    "Averages:\n",
    " Av Cos (base):      {np.mean(all_cos_base)}\n",
    " Av Cos (add):       {np.mean(all_cos_add)}\n",
    " !!Av Cos (diff):      {np.mean(all_cos_diff)}\n",
    " Av Dist (base):     {np.mean(all_dist_base)}\n",
    " Av Dist (add):      {np.mean(all_dist_add)}\n",
    " !!Av Dist (diff):     {np.mean(all_dist_diff)}\n",
    "--------------------------------------------------------------------------------------- \n",
    "        ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad2be2-bdcf-4d7b-8e8f-8ffde7d9b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_cos_diff, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1734a2-86fe-4d22-8c48-da2cdf5943bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_dist_diff, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e16a74-47f9-4179-b021-ba1d81ebe166",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_cos_base, alpha=0.5, label='base', bins=100)\n",
    "plt.hist(all_cos_add, alpha=0.5, label='Vector_addition', bins=100)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bc375-1549-4ad9-a43d-11cb744d75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_dist_base, alpha=0.5, label='base', bins=100)\n",
    "plt.hist(all_dist_add, alpha=0.5, label='add', bins=100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aabaa4-1001-4654-97ef-f8021576f811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd384e0b-cb36-47dd-bcc6-b97151701847",
   "metadata": {},
   "source": [
    "# Comparing the Cosine Similarities of all Vector Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd35fec-582f-49f0-abac-a52092c2b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating the 1 on 1 differences for a single material combination\n",
    "combination = material_combinations['metal_glass']\n",
    "descriptions = []\n",
    "diffs = []\n",
    "\n",
    "for viewpoint, fterms1, fterms2 in combination:\n",
    "    for fterm1 in fterms1:\n",
    "        for fterm2 in fterms2:\n",
    "            try: \n",
    "                vec1 = out_emb_dict[fterm1]\n",
    "                vec2 = out_emb_dict[fterm2]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            diffs.append(vec2- vec1)\n",
    "            descriptions.append([viewpoint, fterm1, fterm2])\n",
    "\n",
    "descriptions = np.array(descriptions)\n",
    "diffs = torch.tensor(diffs)\n",
    "\n",
    "# Generating the cosine similarities\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim =1)\n",
    "\n",
    "diffs = torch.tensor(normalize(diffs))\n",
    "diffs1 = diffs.unsqueeze(0)\n",
    "diffs2 = diffs.unsqueeze(1)\n",
    "desc1 = np.expand_dims(descriptions, 0)\n",
    "desc2 = np.expand_dims(descriptions, 1)\n",
    "diffs1 = diffs1.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "diffs2 = diffs2.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "desc1 = np.broadcast_to(desc1, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc1.shape[-1])\n",
    "desc2 = np.broadcast_to(desc2, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc2.shape[-1])\n",
    "\n",
    "desc = np.stack([desc1, desc2],-1)\n",
    "\n",
    "\n",
    "co_simis = cos(diffs1, diffs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86fa46-48e5-471c-9ed6-2a8c5cd94573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the similarities\n",
    "sort_out = torch.argsort(co_simis)\n",
    "\n",
    "# Removing 'wrong' similarities\n",
    "clean_sort_simis = []\n",
    "clean_desc = []\n",
    "\n",
    "for idx in sort_out:\n",
    "    simi = co_simis[idx]\n",
    "    des = desc[idx]\n",
    "    vp_a, vp_b = des[0]\n",
    "    if vp_a == vp_b:\n",
    "        continue\n",
    "    clean_desc.append(des)\n",
    "    clean_sort_simis.append(simi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ce122-b8a6-4e6d-8f8f-f17b4573aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clean_sort_simis, bins = 100)\n",
    "#plt.hist(co_simis, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614a769-dc55-4b3c-8c23-95c323da0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_simis = []\n",
    "min_simis = []\n",
    "max_50_simis = []\n",
    "min_50_simis = []\n",
    "max_50_desc = []\n",
    "min_50_desc = []\n",
    "mean_simis = []\n",
    "max_simis_desc = []\n",
    "min_simis_desc = []\n",
    "n_comb = []\n",
    "\n",
    "for i, pair  in enumerate(material_combinations.items()):\n",
    "    name, combination = pair\n",
    "    descriptions = []\n",
    "    diffs = []\n",
    "    \n",
    "    for viewpoint, fterms1, fterms2 in combination:\n",
    "        for fterm1 in fterms1:\n",
    "            for fterm2 in fterms2:\n",
    "                try: \n",
    "                    vec1 = out_emb_dict[fterm1]\n",
    "                    vec2 = out_emb_dict[fterm2]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "    \n",
    "                diffs.append(vec2- vec1)\n",
    "                descriptions.append([viewpoint, fterm1, fterm2, name])\n",
    "    \n",
    "    descriptions = np.array(descriptions)\n",
    "    diffs = torch.tensor(diffs)\n",
    "    \n",
    "    # Generating the cosine similarities\n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity(dim =1)\n",
    "    try:\n",
    "        diffs = torch.nn.functional.normalize(diffs, p=2, dim=1)\n",
    "    except IndexError:\n",
    "        continue\n",
    "        \n",
    "    diffs1 = diffs.unsqueeze(0)\n",
    "    diffs2 = diffs.unsqueeze(1)\n",
    "    desc1 = np.expand_dims(descriptions, 0)\n",
    "    desc2 = np.expand_dims(descriptions, 1)\n",
    "    diffs1 = diffs1.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "    diffs2 = diffs2.expand(diffs.shape[0], diffs.shape[0], diffs.shape[1]).flatten(end_dim=-2)\n",
    "    desc1 = np.broadcast_to(desc1, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc1.shape[-1])\n",
    "    desc2 = np.broadcast_to(desc2, [descriptions.shape[0], descriptions.shape[0], descriptions.shape[1]]).reshape(-1, desc2.shape[-1])\n",
    "    \n",
    "    desc = np.stack([desc1, desc2],-1)\n",
    "    \n",
    "    co_simis = cos(diffs1, diffs2)\n",
    "\n",
    "    # sorting the similarities\n",
    "    sort_out = torch.argsort(co_simis)\n",
    "    \n",
    "    # Removing 'wrong' similarities (cosine similarities where the theme is identical for both vector differences)\n",
    "    clean_sort_simis = []\n",
    "    clean_desc = []\n",
    "    \n",
    "    for idx in sort_out:\n",
    "        simi = co_simis[idx]\n",
    "        des = desc[idx]\n",
    "        vp_a, vp_b = des[0]\n",
    "        # Removing Comparisons within the same Viewpoint\n",
    "        if vp_a == vp_b:\n",
    "            continue\n",
    "        clean_desc.append(des)\n",
    "        clean_sort_simis.append(simi)\n",
    "    try:\n",
    "        \n",
    "        max_simis.append(clean_sort_simis[-1])\n",
    "        min_simis.append(clean_sort_simis[0])\n",
    "        max_50_simis.extend(clean_sort_simis[-50:])\n",
    "        min_50_simis.extend(clean_sort_simis[:50])\n",
    "        max_50_desc.extend(clean_desc[-50:])\n",
    "        min_50_desc.extend(clean_desc[:50])\n",
    "\n",
    "        mean_simis.append(np.mean(clean_sort_simis))\n",
    "        max_simis_desc.append(clean_desc[-1])\n",
    "        min_simis_desc.append(clean_desc[0])\n",
    "        n_comb.append(len(sort_out))\n",
    "    except IndexError:\n",
    "        continue\n",
    "\n",
    "    print(f'{i}  {name} max: {max_simis[-1]}, min: {min_simis[-1]}, mean: {mean_simis[-1]}, mean_max: {np.mean(max_simis)}, mean_min: {np.mean(min_simis)} mean_mean: {np.sum(np.array(mean_simis)*np.array(n_comb))/np.sum(n_comb)}', end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc289ebb-c1ab-492f-8dec-a9912d490836",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_max_simis = np.argsort(max_simis)\n",
    "sort_max_simis = sort_max_simis[::-1]\n",
    "top_50 = []\n",
    "for i, idx in enumerate(sort_max_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = max_simis_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = max_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    top_50.append(output_dict)\n",
    "\n",
    "top_50 = pd.DataFrame(top_50)\n",
    "top_50.to_csv('top_50.csv')\n",
    "top_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609a101-18e1-4057-9a0e-4aede5557673",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_min_simis = np.argsort(min_simis)\n",
    "\n",
    "worst_50 = []\n",
    "for i, idx in enumerate(sort_min_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = min_simis_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = min_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    worst_50.append(output_dict)\n",
    "\n",
    "worst_50 = pd.DataFrame(worst_50)\n",
    "worst_50.to_csv('worst_50.csv')\n",
    "worst_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7f5c6-0282-4620-9697-c529c6848006",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_max_simis = np.argsort(max_50_simis)\n",
    "sort_max_simis = sort_max_simis[::-1]\n",
    "top_50_overall = []\n",
    "for i, idx in enumerate(sort_max_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = max_50_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = max_50_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    top_50_overall.append(output_dict)\n",
    "\n",
    "top_50_overall = pd.DataFrame(top_50_overall)\n",
    "top_50_overall.to_csv('top_50_overall.csv')\n",
    "top_50_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c395b9-59d7-424d-82d0-0d691cc1ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_max_simis = np.argsort(min_50_simis)\n",
    "sort_max_simis = sort_max_simis[::-1]\n",
    "top_50_overall = []\n",
    "for i, idx in enumerate(sort_max_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = max_50_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = max_50_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    top_50_overall.append(output_dict)\n",
    "\n",
    "top_50_overall = pd.DataFrame(top_50_overall)\n",
    "top_50_overall.to_csv('top_50_overall.csv')\n",
    "top_50_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e6aff-fb9c-409a-8322-6898a09edf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_min_simis = np.argsort(min_50_simis)\n",
    "worst_50_overall = []\n",
    "\n",
    "for i, idx in enumerate(sort_min_simis[:50]):\n",
    "    output_dict = {}\n",
    "    desc = min_50_desc[idx]\n",
    "    output_dict['Combination'] = desc[-1][0].replace('_', '-')\n",
    "    output_dict['Cosine Similarity'] = min_50_simis[idx].item()\n",
    "    output_dict['Viewpoint 1'] = desc[0][0]\n",
    "    output_dict['Viewpoint 1 Description'] = viewpoint_dict[desc[0][0]]\n",
    "    output_dict['Theme 1'] = theme_dict[desc[0][0].split('/')[0]]\n",
    "    output_dict['Viewpoint 2'] = desc[0][1]\n",
    "    output_dict['Viewpoint 2 Description'] = viewpoint_dict[desc[0][1]]\n",
    "    output_dict['Theme 2'] = theme_dict[desc[0][1].split('/')[0]]\n",
    "    output_dict['F-Terms Vector 1'] = [desc[1][0].tolist(), desc[2][0].tolist()]\n",
    "    output_dict['F-Terms Vector 1 Description'] = 'Description a: ' + number_dict[desc[1][0]] + '     Description b: ' + number_dict[desc[2][0]]\n",
    "    output_dict['F-Terms Vector 2'] = [desc[1][1].tolist(), desc[2][1].tolist()]\n",
    "    output_dict['F-Terms Vector 2 Description'] = 'Description a: ' + number_dict[desc[1][1]] + '     Description b: ' + number_dict[desc[2][1]]\n",
    "    worst_50_overall.append(output_dict)\n",
    "\n",
    "worst_50_overall = pd.DataFrame(worst_50_overall)\n",
    "worst_50_overall.to_csv('worst_50_overall.csv')\n",
    "worst_50_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6df08-5ef9-4775-a783-925a692470bc",
   "metadata": {},
   "source": [
    "# Calculating all in Viewpoint Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21126887-cd85-49d2-91b7-a7e11f0194bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that lists all viewpoints and their f-terms\n",
    "vp_f_terms_dict = {}\n",
    "\n",
    "for f_term in f_term_tokens:\n",
    "    vp = f_term[:8]\n",
    "    try:\n",
    "        vp_f_terms_dict[vp].append(f_term)\n",
    "    except KeyError:\n",
    "        vp_f_terms_dict[vp] = [f_term]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f879a4-8118-482e-886e-8704feefa7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26493"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating all vectors that point from one f-term to another in the viewpoint\n",
    "all_combinations_dict = {}\n",
    "all_diffs_dict = {}\n",
    "for vp, vp_f_terms in vp_f_terms_dict.items():\n",
    "    for i, f_term1 in enumerate(vp_f_terms, start=1):\n",
    "        for f_term2 in vp_f_terms[i:]:\n",
    "            try:\n",
    "                all_combinations_dict[vp].append([f_term1, f_term2])\n",
    "                all_diffs_dict[vp].append(out_emb_dict[f_term1[:-1]] - out_emb_dict[f_term2[:-1]])\n",
    "            except KeyError:\n",
    "                all_combinations_dict[vp] = [[f_term1, f_term2]]\n",
    "                all_diffs_dict[vp] = [out_emb_dict[f_term1[:-1]] - out_emb_dict[f_term2[:-1]]]\n",
    "    # Combining all differences within a viewpoint to a tensor \n",
    "    if len(vp_f_terms) != 1:  # No differences computed if just one f-term is present\n",
    "        all_diffs_dict[vp] = np.stack(all_diffs_dict[vp], 0) \n",
    "\n",
    "len(all_diffs_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c1ab08-68ff-4913-9527-99fa4a6694bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2B002/AA plywoods\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'diffs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#desc, diffs = generate_diff_tensor('2B002/AA')\u001b[39;00m\n\u001b[1;32m     20\u001b[0m all_desc, all_diffs \u001b[38;5;241m=\u001b[39m generate_diff_tensor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mdiffs\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diffs' is not defined"
     ]
    }
   ],
   "source": [
    "for vp, deff in viewpoint_dict.items():\n",
    "    break\n",
    "print(vp, deff)\n",
    "\n",
    "def generate_diff_tensor(viewpoint):\n",
    "    \"\"\"\n",
    "    combines all differences and combinations except the queery viewpoint \"viewpoint\"\n",
    "    \"\"\"\n",
    "    desc = []\n",
    "    diffs = []\n",
    "    for vp, combinations in all_combinations_dict.items():\n",
    "        if vp == viewpoint:\n",
    "            continue\n",
    "        desc.extend(combinations)\n",
    "        diffs.append(all_diffs_dict[vp])\n",
    "    diffs = np.concatenate(diffs, 0)\n",
    "    return desc, diffs\n",
    "\n",
    "#desc, diffs = generate_diff_tensor('2B002/AA')\n",
    "all_desc, all_diffs = generate_diff_tensor('')\n",
    "all_diffs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ec3d9-687b-4a90-b180-44102e6cb22e",
   "metadata": {},
   "source": [
    "# Nearest Neighbors Search within all Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "545e7ad4-e9d0-4481-a1ed-0c107acd2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nbrs = NearestNeighbors(n_neighbors = 100, algorithm = 'ball_tree').fit(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11da57cf-73d5-427a-bd89-e8a59e59d9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('. . etchants', '. . . silicon dioxide (sio2)')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100042\n",
    "\n",
    "distances, indices = all_nbrs.kneighbors(np.array([all_diffs[n]]))\n",
    "\n",
    "number_dict[all_desc[n][0][:-1]], number_dict[all_desc[n][1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10810dc2-61d5-41b3-a0ae-9e6acb014f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance 0.0\n",
      "F-Terms . . etchants . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.7485085907481469\n",
      "F-Terms . . etchants . . . silicon nitride (sin)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.7901796234217575\n",
      "F-Terms . . metals or alloys . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8012583394822197\n",
      "F-Terms . . glass . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8057145926629278\n",
      "F-Terms . . . silicon monocrystals . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8071759502535041\n",
      "F-Terms . . silicon (si) . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8230082082072241\n",
      "F-Terms . . etchants . . doping materials or impurities\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8349739350656916\n",
      "F-Terms . . etchants . . organic polymers\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8496244996445845\n",
      "F-Terms . materials of which element main units are constituted (*) . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8534943347944154\n",
      "F-Terms . . etchants . . semiconductors other than silicon\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8542016847409861\n",
      "F-Terms . . etchants . . . polysilicon\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8546368464779363\n",
      "F-Terms . . etchants . other materials\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8583783749237682\n",
      "F-Terms . . . gallium arsenide (gaas) systems or other group iii-v semiconductors . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.863325355833251\n",
      "F-Terms . . etchants . . . amorphous silicon\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.88234487983846\n",
      "F-Terms . . metal compounds . . . silicon dioxide (sio2)\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8903891062551214\n",
      "F-Terms . . etchants . . metal compounds\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.8926921906326026\n",
      "F-Terms . . etchants . materials with which element main units are treated\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.9136630994118048\n",
      "F-Terms . . etchants . . fluids\n",
      "Viewpoints materials materials\n",
      "---------------------------\n",
      "Distance 0.9558065470291188\n",
      "F-Terms . annular support or cylindrical support e.g. hoses . . . jigs\n",
      "Viewpoints support device for work or article * support device for work or article *\n",
      "---------------------------\n",
      "Distance 0.9608526029162925\n",
      "F-Terms . passages or piping * . . throttles or orifices\n",
      "Viewpoints structure, arrangement, component, mounting corresponding to purposes or effects structure, arrangement, component, mounting corresponding to purposes or effects\n",
      "---------------------------\n",
      "Distance 0.9640171981424533\n",
      "F-Terms . . reaction-pressure chambers . return springs\n",
      "Viewpoints structures of hydraulic boosters that use liquids structures of hydraulic boosters that use liquids\n",
      "---------------------------\n",
      "Distance 0.9658552415645044\n",
      "F-Terms . . variable processing volume and rotation angle . . mechanical mechanisms\n",
      "Viewpoints control or adjustment control or adjustment\n",
      "---------------------------\n",
      "Distance 0.9677785303131277\n",
      "F-Terms . . improvement of the durability of pile structures . . improvement of end bearing capacity of pile\n",
      "Viewpoints purpose and effects of improvement purpose and effects of improvement\n",
      "---------------------------\n",
      "Distance 0.9692808825822511\n",
      "F-Terms . . . .\t binding agents . . . additives\n",
      "Viewpoints means for transfer of ink from the master sheet means for transfer of ink from the master sheet\n",
      "---------------------------\n",
      "Distance 0.9702704329475471\n",
      "F-Terms both axial and radial ...characterized by the connection structure with the inner conductor of the cable\n",
      "Viewpoints coaxial connector elements coaxial connector elements\n",
      "---------------------------\n",
      "Distance 0.9719369837463953\n",
      "F-Terms . . inflow systems . return springs\n",
      "Viewpoints structures of hydraulic boosters that use liquids structures of hydraulic boosters that use liquids\n",
      "---------------------------\n",
      "Distance 0.9727350850853305\n",
      "F-Terms . . . spool valves . . intermediate systems\n",
      "Viewpoints structures of hydraulic boosters that use liquids structures of hydraulic boosters that use liquids\n",
      "---------------------------\n",
      "Distance 0.9733760377815862\n",
      "F-Terms . . acylamino acids . . oso3 groups (sulfates) (quaternary salts to ac69)\n",
      "Viewpoints organic ingredients characterized by elements organic ingredients characterized by elements\n",
      "---------------------------\n",
      "Distance 0.9734971674407007\n",
      "F-Terms . . . installation and fixing of head-support elements by use of screws . . head-support structures other than elastic supports\n",
      "Viewpoints head-support means characterized by structure head-support means characterized by structure\n",
      "---------------------------\n",
      "Distance 0.9735596258090601\n",
      "F-Terms . . . . devices whereby filtration materials vary depending on the tank or chamber . . . devices consisting of partitioned chambers in water tanks\n",
      "Viewpoints managing of farming environments (4) filtration of feculence managing of farming environments (4) filtration of feculence\n",
      "---------------------------\n",
      "Distance 0.9740536006813352\n",
      "F-Terms . . constant pressure valves . . . return to the intake side\n",
      "Viewpoints lubricating oil supply means lubricating oil supply means\n",
      "---------------------------\n",
      "Distance 0.9745386583588319\n",
      "F-Terms . . synthetic organic fibers . . natural organic fibers\n",
      "Viewpoints products products\n",
      "---------------------------\n",
      "Distance 0.9745429567832711\n",
      "F-Terms . . liquids or steam . . nozzle mounting or arrangement\n",
      "Viewpoints fluid cleaning means fluid cleaning means\n",
      "---------------------------\n",
      "Distance 0.9756957770032203\n",
      "F-Terms . forming of magnetic-head cores (i.e., sintering and punching) . manufacturing of gaps\n",
      "Viewpoints parts to be manufactured parts to be manufactured\n",
      "---------------------------\n",
      "Distance 0.9758678830075297\n",
      "F-Terms . restoration and correction devices that are used without attachment to the body . . . structures divided into upper-body-receiving units and lower-body-receiving units\n",
      "Viewpoints devices for restoration and correction, and structure thereof devices for restoration and correction, and structure thereof\n",
      "---------------------------\n",
      "Distance 0.9760055716075924\n",
      "F-Terms . . . poppet valves . return springs\n",
      "Viewpoints structures of hydraulic boosters that use liquids structures of hydraulic boosters that use liquids\n",
      "---------------------------\n",
      "Distance 0.976055558040944\n",
      "F-Terms . . . control with characteristic sequencing . . . . movement of contact members by fluid pressure\n",
      "Viewpoints vehicle cleaning vehicle cleaning\n",
      "---------------------------\n",
      "Distance 0.976077993282523\n",
      "F-Terms pumps other\n",
      "Viewpoints detection and control of filtering machines this is about detection and control in filtering machines. detection and control of filtering machines this is about detection and control in filtering machines.\n",
      "---------------------------\n",
      "Distance 0.976161290229204\n",
      "F-Terms . passages or piping * . . treating condensed fuel\n",
      "Viewpoints structure, arrangement, component, mounting corresponding to purposes or effects structure, arrangement, component, mounting corresponding to purposes or effects\n",
      "---------------------------\n",
      "Distance 0.9764265631042697\n",
      "F-Terms message signal connection, control signal\n",
      "Viewpoints transmission signal (*) transmission signal (*)\n",
      "---------------------------\n",
      "Distance 0.9765193560599962\n",
      "F-Terms . . . liquid jets . . . tubes for drilling fluids\n",
      "Viewpoints use of fluid use of fluid\n",
      "---------------------------\n",
      "Distance 0.9773182175796833\n",
      "F-Terms . . characterized by materials . . . cross-section shapes\n",
      "Viewpoints details of field emission types details of field emission types\n",
      "---------------------------\n",
      "Distance 0.9774888153032423\n",
      "F-Terms . synthetic textiles . nonwoven cloth\n",
      "Viewpoints bedding or futon materials bedding or futon materials\n",
      "---------------------------\n",
      "Distance 0.9775479122175195\n",
      "F-Terms . . electromagnetic valves . . multiple-operation cylinders\n",
      "Viewpoints fluid-pressure mechanism fluid-pressure mechanism\n",
      "---------------------------\n",
      "Distance 0.9779490547747499\n",
      "F-Terms . . evaporation fuel inlet parts . space, chambers\n",
      "Viewpoints quantity of electricity (current, voltage, electric power, or the like) quantity of electricity (current, voltage, electric power, or the like)\n",
      "---------------------------\n",
      "Distance 0.9780277708082654\n",
      "F-Terms . . . actuator (the concrete contents is classified with hs) . . generation of the movement, assistance to maintenance\n",
      "Viewpoints characteristic about the assist function characteristic about the assist function\n",
      "---------------------------\n",
      "Distance 0.9782719858142918\n",
      "F-Terms . . . . . ring-shaped sealing members . . . . space between stator components adjacent each other in the axial direction\n",
      "Viewpoints sealing sealing\n",
      "---------------------------\n",
      "Distance 0.9783434970632475\n",
      "F-Terms . . having recesses and protrusions for connections . . connections between rods and driving mechanisms\n",
      "Viewpoints connection of drilling rig, pipe or casing connection of drilling rig, pipe or casing\n",
      "---------------------------\n",
      "Distance 0.9794517553623465\n",
      "F-Terms . . lead (pb) and organic lead . . arsenic (as) and selenium (se)\n",
      "Viewpoints substances to be removed substances to be removed\n",
      "---------------------------\n",
      "Distance 0.9795909958904436\n",
      "F-Terms . structures having flat parts with rotational axes differing from barrel axes . structures that have sound-emitting bodies\n",
      "Viewpoints artificial bait structures artificial bait structures\n",
      "---------------------------\n",
      "Distance 0.979639779323997\n",
      "F-Terms . . mud watering or agitation means or methods thereof . . . outer or inner screwed steel pipes\n",
      "Viewpoints placing device * placing device *\n",
      "---------------------------\n",
      "Distance 0.9796749316775607\n",
      "F-Terms . . characterized by materials . . characterized by materials\n",
      "Viewpoints details of field emission types details of field emission types\n",
      "---------------------------\n",
      "Distance 0.979705500174176\n",
      "F-Terms those with latches/registers other\n",
      "Viewpoints peripheral circuit peripheral circuit\n",
      "---------------------------\n",
      "Distance 0.9805793331372936\n",
      "F-Terms . forward bias variants . metal-insulator-metal (i.e., mim) type variants\n",
      "Viewpoints cold cathodes cold cathodes\n",
      "---------------------------\n",
      "Distance 0.9810913535903762\n",
      "F-Terms . . . devices for support during traction . . . structures divided into upper-body-receiving units and lower-body-receiving units\n",
      "Viewpoints devices for restoration and correction, and structure thereof devices for restoration and correction, and structure thereof\n",
      "---------------------------\n",
      "Distance 0.9813037960281364\n",
      "F-Terms . . brushes behind suction openings . . brushes in front of suction openings\n",
      "Viewpoints fixed brush mounting fixed brush mounting\n",
      "---------------------------\n",
      "Distance 0.9813493872065848\n",
      "F-Terms response signal connection, control signal\n",
      "Viewpoints transmission signal (*) transmission signal (*)\n",
      "---------------------------\n",
      "Distance 0.981646684458409\n",
      "F-Terms . . . hot water dispensing nozzles or shower nozzles . . valves\n",
      "Viewpoints structure structure\n",
      "---------------------------\n",
      "Distance 0.9818098064563042\n",
      "F-Terms . . . . semi-trailing arm types . . swing arm type including swing axle suspensions\n",
      "Viewpoints suspension type suspension type\n",
      "---------------------------\n",
      "Distance 0.9818217612757877\n",
      "F-Terms . . single-operation cylinders . . multiple-operation cylinders\n",
      "Viewpoints fluid-pressure mechanism fluid-pressure mechanism\n",
      "---------------------------\n",
      "Distance 0.9819766889661005\n",
      "F-Terms items with special characteristics in terms of materials and cross-sectional shape\n",
      "Viewpoints details of field emission type details of field emission type\n",
      "---------------------------\n",
      "Distance 0.9820000043826449\n",
      "F-Terms . means for reflecting vibration . solid horns for increasing vibration transmission\n",
      "Viewpoints details details\n",
      "---------------------------\n",
      "Distance 0.9821982118095349\n",
      "F-Terms . . . . fluorine-containing atmosphere . . . sintering atmosphere\n",
      "Viewpoints manufacture of preforms using chemical means manufacture of preforms using chemical means\n",
      "---------------------------\n",
      "Distance 0.9823474082647534\n",
      "F-Terms . . . organic substances . . dielectric films\n",
      "Viewpoints characteristic feature of coating structures or materials characteristic feature of coating structures or materials\n",
      "---------------------------\n",
      "Distance 0.9827216086770877\n",
      "F-Terms . main body, exterior ** . . inner walls\n",
      "Viewpoints main bodies of apparatuses,  interior and exterior thereof main bodies of apparatuses,  interior and exterior thereof\n",
      "---------------------------\n",
      "Distance 0.9828416985984477\n",
      "F-Terms . . spraying upward in a ring shape . spraying of mixed gases\n",
      "Viewpoints burner-head structures burner-head structures\n",
      "---------------------------\n",
      "Distance 0.982845279487341\n",
      "F-Terms . . . . on conveyers . . supporting objects to be cleaned\n",
      "Viewpoints handling of object to be cleaned handling of object to be cleaned\n",
      "---------------------------\n",
      "Distance 0.9829194135865045\n",
      "F-Terms collation, retrieval, and comparison of data storing, reading, and erasing in memory\n",
      "Viewpoints auxiliary functions for connection (*) auxiliary functions for connection (*)\n",
      "---------------------------\n",
      "Distance 0.9833840263886234\n",
      "F-Terms pumps flow rate, velocity\n",
      "Viewpoints detection and control of filtering machines this is about detection and control in filtering machines. detection and control of filtering machines this is about detection and control in filtering machines.\n",
      "---------------------------\n",
      "Distance 0.9835571882436067\n",
      "F-Terms . sealing members . return springs\n",
      "Viewpoints structures of hydraulic boosters that use liquids structures of hydraulic boosters that use liquids\n",
      "---------------------------\n",
      "Distance 0.9836351583385005\n",
      "F-Terms . . . . discs or columns . auxiliary equipment or devices *\n",
      "Viewpoints atomising apparatus by ultrasonic vibration atomising apparatus by ultrasonic vibration\n",
      "---------------------------\n",
      "Distance 0.9836777649282384\n",
      "F-Terms . . damper disks with characteristic drive disks . . . plate springs\n",
      "Viewpoints shock-absorbing devices for absorbing fluctuations in rotary torque shock-absorbing devices for absorbing fluctuations in rotary torque\n",
      "---------------------------\n",
      "Distance 0.9836821901808896\n",
      "F-Terms . . . pins or projections . . . . discontinuous flights\n",
      "Viewpoints detail of extrusion moulding detail of extrusion moulding\n",
      "---------------------------\n",
      "Distance 0.9837875249687356\n",
      "F-Terms early warming and cold water discharge safety and cleanliness\n",
      "Viewpoints shower unit shower unit\n",
      "---------------------------\n",
      "Distance 0.9838746091983861\n",
      "F-Terms dictionary for speech recognition word recognition\n",
      "Viewpoints voice input voice input\n",
      "---------------------------\n",
      "Distance 0.9839324513878945\n",
      "F-Terms . . . glass . . . . glass\n",
      "Viewpoints filler filler\n",
      "---------------------------\n",
      "Distance 0.9840218919569026\n",
      "F-Terms . . inorganic compounds, or inorganic compounds as the primary constituent . . . variants based on valve metals\n",
      "Viewpoints spraying of powdered materials spraying of powdered materials\n",
      "---------------------------\n",
      "Distance 0.9841945934876977\n",
      "F-Terms pumps startup of filtration and filtration operations\n",
      "Viewpoints detection and control of filtering machines this is about detection and control in filtering machines. detection and control of filtering machines this is about detection and control in filtering machines.\n",
      "---------------------------\n",
      "Distance 0.984335065394045\n",
      "F-Terms . . adjustment of the positions by sliding radially . . . . turret-type heads\n",
      "Viewpoints tools tools\n",
      "---------------------------\n",
      "Distance 0.9843832238143058\n",
      "F-Terms . low molecular compounds (i.e., non-polymers) . . olefins\n",
      "Viewpoints organic compounds organic compounds\n",
      "---------------------------\n",
      "Distance 0.9845535556082209\n",
      "F-Terms . . . poppet valves . positioning mechanisms\n",
      "Viewpoints structures of hydraulic boosters that use liquids structures of hydraulic boosters that use liquids\n",
      "---------------------------\n",
      "Distance 0.9847770765535008\n",
      "F-Terms . . fixing at least one part to the building, base, etc. . . assisting the object is holding with at least one part\n",
      "Viewpoints characteristic about the assist function characteristic about the assist function\n",
      "---------------------------\n",
      "Distance 0.9847895962624658\n",
      "F-Terms . cylindrical bodies . . . conical tubes and tapered tubes\n",
      "Viewpoints products products\n",
      "---------------------------\n",
      "Distance 0.9850524501375862\n",
      "F-Terms . partial water-immersion methods . direct-contact methods\n",
      "Viewpoints method for using ultrasonic wave method for using ultrasonic wave\n",
      "---------------------------\n",
      "Distance 0.9851007774999869\n",
      "F-Terms . . . gate valves . pipeline systems\n",
      "Viewpoints distinctive features of structure distinctive features of structure\n",
      "---------------------------\n",
      "Distance 0.9851036621414774\n",
      "F-Terms . . . . switching of all loads at once . shutdown of operation when operation is unnecessary\n",
      "Viewpoints controls* controls*\n",
      "---------------------------\n",
      "Distance 0.985114466974853\n",
      "F-Terms improved texture taste and aroma improvement\n",
      "Viewpoints intended functional characteristics (except, jb23-29) intended functional characteristics (except, jb23-29)\n",
      "---------------------------\n",
      "Distance 0.9852719873653427\n",
      "F-Terms . . . parameter names . . . parameter set values\n",
      "Viewpoints indication indication\n",
      "---------------------------\n",
      "Distance 0.9855053983028084\n",
      "F-Terms . . . polysaccharides or derivatives thereof . . carbohydrates or derivatives thereof\n",
      "Viewpoints active ingredient active ingredient\n",
      "---------------------------\n",
      "Distance 0.9856015790664167\n",
      "F-Terms . cursor controls . . changing processing according to cursor positions\n",
      "Viewpoints document display control document display control\n",
      "---------------------------\n",
      "Distance 0.9857048510979741\n",
      "F-Terms . arms, and attachment thereof (i.e., including arm-support springs) . gimbals\n",
      "Viewpoints support and pressure means for heads in general support and pressure means for heads in general\n",
      "---------------------------\n",
      "Distance 0.985742010194195\n",
      "F-Terms . silicon . nickel\n",
      "Viewpoints elements of addition compound elements of addition compound\n",
      "---------------------------\n",
      "Distance 0.9857694936584719\n",
      "F-Terms calling and dialing processing ... connection processing by subscriber class\n",
      "Viewpoints switching process switching process\n",
      "---------------------------\n",
      "Distance 0.9859814549273731\n",
      "F-Terms . . footrests . . . . characterised by foot portions\n",
      "Viewpoints structure structure\n",
      "---------------------------\n",
      "Distance 0.9860450534125421\n",
      "F-Terms . . common to heat recovery coils . . chemical agents *\n",
      "Viewpoints construction or arrangements * construction or arrangements *\n",
      "---------------------------\n",
      "Distance 0.9860547478863384\n",
      "F-Terms . . histograms . . processes for converting number of gradation\n",
      "Viewpoints preprocess for coding preprocess for coding\n",
      "---------------------------\n",
      "Distance 0.9860610291211193\n",
      "F-Terms . . . tic . . borides of iva - via group elements\n",
      "Viewpoints product compositions product compositions\n",
      "---------------------------\n",
      "Distance 0.9862016043623942\n",
      "F-Terms . . control of cleaning processes . . immersing in cleaning liquids\n",
      "Viewpoints characteristic feature of accessory characteristic feature of accessory\n",
      "---------------------------\n",
      "Distance 0.9862780129479264\n",
      "F-Terms . . . structures whereby the structure of the grasping element is defined . . . structures whereby the grasping elements can rotate or be attached and detached\n",
      "Viewpoints structures of mountings to hold fishing rods structures of mountings to hold fishing rods\n",
      "---------------------------\n",
      "Distance 0.9862870316009112\n",
      "F-Terms . . resins having polar functional groups that contain nitrogen . . . cross-linked polymers\n",
      "Viewpoints organic compounds organic compounds\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "for distance, ind in zip(distances[0], indices[0]):\n",
    "    print('Distance', distance)\n",
    "    f_term1, f_term2 = all_desc[ind]\n",
    "    print('F-Terms',number_dict[f_term1[:-1]], number_dict[f_term2[:-1]])\n",
    "    vp1, vp2 = f_term1[:8], f_term2[:8]\n",
    "    print('Viewpoints', viewpoint_dict[vp1])\n",
    "    print('---------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63bb1d8e-bcd0-4bcd-95d6-842d01abeb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (1, 100))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape, distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c69310c-112e-4523-8d49-9bdfd2b0ba21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021420637"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt(all_diffs[0]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da2bf4a1-9b02-4b61-9fb8-cc54ba31a6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033037875"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt(out_emb[0]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6ec4331-d709-4085-8acd-d131b6eeb76e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f_term2 \u001b[38;5;129;01min\u001b[39;00m f_terms[i:]:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m---> 16\u001b[0m         \u001b[43mtheme_diffs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtheme\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_emb_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf_term1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mout_emb_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf_term2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         theme_desc[theme]\u001b[38;5;241m.\u001b[39mappend([f_term1, f_term2])\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theme_f_term_dict = {}\n",
    "for f_term in f_term_tokens:\n",
    "    theme = f_term.split('/')[0]\n",
    "    try: \n",
    "        theme_f_term_dict[theme].append(f_term)\n",
    "    except KeyError:\n",
    "        theme_f_term_dict[theme] = [f_term]\n",
    "\n",
    "theme_diffs = {}\n",
    "theme_desc = {}\n",
    "\n",
    "for theme, f_terms in theme_f_term_dict.items():\n",
    "    for i, f_term1 in enumerate(f_terms, start=1):\n",
    "        for f_term2 in f_terms[i:]:\n",
    "            try: \n",
    "                theme_diffs[theme].append(out_emb_dict[f_term1[:-1]] -out_emb_dict[f_term2[:-1]])\n",
    "                theme_desc[theme].append([f_term1, f_term2])\n",
    "            except KeyError:\n",
    "                theme_diffs[theme] = [out_emb_dict[f_term1[:-1]] -out_emb_dict[f_term2[:-1]]]\n",
    "                theme_desc[theme] = [[f_term1, f_term2]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "616e0ea7-82e3-45d5-8a67-f5473cbffa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160765, 768), 0.025799876)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = 100000\n",
    "\n",
    "y = []\n",
    "\n",
    "for v in theme_diffs.values():#\n",
    "    y.extend(v)\n",
    "    if len(y) > x:\n",
    "        break\n",
    "    \n",
    "y = np.array(y)\n",
    "y = np.sqrt(y**2)\n",
    "\n",
    "y.shape, np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d372c8da-f49c-49c3-88da-891baef3f3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100107, 768), 0.025510918)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 100000\n",
    "\n",
    "y = []\n",
    "\n",
    "for v in all_diffs_dict.values():#\n",
    "    y.extend(v)\n",
    "    if len(y) > x:\n",
    "        break\n",
    "    \n",
    "y = np.array(y)\n",
    "y = np.sqrt(y**2)\n",
    "\n",
    "y.shape, np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43090e39-25f5-4fa3-85b1-17494a11c431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031408936"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt(out_emb**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165cbaad-149a-4e66-abbc-a92b97689de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
