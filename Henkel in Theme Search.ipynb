{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11acea55-4f6c-4fa4-944e-d36627f561b6",
   "metadata": {},
   "source": [
    "In this notebook I will analyze the patents about electrically debondable adhesives form Henkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8aa1a-8fa5-4f2e-9829-619906b65754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils_agg import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "import psutil\n",
    "from collections import Counter\n",
    "import itertools\n",
    "# Dimension reduction algorithms\n",
    "#from cuml.manifold import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from scipy.fft import fft, fftfreq\n",
    "# Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import export_png\n",
    "from bokeh.palettes import Viridis256, Category20\n",
    "from bokeh.transform import linear_cmap, factor_cmap\n",
    "from bokeh.colors import RGB\n",
    "\n",
    "# Huggingface\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54613b8-2a8e-4828-84a7-a76765492dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Loading the Henkel Patents\n",
    "###########################################################\n",
    "\n",
    "# Directories in which data important for the notebook is stored\n",
    "dump_dir = 'PK_DUMP'\n",
    "data_dir = 'data'\n",
    "\n",
    "# Loading the dataframes with the patents deemed most important for electrically debondable adhesives from Henkel\n",
    "henkel_patents = pd.read_csv(f'{data_dir}/Henkel_patente_patstat_docdb_families_abstract.csv', delimiter=',').reset_index(drop=True)\n",
    "henkel_orbit = pd.read_csv(f'{data_dir}/Henkel_Orbit_Suche_Patstat_Export.csv', delimiter=',')\n",
    "\n",
    "# Filtering the Samples which contain F-Terms\n",
    "henkel_filtered = henkel_patents[henkel_patents['fterms'].notna()]\n",
    "henkel_filtered = henkel_filtered.reset_index(drop=True)\n",
    "\n",
    "orbit_filtered = henkel_orbit[henkel_orbit['fterms'].notna()]\n",
    "orbit_filtered = orbit_filtered.reset_index(drop=True)\n",
    "\n",
    "print(f\"There are {len(henkel_patents['doc_db_family_id'].unique())} unique patents in the Henkel dataset, only {len(henkel_filtered['doc_db_family_id'].unique())} of them contain F-Terms.\")\n",
    "\n",
    "# extracting all f-terms form the datasets\n",
    "fterms_henkel = [fterm[:10] for fterms in henkel_filtered['fterms'] for fterm in fterms.split(',')]\n",
    "fterms_orbit = [fterm[:10] for fterms in orbit_filtered['fterms'] for fterm in fterms.split(',')]\n",
    "\n",
    "# Aggreagting the F-Terms\n",
    "with open(f'{dump_dir}/aggregation_dict_new.pk', 'rb') as f:\n",
    "    aggregation_dict = pk.load(f)\n",
    "\n",
    "def aggregate(f_term):\n",
    "    try:\n",
    "        return aggregation_dict[f_term]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "fterms_henkel_agg = [aggregate(fterm) for fterm in fterms_henkel if aggregate(fterm) is not None]\n",
    "fterms_orbit_agg = [aggregate(fterm) for fterm in fterms_orbit if aggregate(fterm) is not None]\n",
    "\n",
    "# Counting the occurrences of the henkel and orbit fterms\n",
    "counter_henkel = Counter(fterms_henkel_agg)\n",
    "counter_orbit = Counter(fterms_orbit_agg)\n",
    "\n",
    "# Structuring the henkel F-Terms\n",
    "henkel_dict = {}\n",
    "for fterm in counter_henkel.keys():\n",
    "    theme = fterm[:5]\n",
    "    try:\n",
    "        _ = henkel_dict[theme]\n",
    "    except KeyError:\n",
    "        henkel_dict[theme] = {}\n",
    "\n",
    "    vp = fterm[:8]\n",
    "    try: \n",
    "        henkel_dict[theme][vp].append(fterm)\n",
    "    except KeyError:\n",
    "        henkel_dict[theme][vp] = [fterm]\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Loading the Model\n",
    "################################################################\n",
    "\n",
    "model_name = 'gal_125_new_1'\n",
    "checkpoint = int(2*86515)\n",
    "# If True normalization is applied to the embeddings\n",
    "norm = True\n",
    "context_less = False\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "if model_name.split('_')[1] == '125':\n",
    "    base_model_name = 'mini'\n",
    "elif model_name.split('_')[1] == '1300':\n",
    "    base_model_name = 'base'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cpu'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "###########################\n",
    "# Loading the Tokenizer\n",
    "###########################\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print('Loaded Tokenizer from serialized instance!')    \n",
    "print(f'There are {n_f_terms:,} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading Descriptions\n",
    "###########################\n",
    "with open(f'{dump_dir}/agg_themes_descriptions_new.pk', 'rb') as f:\n",
    "    theme_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_viewpoints_descriptions_new.pk', 'rb') as f:\n",
    "    viewpoint_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_numbers_descriptions_new.pk', 'rb') as f:\n",
    "    number_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_full_descriptions_new.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)\n",
    "\n",
    "\n",
    "###########################\n",
    "# Extracting the Embeddings\n",
    "###########################\n",
    "\n",
    "# Extracting the classification Head weights\n",
    "inp_emb = model.get_input_embeddings()\n",
    "\n",
    "\n",
    "#Embeddings if the model is not a sequence classification model\n",
    "out_emb = model.get_output_embeddings()\n",
    "out_emb = next(out_emb.parameters()).to('cpu').detach().numpy()[2:]\n",
    "inp_emb = inp_emb(torch.arange(len(tokenizer))).to('cpu').detach().numpy()[50002:]\n",
    "\n",
    "if context_less:\n",
    "    # Extracting context less embeddings\n",
    "    if not os.path.isfile(f'{model_folder}/context_less_emb{checkpoint}.pk'):\n",
    "        print('Calculating context less embeddings!')\n",
    "        context_less_emb = [[] for _ in range(len([1 for _ in model.parameters()]))]\n",
    "        for i in range(len(tokenizer)):\n",
    "            print(i, end='\\r')\n",
    "            out = model(input_ids= torch.tensor([[i]]), attention_mask = torch.tensor([[1]]), output_hidden_states=True)\n",
    "                \n",
    "            out = out.hidden_states\n",
    "            for i, k in enumerate(out):\n",
    "                context_less_emb[i].append(k.to('cpu').detach().numpy())\n",
    "        with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'wb') as f:\n",
    "            pk.dump(context_less_emb, f)\n",
    "    else:\n",
    "        print('Loading context less embeddings from disk')\n",
    "        with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'rb') as f:\n",
    "            context_less_emb = pk.load(f)\n",
    "        \n",
    "    # Combining context less embeddings of a layer to a single tensor\n",
    "    for i, layer in enumerate(context_less_emb):\n",
    "        layer = [e[0] for e in layer]\n",
    "        layer = np.concatenate(layer, 0)\n",
    "        context_less_emb[i] = layer\n",
    "\n",
    "# Normalizing the embeddings \n",
    "def normalize(tensor):\n",
    "    if norm:\n",
    "        return torch.nn.functional.normalize(torch.tensor(tensor), p=2).numpy()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "out_emb = normalize(out_emb)\n",
    "inp_emb = normalize(inp_emb)\n",
    "\n",
    "if context_less:\n",
    "    context_less_emb = [normalize(layer) for layer in context_less_emb]\n",
    "\n",
    "# Extracting the matching F_terms for the weights and creating lists with the defintions\n",
    "tokens = [tokenizer.decode(i) for i in range(len(tokenizer))]\n",
    "f_term_tokens = tokens[50002:]\n",
    "\n",
    "# Creating  a dict with f-Terms and their embedding vectors:\n",
    "out_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, out_emb)}\n",
    "ft_emb_dict = {key: np.abs(fft(value)) for key, value in out_emb_dict.items()}\n",
    "inp_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, inp_emb)}\n",
    "    \n",
    "# Creating Context Less Embedding Dicts\n",
    "if context_less:\n",
    "    context_less_dicts = []\n",
    "    for layer in context_less_emb:\n",
    "        context_less_dicts.append({token[:-1]: vec for token, vec in zip(tokens, layer)})\n",
    "\n",
    "# Extracting the emb_dim\n",
    "for e in out_emb_dict.values():\n",
    "    break\n",
    "emb_dim = e.shape[-1]\n",
    "print('Embedding Dimension: ', emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148b3e2-564b-4a0f-a47c-a7c98cbb8dd6",
   "metadata": {},
   "source": [
    "# Search in Theme Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939424e-4aa4-40df-95d4-d4cdf4c7d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fterm_dict():\n",
    "    \"\"\"\n",
    "    Creates a hirachical dict with all F-Terms ordered by Theme -> Viewpoint > F-Terms\n",
    "    \"\"\"\n",
    "    f_term_dict = {}\n",
    "    for f_term in f_term_tokens:\n",
    "        theme = f_term.split('/')[0]\n",
    "        vp = f_term[:8]\n",
    "        # Creating a dict entry for the theme\n",
    "        try: \n",
    "            _ = f_term_dict[theme]\n",
    "        except KeyError:\n",
    "            f_term_dict[theme] = {}\n",
    "    \n",
    "        # Creating a dict entry for the viewpoint\n",
    "    \n",
    "        try:\n",
    "            # The first dict call will def. work the second may work if the vp-dict entry \n",
    "            # was already made. If it works the theme is appended to the viewpoint dict\n",
    "            f_term_dict[theme][vp].append(f_term)\n",
    "        except KeyError:\n",
    "            f_term_dict[theme][vp] = []\n",
    "\n",
    "    return f_term_dict\n",
    "\n",
    "\n",
    "def extract_theme_fterms(t_dict):\n",
    "    fterms = []\n",
    "    for vp_list in t_dict.values():\n",
    "        fterms.extend(vp_list)\n",
    "    return fterms        \n",
    "\n",
    "\n",
    "def create_all_diffs(emb=out_emb_dict):\n",
    "    \"\"\"\n",
    "    Creates all possible in viewpoint combinations and returns them a s a \n",
    "    \"\"\"\n",
    "    all_diffs = {}\n",
    "    # Calculating the needed combinations\n",
    "    f_term_dict = create_fterm_dict()\n",
    "    for i, (theme, t_dict) in enumerate(f_term_dict.items()):\n",
    "        print(i, theme, end='\\r')\n",
    "        all_diffs[theme] = {}\n",
    "        fterms = extract_theme_fterms(t_dict)\n",
    "        combinations = itertools.combinations(fterms, 2)\n",
    "        for fterm1, fterm2 in combinations:\n",
    "            viewpoint = fterm1[:8]\n",
    "            diff = emb[fterm2[:10]] - emb[fterm1[:10]]\n",
    "            diff = normalize(np.array([diff]))\n",
    "            try:\n",
    "                all_diffs[theme][viewpoint][(fterm1, fterm2)] = diff\n",
    "            except KeyError:\n",
    "                all_diffs[theme][viewpoint] = {}\n",
    "                all_diffs[theme][viewpoint][(fterm1, fterm2)] = diff\n",
    "    return all_diffs\n",
    "    \n",
    "\n",
    "def create_diffs_tensor(block_theme, all_diffs):\n",
    "    \"\"\"\n",
    "    Creates a tensor with all diffs, which do not contain the block theme.\n",
    "    Additionaly also returns a list with all comination descriptions\n",
    "    \"\"\"\n",
    "    # Filtering out the unwanted theme\n",
    "    diffs = {theme: t_dict for theme, t_dict in all_diffs.items() if theme != block_theme}\n",
    "    out_diffs = []\n",
    "    out_desc = []\n",
    "    for i, (theme, t_dict) in enumerate(diffs.items()):\n",
    "        #print(i, theme, end='\\r')\n",
    "        for vp_dict in t_dict.values():\n",
    "            for comb, diff in vp_dict.items():\n",
    "                out_desc.append(comb)\n",
    "                out_diffs.append(diff)\n",
    "\n",
    "    out_diffs = np.array(out_diffs)\n",
    "    out_diffs = out_diffs.squeeze(1)\n",
    "    return out_diffs, out_desc\n",
    "\n",
    "\n",
    "def search_cos(query_vec, vecs):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarities between all_vecs and the query_vec \n",
    "    \"\"\"\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    # Creating an array of query vectors, with the same number of vectors as the all_vecs array.\n",
    "    query = np.concatenate([query_vec for _ in vecs], 0)\n",
    "\n",
    "    vecs = torch.tensor(vecs, requires_grad=False)\n",
    "    query = torch.tensor(query, requires_grad=False)\n",
    "    simis = cos(vecs, query)\n",
    "    return simis\n",
    "\n",
    "def search_in_all(fterm1, fterm2, all_diffs, step=100):\n",
    "    \"\"\"\n",
    "    This function computes the most similar combinations in steps of 'step' themes at a time\n",
    "    \"\"\"\n",
    "    \n",
    "    theme = fterm1[:5]\n",
    "    query = out_emb_dict[fterm2[:10]] - out_emb_dict[fterm1[:10]]\n",
    "    query = normalize(np.array([query]))\n",
    "    iterations = -(len(all_diffs)//-step)    # Ceiling devision\n",
    "    simis = []                               # Stores all computed cosine similarities\n",
    "    descs = []                               # Stores all combination descriptions (fterm1, fterm2)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        diffs_chunk = dict([d for d in all_diffs.items()][i*step: (i+1)*step])\n",
    "        search_diffs, search_descs = create_diffs_tensor(theme, diffs_chunk)\n",
    "        simis_chunk = search_cos(query, search_diffs)\n",
    "        simis.extend(simis_chunk)\n",
    "        descs.extend(search_descs)\n",
    "\n",
    "    # Sorting for highest similarity\n",
    "    idx = np.argsort(simis)[::-1]\n",
    "    simis = [simis[i] for i in idx]\n",
    "    descs = [descs[i] for i in idx]\n",
    "    return simis,  descs,  idx\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f132dc5-0f4b-4624-acbb-9128b08f3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(f'{model_folder}/all_theme_diffs.pk'):\n",
    "    with open(f'{model_folder}/all_theme_diffs.pk', 'rb') as f:\n",
    "        all_diffs = pk.load(f)\n",
    "\n",
    "else:\n",
    "    all_diffs = create_all_diffs()\n",
    "    with open(f'{model_folder}/all_theme_diffs.pk', 'wb') as f:\n",
    "        pk.dump(all_diffs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8e9cc-f239-462a-8b07-f42a1b740722",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Searching for promising Henkel Combinations  (In Theme)\n",
    "###############################################################\n",
    "\n",
    "\n",
    "search_combination = ['4F100/JL11', '4F100/JG01']  # (adhesiveness, conductivity being properties or funcitons)\n",
    "#search_combination = ['4J040/JB09', '4J040/PA21']  # (pressure sensitive adhesive or adhesive types, use of adhesive characterised by specific shapess of functions)\n",
    "#search_combination = ['4J004/CC02', '4J004/CA07']  # (foil like, inorganic materials)\n",
    "\n",
    "simis, descs, idx = search_in_all(*search_combination, all_diffs, step=500)\n",
    "results = descs[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd343ad-46c0-4336-9389-a7b87a0ad97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(adhesiveness, conductivity being properties or funcitons)')\n",
    "for fterm1, fterm2 in results:\n",
    "    theme = theme_dict[fterm1[:5]]\n",
    "    vp1 = viewpoint_dict[fterm1[:8]]\n",
    "    vp2 = viewpoint_dict[fterm2[:8]]\n",
    "    n1 = number_dict[fterm1[:10]]\n",
    "    n2 = number_dict[fterm2[:10]]\n",
    "\n",
    "    print(f'''    \n",
    "Theme: {theme}\n",
    "vp1: {vp1}     vp2: {vp2}\n",
    "n1: {n1}       n2:{n2}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d443af-47ab-4ddd-88d3-cf4867a604d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting uniqe F-terms from the results\n",
    "found_f_terms = []\n",
    "[found_f_terms.extend(comb) for comb in results]\n",
    "found_f_terms = list(set(found_f_terms))\n",
    "\n",
    "# Embedding the found F-Terms\n",
    "hits_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in found_f_terms]))\n",
    "\n",
    "orbit_emb = []\n",
    "o_ft = []\n",
    "for fterm in counter_orbit.keys(): \n",
    "    try:\n",
    "        orbit_emb.append(out_emb_dict[fterm])\n",
    "        o_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "orbit_emb = np.array(orbit_emb)\n",
    "\n",
    "henkel_emb = []\n",
    "h_ft = []\n",
    "for fterm in search_combination:\n",
    "    try:\n",
    "        henkel_emb.append(out_emb_dict[fterm])\n",
    "        h_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "henkel_emb = np.array(henkel_emb)\n",
    "\n",
    "all_emb = np.concatenate([orbit_emb, henkel_emb, hits_emb], 0)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69)\n",
    "rep = tsne.fit_transform(all_emb)\n",
    "\n",
    "datasource_henkel = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft):len(o_ft) + len(h_ft),0],\n",
    "            y = rep[len(o_ft):len(o_ft) + len(h_ft),1],\n",
    "            fterms = h_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in h_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in h_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in h_ft]))\n",
    "\n",
    "datasource_orbit = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[:len(o_ft), 0],\n",
    "            y = rep[:len(o_ft), 1],\n",
    "            fterms = o_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in o_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in o_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in o_ft]))\n",
    "\n",
    "datasource_emb_search = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft) + len(h_ft):, 0],\n",
    "            y = rep[len(o_ft) + len(h_ft):, 1],\n",
    "            fterms = found_f_terms,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in found_f_terms],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in found_f_terms],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in found_f_terms]))\n",
    "\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>F-Term:</b> @fterms<br><b>Theme:</b> @themes<br><b>Viewpoint:</b> @viewpoints<br><b>Number:</b> @numbers<br><b>Query:</b> @found_by</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Henkel and Orbit Embeddings')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=10, fill_color=RGB(250, 50, 75), alpha=1, line_width=0, source=datasource_henkel, name=\"Henkel Embeddings\")\n",
    "plot_tsne.square('x', 'y', size=7, fill_color=RGB(50, 75, 250), alpha=0.2, line_width=0, source=datasource_orbit, name=\"Orbit Embeddings\")\n",
    "plot_tsne.triangle('x', 'y', size=7, fill_color=RGB(75, 250, 50), alpha=1, line_width=0, source=datasource_emb_search, name=\"Cos Similar Embeddings\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be44417-e902-4b9d-9d17-3dd403901fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
