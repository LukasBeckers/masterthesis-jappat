{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11acea55-4f6c-4fa4-944e-d36627f561b6",
   "metadata": {},
   "source": [
    "In this notebook I will analyze the patents about electrically debondable adhesives form Henkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8aa1a-8fa5-4f2e-9829-619906b65754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils_agg import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "import psutil\n",
    "from collections import Counter\n",
    "import itertools\n",
    "# Dimension reduction algorithms\n",
    "#from cuml.manifold import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from scipy.fft import fft, fftfreq\n",
    "# Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import export_png\n",
    "from bokeh.palettes import Viridis256, Category20\n",
    "from bokeh.transform import linear_cmap, factor_cmap\n",
    "from bokeh.colors import RGB\n",
    "\n",
    "# Huggingface\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54613b8-2a8e-4828-84a7-a76765492dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Loading the Henkel Patents\n",
    "###########################################################\n",
    "\n",
    "# Directories in which data important for the notebook is stored\n",
    "dump_dir = 'PK_DUMP'\n",
    "data_dir = 'data'\n",
    "\n",
    "# Loading the dataframes with the patents deemed most important for electrically debondable adhesives from Henkel\n",
    "henkel_patents = pd.read_csv(f'{data_dir}/Henkel_patente_patstat_docdb_families_abstract.csv', delimiter=',').reset_index(drop=True)\n",
    "henkel_orbit = pd.read_csv(f'{data_dir}/Henkel_Orbit_Suche_Patstat_Export.csv', delimiter=',')\n",
    "\n",
    "# Filtering the Samples which contain F-Terms\n",
    "henkel_filtered = henkel_patents[henkel_patents['fterms'].notna()]\n",
    "henkel_filtered = henkel_filtered.reset_index(drop=True)\n",
    "\n",
    "orbit_filtered = henkel_orbit[henkel_orbit['fterms'].notna()]\n",
    "orbit_filtered = orbit_filtered.reset_index(drop=True)\n",
    "\n",
    "print(f\"There are {len(henkel_patents['doc_db_family_id'].unique())} unique patents in the Henkel dataset, only {len(henkel_filtered['doc_db_family_id'].unique())} of them contain F-Terms.\")\n",
    "\n",
    "# extracting all f-terms form the datasets\n",
    "fterms_henkel = [fterm[:10] for fterms in henkel_filtered['fterms'] for fterm in fterms.split(',')]\n",
    "fterms_orbit = [fterm[:10] for fterms in orbit_filtered['fterms'] for fterm in fterms.split(',')]\n",
    "\n",
    "# Aggreagting the F-Terms\n",
    "with open(f'{dump_dir}/aggregation_dict_new.pk', 'rb') as f:\n",
    "    aggregation_dict = pk.load(f)\n",
    "\n",
    "def aggregate(f_term):\n",
    "    try:\n",
    "        return aggregation_dict[f_term]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "fterms_henkel_agg = [aggregate(fterm) for fterm in fterms_henkel if aggregate(fterm) is not None]\n",
    "fterms_orbit_agg = [aggregate(fterm) for fterm in fterms_orbit if aggregate(fterm) is not None]\n",
    "\n",
    "# Counting the occurrences of the henkel and orbit fterms\n",
    "counter_henkel = Counter(fterms_henkel_agg)\n",
    "counter_orbit = Counter(fterms_orbit_agg)\n",
    "\n",
    "# Structuring the henkel F-Terms\n",
    "henkel_dict = {}\n",
    "for fterm in counter_henkel.keys():\n",
    "    theme = fterm[:5]\n",
    "    try:\n",
    "        _ = henkel_dict[theme]\n",
    "    except KeyError:\n",
    "        henkel_dict[theme] = {}\n",
    "\n",
    "    vp = fterm[:8]\n",
    "    try: \n",
    "        henkel_dict[theme][vp].append(fterm)\n",
    "    except KeyError:\n",
    "        henkel_dict[theme][vp] = [fterm]\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Loading the Model\n",
    "################################################################\n",
    "\n",
    "model_name = 'gal_125_new_1'\n",
    "checkpoint = int(2*86515)\n",
    "# If True normalization is applied to the embeddings\n",
    "norm = True\n",
    "context_less = False\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "if model_name.split('_')[1] == '125':\n",
    "    base_model_name = 'mini'\n",
    "elif model_name.split('_')[1] == '1300':\n",
    "    base_model_name = 'base'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cpu'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "###########################\n",
    "# Loading the Tokenizer\n",
    "###########################\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print('Loaded Tokenizer from serialized instance!')    \n",
    "print(f'There are {n_f_terms:,} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading Descriptions\n",
    "###########################\n",
    "with open(f'{dump_dir}/agg_themes_descriptions_new.pk', 'rb') as f:\n",
    "    theme_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_viewpoints_descriptions_new.pk', 'rb') as f:\n",
    "    viewpoint_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_numbers_descriptions_new.pk', 'rb') as f:\n",
    "    number_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_full_descriptions_new.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)\n",
    "\n",
    "\n",
    "###########################\n",
    "# Extracting the Embeddings\n",
    "###########################\n",
    "\n",
    "# Extracting the classification Head weights\n",
    "inp_emb = model.get_input_embeddings()\n",
    "\n",
    "\n",
    "#Embeddings if the model is not a sequence classification model\n",
    "out_emb = model.get_output_embeddings()\n",
    "out_emb = next(out_emb.parameters()).to('cpu').detach().numpy()[2:]\n",
    "inp_emb = inp_emb(torch.arange(len(tokenizer))).to('cpu').detach().numpy()[50002:]\n",
    "\n",
    "if context_less:\n",
    "    # Extracting context less embeddings\n",
    "    if not os.path.isfile(f'{model_folder}/context_less_emb{checkpoint}.pk'):\n",
    "        print('Calculating context less embeddings!')\n",
    "        context_less_emb = [[] for _ in range(len([1 for _ in model.parameters()]))]\n",
    "        for i in range(len(tokenizer)):\n",
    "            print(i, end='\\r')\n",
    "            out = model(input_ids= torch.tensor([[i]]), attention_mask = torch.tensor([[1]]), output_hidden_states=True)\n",
    "                \n",
    "            out = out.hidden_states\n",
    "            for i, k in enumerate(out):\n",
    "                context_less_emb[i].append(k.to('cpu').detach().numpy())\n",
    "        with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'wb') as f:\n",
    "            pk.dump(context_less_emb, f)\n",
    "    else:\n",
    "        print('Loading context less embeddings from disk')\n",
    "        with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'rb') as f:\n",
    "            context_less_emb = pk.load(f)\n",
    "        \n",
    "    # Combining context less embeddings of a layer to a single tensor\n",
    "    for i, layer in enumerate(context_less_emb):\n",
    "        layer = [e[0] for e in layer]\n",
    "        layer = np.concatenate(layer, 0)\n",
    "        context_less_emb[i] = layer\n",
    "\n",
    "# Normalizing the embeddings \n",
    "def normalize(tensor):\n",
    "    if norm:\n",
    "        return torch.nn.functional.normalize(torch.tensor(tensor), p=2).numpy()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "out_emb = normalize(out_emb)\n",
    "inp_emb = normalize(inp_emb)\n",
    "\n",
    "if context_less:\n",
    "    context_less_emb = [normalize(layer) for layer in context_less_emb]\n",
    "\n",
    "# Extracting the matching F_terms for the weights and creating lists with the defintions\n",
    "tokens = [tokenizer.decode(i) for i in range(len(tokenizer))]\n",
    "f_term_tokens = tokens[50002:]\n",
    "\n",
    "# Creating  a dict with f-Terms and their embedding vectors:\n",
    "out_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, out_emb)}\n",
    "ft_emb_dict = {key: np.abs(fft(value)) for key, value in out_emb_dict.items()}\n",
    "inp_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, inp_emb)}\n",
    "    \n",
    "# Creating Context Less Embedding Dicts\n",
    "if context_less:\n",
    "    context_less_dicts = []\n",
    "    for layer in context_less_emb:\n",
    "        context_less_dicts.append({token[:-1]: vec for token, vec in zip(tokens, layer)})\n",
    "\n",
    "# Extracting the emb_dim\n",
    "for e in out_emb_dict.values():\n",
    "    break\n",
    "emb_dim = e.shape[-1]\n",
    "print('Embedding Dimension: ', emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bcdc8-183c-450b-8ba2-709994291b3c",
   "metadata": {},
   "source": [
    "# Plotting the Henkel F-Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baaa354-46f9-4906-a304-266e4f90512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Create a histogram of the Henkel F-Terms\n",
    "###########################################\n",
    "\n",
    "# Extracting the frequencies and the fterms from the counters\n",
    "hist = list(counter_henkel.values())\n",
    "fterms = list(counter_henkel.keys())\n",
    "\n",
    "\n",
    "# Sorting the frequencies and the fterms \n",
    "idx = np.argsort(hist)[::-1]\n",
    "hist = [hist[i] for i in idx]\n",
    "fterms = [fterms[i] for i in idx]\n",
    "edges = np.arange(len(hist)+1)\n",
    "\n",
    "# Extracting the theme viewpoint and number definitions\n",
    "\n",
    "themes = []\n",
    "for fterm in fterms:\n",
    "    try:\n",
    "        themes.append(theme_dict[fterm[:5]]) \n",
    "    except KeyError:\n",
    "        themes.append('Not Found')\n",
    "        \n",
    "vps = []\n",
    "for fterm in fterms:\n",
    "    try: \n",
    "        vps.append(viewpoint_dict[fterm[:8]])\n",
    "    except KeyError:\n",
    "        vps.append('Not Found')\n",
    "        \n",
    "numbers = []\n",
    "for fterm in fterms:\n",
    "    try:\n",
    "        numbers.append(number_dict[fterm[:10]]) \n",
    "    except KeyError:\n",
    "        numbers.append('Not Found')\n",
    "\n",
    "# Create a Bokeh figure\n",
    "output_notebook()  # Display Bokeh plots in Jupyter Notebook\n",
    "p = figure(title=\"Histogram Henkel F-Terms\", y_axis_label=\"Frequency\", width=1000, height=1000)\n",
    "\n",
    "# Create a ColumnDataSource for the histogram bars\n",
    "source = ColumnDataSource(data={\"top\": hist, \"left\": edges[:-1], \"right\": edges[1:], \"theme\": themes})\n",
    "\n",
    "# Create a color mapping for themes\n",
    "unique_themes = list(set(themes))\n",
    "color_mapping = factor_cmap(\"theme\", palette=Category20[len(unique_themes)], factors=unique_themes)\n",
    "\n",
    "# Create VBar glyph for the histogram bars with the color mapping\n",
    "p.vbar(x=\"left\", top=\"top\", bottom=0, width=1, source=source, fill_color=color_mapping, legend_field=\"theme\")\n",
    "\n",
    "# Create a new column in the ColumnDataSource for text annotations\n",
    "text_annotations = [str(count) for count in hist]\n",
    "\n",
    "# Add annotations to each bin at the correct position\n",
    "source.add(text_annotations, \"text\")\n",
    "source.add(fterms, 'F-Term')\n",
    "source.add(themes, 'Theme')\n",
    "source.add(vps, 'Vp.')\n",
    "source.add(numbers, 'Number')\n",
    "\n",
    "# Create a HoverTool to display annotations on hover\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Count\", \"@text\"),(\"F-Term\", \"@{F-Term}\"), (\"Theme\", \"@Theme\"), (\"Viewpoint\", \"@{Vp.}\"), (\"Number\", \"@Number\")]\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Remove x-axis ticks and description\n",
    "p.xaxis.visible = False\n",
    "\n",
    "# Show the plot\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448aab2-e2fb-40c1-b6e3-a330a3aa82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Henkel Embeddings and the Orbit Embeddings in TSNE Plots \n",
    "orbit_emb = []\n",
    "o_ft = []\n",
    "for fterm in counter_orbit.keys(): \n",
    "    try:\n",
    "        orbit_emb.append(out_emb_dict[fterm])\n",
    "        o_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "orbit_emb = np.array(orbit_emb)\n",
    "\n",
    "henkel_emb = []\n",
    "h_ft = []\n",
    "for fterm in counter_henkel.keys():\n",
    "    try:\n",
    "        henkel_emb.append(out_emb_dict[fterm])\n",
    "        h_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "henkel_emb = np.array(henkel_emb)\n",
    "\n",
    "all_emb = np.concatenate([orbit_emb, henkel_emb], 0)\n",
    "all_ft = [*o_ft, *h_ft]\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69)\n",
    "rep = tsne.fit_transform(all_emb)\n",
    "\n",
    "datasource_henkel = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft):,0],\n",
    "            y = rep[len(o_ft):,1],\n",
    "            fterms = h_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in h_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in h_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in h_ft]))\n",
    "\n",
    "datasource_orbit = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[:len(o_ft), 0],\n",
    "            y = rep[:len(o_ft), 1],\n",
    "            fterms = o_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in o_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in o_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in o_ft]))\n",
    "\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>F-Term:</b> @fterms<br><b>Theme:</b> @themes<br><b>Viewpoint:</b> @viewpoints<br><b>Number:</b> @numbers</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Henkel and Orbit Embeddings')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=10, fill_color=RGB(250, 50, 100), alpha=1, line_width=0, source=datasource_henkel, name=\"Henkel Embeddings\")\n",
    "plot_tsne.square('x', 'y', size=7, fill_color=RGB(50, 75, 250), alpha=0.2, line_width=0, source=datasource_orbit, name=\"Orbit Embeddings\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe84053-63c0-408d-b61a-ec1938a784a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "There are 7 main clusters of Henkel F-Terms, which correspont to 7 main Themes:\n",
    "    \"Containers, tranfer, fixing, positioning etc. of wavers ,etc.\"\n",
    "    \"Adhesives or adhesive processes\"\n",
    "    \"Laminated bodies #2\"\n",
    "    \"Treatments of macromolecular shaped articles\"\n",
    "    \"Paints or removers\"\n",
    "    \"ink jet, e.g. ink supply, others\"\n",
    "    \"golf clubs\"''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277ba48-4cdd-4b39-bf33-1bf4df115baf",
   "metadata": {},
   "source": [
    "# Simply Searching for Similar F-Term Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7103614-d376-45aa-8500-91d240e72142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Searching for similar F-Terms without creating difference Vectors\"\n",
    "\n",
    "# n = number of similar and novel F-Terms (not jet found by previous similarity search), That are saved for each sample. \n",
    "n = 50\n",
    "hits = []\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "for i, q_fterm in enumerate(counter_henkel.keys()):\n",
    "    print(i, q_fterm, end='\\r')\n",
    "    q_emb = out_emb_dict[q_fterm]\n",
    "    search_embs = []\n",
    "    search_fterms = []\n",
    "    for fterm, emb in out_emb_dict.items():\n",
    "        # Filtering all previously found f\n",
    "        if fterm in hits:\n",
    "            continue\n",
    "        if fterm in counter_henkel.keys():\n",
    "            continue\n",
    "        search_embs.append(emb)\n",
    "        search_fterms.append(fterm)\n",
    "\n",
    "    search_emb = torch.tensor(np.array(search_embs))\n",
    "    q_emb = torch.tensor(np.array([q_emb for _ in search_emb]))\n",
    "\n",
    "    simis = cos(search_emb, q_emb)\n",
    "    idx = np.argsort(simis)\n",
    "    for i in idx[-n:]:\n",
    "        hits.append(search_fterms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4819f-5eb6-46bd-9070-3fd9e241aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the embeddigns hits vs the orbit search results\n",
    "\n",
    "hits_emb = torch.tensor(np.array([out_emb_dict[fterm] for fterm in hits]))\n",
    "\n",
    "orbit_emb = []\n",
    "o_ft = []\n",
    "for fterm in counter_orbit.keys(): \n",
    "    try:\n",
    "        orbit_emb.append(out_emb_dict[fterm])\n",
    "        o_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "orbit_emb = np.array(orbit_emb)\n",
    "\n",
    "henkel_emb = []\n",
    "h_ft = []\n",
    "for fterm in counter_henkel.keys():\n",
    "    try:\n",
    "        henkel_emb.append(out_emb_dict[fterm])\n",
    "        h_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "henkel_emb = np.array(henkel_emb)\n",
    "\n",
    "all_emb = np.concatenate([orbit_emb, henkel_emb, hits_emb], 0)\n",
    "all_ft = [*o_ft, *h_ft, *hits]\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69)\n",
    "rep = tsne.fit_transform(all_emb)\n",
    "\n",
    "datasource_henkel = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft):len(o_ft) + len(h_ft),0],\n",
    "            y = rep[len(o_ft):len(o_ft) + len(h_ft),1],\n",
    "            fterms = h_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in h_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in h_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in h_ft]))\n",
    "\n",
    "datasource_orbit = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[:len(o_ft), 0],\n",
    "            y = rep[:len(o_ft), 1],\n",
    "            fterms = o_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in o_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in o_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in o_ft]))\n",
    "\n",
    "datasource_emb_search = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft) + len(h_ft):, 0],\n",
    "            y = rep[len(o_ft) + len(h_ft):, 1],\n",
    "            fterms = hits,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in hits],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in hits],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in hits]))\n",
    "\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>F-Term:</b> @fterms<br><b>Theme:</b> @themes<br><b>Viewpoint:</b> @viewpoints<br><b>Number:</b> @numbers</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Henkel and Orbit Embeddings')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=10, fill_color=RGB(250, 50, 75), alpha=1, line_width=0, source=datasource_henkel, name=\"Henkel Embeddings\")\n",
    "plot_tsne.square('x', 'y', size=7, fill_color=RGB(50, 75, 250), alpha=0.2, line_width=0, source=datasource_orbit, name=\"Orbit Embeddings\")\n",
    "plot_tsne.triangle('x', 'y', size=7, fill_color=RGB(75, 250, 50), alpha=0.2, line_width=0, source=datasource_emb_search, name=\"Cos Similar Embeddings\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd67ad-56f2-4436-92bd-1099d306377a",
   "metadata": {},
   "source": [
    "# In Viewpoint Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b130d7-6f4a-4377-8fa4-d33cce0aae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fterm_dict():\n",
    "    \"\"\"\n",
    "    Creates a hirachical dict with all F-Terms ordered by Theme -> Viewpoint > F-Terms\n",
    "    \"\"\"\n",
    "    f_term_dict = {}\n",
    "    for f_term in f_term_tokens:\n",
    "        theme = f_term.split('/')[0]\n",
    "        vp = f_term[:8]\n",
    "        # Creating a dict entry for the theme\n",
    "        try: \n",
    "            _ = f_term_dict[theme]\n",
    "        except KeyError:\n",
    "            f_term_dict[theme] = {}\n",
    "    \n",
    "        # Creating a dict entry for the viewpoint\n",
    "    \n",
    "        try:\n",
    "            # The first dict call will def. work the second may work if the vp-dict entry \n",
    "            # was already made. If it works the theme is appended to the viewpoint dict\n",
    "            f_term_dict[theme][vp].append(f_term)\n",
    "        except KeyError:\n",
    "            f_term_dict[theme][vp] = []\n",
    "\n",
    "    return f_term_dict\n",
    "    \n",
    "\n",
    "def create_all_diffs():\n",
    "    \"\"\"\n",
    "    Creates all possible in viewpoint combinations and returns them a s a \n",
    "    \"\"\"\n",
    "    all_diffs = {}\n",
    "    emb = out_emb_dict\n",
    "    # Calculating the needed combinations\n",
    "    f_term_dict = create_fterm_dict()\n",
    "    for i, (theme, t_dict) in enumerate(f_term_dict.items()):\n",
    "        # print(i, theme, len(t_dict), end='\\r')\n",
    "        all_diffs[theme] = {}\n",
    "        for viewpoint, fterms in t_dict.items():\n",
    "            all_diffs[theme][viewpoint] = {}\n",
    "            combinations = itertools.combinations(fterms, 2)\n",
    "            for fterm1, fterm2 in combinations:\n",
    "                diff = emb[fterm2[:10]] - emb[fterm1[:10]]\n",
    "                diff = normalize(np.array([diff]))\n",
    "                all_diffs[theme][viewpoint][(fterm1, fterm2)] = diff\n",
    "\n",
    "    return all_diffs\n",
    "    \n",
    "\n",
    "def create_diffs_tensor(block_theme, all_diffs):\n",
    "    \"\"\"\n",
    "    Creates a tensor with all diffs, which do not contain the block theme.\n",
    "    Additionaly also returns a list with all comination descriptions\n",
    "    \"\"\"\n",
    "    # Filtering out the unwanted theme\n",
    "    diffs = {theme: t_dict for theme, t_dict in all_diffs.items() if theme != block_theme}\n",
    "    out_diffs = []\n",
    "    out_desc = []\n",
    "    for _, t_dict in diffs.items():\n",
    "        for _, vp_dict in t_dict.items():\n",
    "            for comb, diff in vp_dict.items():\n",
    "                out_desc.append(comb)\n",
    "                out_diffs.append(diff)\n",
    "\n",
    "    out_diffs = np.array(out_diffs)\n",
    "    out_diffs = out_diffs.squeeze(1)\n",
    "    return out_diffs, out_desc\n",
    "\n",
    "\n",
    "def search_cos(query_vec, all_vecs, all_desc, n):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarities between all_vecs and the query_vec and returns the descriptions of the n best matches.\n",
    "    \n",
    "    \"\"\"\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    # Creating an array of query vectors, with the same number of vectors as the all_vecs array.\n",
    "    query = np.concatenate([query_vec for _ in all_vecs], 0)\n",
    "    simis = cos(torch.tensor(all_vecs, requires_grad=False), torch.tensor(query, requires_grad=False))\n",
    "    # Creating the indices of the top n similarities\n",
    "    idx = np.argsort(simis).numpy()[::-1][:n]\n",
    "    matches = [all_desc[i] for i in idx]\n",
    "    simis = [simis[i] for i in idx]\n",
    "    return matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc9ec5-2249-4c2e-a7f0-efb556d07fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hirachical dict which contains all in viewpoint differences\n",
    "all_diffs = create_all_diffs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23249fe5-8620-4c94-bc34-3a69fa265d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Searching with henkel in viewpoint differences\n",
    "########################################################\n",
    "\n",
    "# maximum number of new patents added to the results per query diff\n",
    "n = 50\n",
    "# Hirachical dict containing all Henkel f-terms\n",
    "henkel_dict = {}\n",
    "\n",
    "for fterm in counter_henkel.keys():\n",
    "    theme = fterm[:5]\n",
    "    try:\n",
    "        _ = henkel_dict[theme]\n",
    "    except KeyError:\n",
    "        henkel_dict[theme] = {}\n",
    "\n",
    "    vp = fterm[:8]\n",
    "    try: \n",
    "        henkel_dict[theme][vp].append(fterm)\n",
    "    except KeyError:\n",
    "        henkel_dict[theme][vp] = [fterm]\n",
    "\n",
    "# List of all henkel f-term in viewpoint combinations \n",
    "henkel_invp = []\n",
    "henkel_invp_diff = []\n",
    "\n",
    "for t_dict in henkel_dict.values():\n",
    "    for fterms in t_dict.values():\n",
    "        combinations = list(itertools.combinations(fterms, 2))\n",
    "        henkel_invp.extend(combinations)\n",
    "        for fterm1, fterm2 in combinations:\n",
    "            diff = normalize(np.array([out_emb_dict[fterm2]])-np.array([out_emb_dict[fterm1]]))\n",
    "            henkel_invp_diff.append(diff)\n",
    "\n",
    "\n",
    "found_f_terms = []\n",
    "# list which stores the query diffs, by which the found_f_terms were found\n",
    "found_by = []\n",
    "for i, (query_diff, desc) in enumerate(zip(henkel_invp_diff, henkel_invp)):\n",
    "    print(i, len(found_f_terms), len(found_by), end='\\r')\n",
    "    f_terms_added = 0\n",
    "    theme = desc[0][:5]\n",
    "    search_diffs, search_descs = create_diffs_tensor(theme, all_diffs)\n",
    "    results = search_cos(query_diff, search_diffs, search_descs, n*10) # Getting more results to account for multiple finings of one f-term\n",
    "    while f_terms_added < n:\n",
    "        try:\n",
    "            fterm1, fterm2 = results.pop(0)\n",
    "            if fterm1 not in found_f_terms:\n",
    "                found_f_terms.append(fterm1)\n",
    "                found_by.append(desc)\n",
    "                f_terms_added += 1\n",
    "    \n",
    "            if fterm2 not in found_f_terms:\n",
    "                found_f_terms.append(fterm2)\n",
    "                found_by.append(desc)\n",
    "                f_terms_added += 1\n",
    "        except IndexError:\n",
    "            results = search_cos(query_diff, search_diffs, search_descs, n*1000)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ff147-a862-46e2-a1da-7aec9a2a0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Plotting the in viewpoint search results\n",
    "###############################################\n",
    "\n",
    "hits_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in found_f_terms]))\n",
    "# To compare to a random sample of F-Terms\n",
    "#random_fterms = np.random.choice(f_term_tokens, len(found_f_terms), replace=False)\n",
    "#hits_emb = torch.tensor(np.array([out_emb_dict[fterm[:10]] for fterm in random_fterms]))\n",
    "\n",
    "orbit_emb = []\n",
    "o_ft = []\n",
    "for fterm in counter_orbit.keys(): \n",
    "    try:\n",
    "        orbit_emb.append(out_emb_dict[fterm])\n",
    "        o_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "orbit_emb = np.array(orbit_emb)\n",
    "\n",
    "henkel_emb = []\n",
    "h_ft = []\n",
    "for fterm in counter_henkel.keys():\n",
    "    try:\n",
    "        henkel_emb.append(out_emb_dict[fterm])\n",
    "        h_ft.append(fterm)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "henkel_emb = np.array(henkel_emb)\n",
    "\n",
    "all_emb = np.concatenate([orbit_emb, henkel_emb, hits_emb], 0)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69)\n",
    "rep = tsne.fit_transform(all_emb)\n",
    "\n",
    "datasource_henkel = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft):len(o_ft) + len(h_ft),0],\n",
    "            y = rep[len(o_ft):len(o_ft) + len(h_ft),1],\n",
    "            fterms = h_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in h_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in h_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in h_ft]))\n",
    "\n",
    "datasource_orbit = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[:len(o_ft), 0],\n",
    "            y = rep[:len(o_ft), 1],\n",
    "            fterms = o_ft,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in o_ft],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in o_ft],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in o_ft]))\n",
    "\n",
    "datasource_emb_search = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = rep[len(o_ft) + len(h_ft):, 0],\n",
    "            y = rep[len(o_ft) + len(h_ft):, 1],\n",
    "            fterms = found_f_terms,\n",
    "            themes = [theme_dict[fterm[:5]] for fterm in found_f_terms],\n",
    "            viewpoints = [viewpoint_dict[fterm[:8]] for fterm in found_f_terms],\n",
    "            numbers = [number_dict[fterm[:10]] for fterm in found_f_terms], \n",
    "            found_by = [number_dict[comb[0][:10]] +'---' + number_dict[comb[1][:10]] for comb in found_by]))\n",
    "\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>F-Term:</b> @fterms<br><b>Theme:</b> @themes<br><b>Viewpoint:</b> @viewpoints<br><b>Number:</b> @numbers<br><b>Query:</b> @found_by</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Henkel and Orbit Embeddings')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=10, fill_color=RGB(250, 50, 75), alpha=1, line_width=0, source=datasource_henkel, name=\"Henkel Embeddings\")\n",
    "plot_tsne.square('x', 'y', size=7, fill_color=RGB(50, 75, 250), alpha=0.2, line_width=0, source=datasource_orbit, name=\"Orbit Embeddings\")\n",
    "plot_tsne.triangle('x', 'y', size=7, fill_color=RGB(75, 250, 50), alpha=0.2, line_width=0, source=datasource_emb_search, name=\"Cos Similar Embeddings\")\n",
    "\n",
    "show(plot_tsne)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be44417-e902-4b9d-9d17-3dd403901fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
