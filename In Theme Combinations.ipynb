{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4680fcf-7b25-424f-b4cb-62ee23810736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fa33e0d5-c214-4f9c-8281-dabf542e1073\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"fa33e0d5-c214-4f9c-8281-dabf542e1073\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"fa33e0d5-c214-4f9c-8281-dabf542e1073\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"fa33e0d5-c214-4f9c-8281-dabf542e1073\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fa33e0d5-c214-4f9c-8281-dabf542e1073\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils_agg import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bokeh\n",
    "import time\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from cProfile import Profile\n",
    "from pstats import SortKey, Stats\n",
    "\n",
    "# Dimension reduction algorithms\n",
    "#from cuml.manifold import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "# Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import export_png\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf1840b-f565-4b68-9c4e-c8ee99c92e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenizer from serialized instance!\n",
      "There are 193383 different F-Terms in the whole Dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27850/1082341905.py:152: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return torch.nn.functional.normalize(torch.tensor(tensor), p=2).numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['Metal_Wood', 'Metal_Polymer', 'Metal_Cutting', 'Metal_Bleaching', 'Metal_Adhes', 'Wood_Metal', 'Wood_Polymer', 'Wood_Cutting', 'Wood_Bleaching', 'Wood_Adhes', 'Polymer_Metal', 'Polymer_Wood', 'Polymer_Cutting', 'Polymer_Bleaching', 'Polymer_Adhes', 'Cutting_Metal', 'Cutting_Wood', 'Cutting_Polymer', 'Cutting_Bleaching', 'Cutting_Adhes', 'Bleaching_Metal', 'Bleaching_Wood', 'Bleaching_Polymer', 'Bleaching_Cutting', 'Bleaching_Adhes', 'Adhes_Metal', 'Adhes_Wood', 'Adhes_Polymer', 'Adhes_Cutting', 'Adhes_Bleaching'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell has to be run twice due to an unknown bug\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_1300_agg_aug_1'\n",
    "checkpoint = 83438\n",
    "# If True normalization is applied to the embeddings\n",
    "norm = True\n",
    "seq_class = False\n",
    "context_less = False\n",
    "\n",
    "if seq_class:\n",
    "    # Importing code for sequence classification\n",
    "    from Masterarbeit_utils.model_utils_seq_class import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "    from transformers import OPTForSequenceClassification\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/agg_dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cpu'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "if seq_class:\n",
    "    model = OPTForSequenceClassification.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "else:\n",
    "    model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "###########################\n",
    "# Loading the Tokenizer\n",
    "###########################\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print('Loaded Tokenizer from serialized instance!')    \n",
    "print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading Descriptions\n",
    "###########################\n",
    "with open(f'{dump_dir}/agg_themes_descriptions.pk', 'rb') as f:\n",
    "    theme_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_viewpoints_descriptions.pk', 'rb') as f:\n",
    "    viewpoint_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_numbers_descriptions.pk', 'rb') as f:\n",
    "    number_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_full_descriptions.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)\n",
    "\n",
    "\n",
    "###########################\n",
    "# Extracting the Embeddings\n",
    "###########################\n",
    "\n",
    "# Extracting the classification Head weights\n",
    "inp_emb = model.get_input_embeddings()\n",
    "\n",
    "if not seq_class:\n",
    "    #Embeddings if the model is not a sequence classification model\n",
    "    out_emb = model.get_output_embeddings()\n",
    "    out_emb = next(out_emb.parameters()).to('cpu').detach().numpy()[2:]\n",
    "    inp_emb = inp_emb(torch.arange(len(tokenizer))).to('cpu').detach().numpy()[50002:]\n",
    "\n",
    "    if context_less:\n",
    "        # Extracting context less embeddings\n",
    "        if not os.path.isfile(f'{model_folder}/context_less_emb.pk'):\n",
    "            print('Calculating context less embeddings!')\n",
    "            context_less_emb = [[] for _ in range(13)]\n",
    "            for i in range(len(tokenizer)):\n",
    "                print(i, end='\\r')\n",
    "                out = model(input_ids= torch.tensor([[i]]), attention_mask = torch.tensor([[1]]), output_hidden_states=True)\n",
    "                \n",
    "                out = out.hidden_states\n",
    "                for i, k in enumerate(out):\n",
    "                    print(i)\n",
    "                    context_less_emb[i].append(k.to('cpu').detach().numpy())\n",
    "            with open(f'{model_folder}/context_less_emb.pk', 'wb') as f:\n",
    "                pk.dump(context_less_emb, f)\n",
    "        else:\n",
    "            print('Loading context less embeddings from disk')\n",
    "            with open(f'{model_folder}/context_less_emb.pk', 'rb') as f:\n",
    "                context_less_emb = pk.load(f)\n",
    "        \n",
    "        # Combining context less embeddings of a layer to a single tensor\n",
    "        for i, layer in enumerate(context_less_emb):\n",
    "            layer = [e[0] for e in layer]\n",
    "            layer = np.concatenate(layer, 0)\n",
    "            context_less_emb[i] = layer\n",
    "\n",
    "else: \n",
    "    # embeddings if the model is a Sequence Classifier\n",
    "    inp_emb = inp_emb(torch.arange(50000)).to('cpu').detach().numpy()\n",
    "    out_emb = model.score.weight\n",
    "    out_emb.to('cpu').detach().numpy()\n",
    "    \n",
    "\n",
    "## Normalizing the embeddings \n",
    "def normalize(tensor):\n",
    "    if norm:\n",
    "        return torch.nn.functional.normalize(torch.tensor(tensor), p=2).numpy()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "out_emb = normalize(out_emb)\n",
    "inp_emb = normalize(inp_emb)\n",
    "if not seq_class:\n",
    "    context_less_emb = [normalize(layer) for layer in context_less_emb]\n",
    "\n",
    "# Extracting the matching F_terms for the weights and creating lists with the defintions\n",
    "tokens = [tokenizer.decode(i) for i in range(len(tokenizer))]\n",
    "f_term_tokens = tokens[50002:]\n",
    "\n",
    "# Creating  a dict with f-Terms and their embedding vectors:\n",
    "\n",
    "out_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, out_emb)}\n",
    "if seq_class:\n",
    "    inp_emb_dict = {token[:-1]: vec for token, vec in zip(tokens[:50000], inp_emb)}\n",
    "else: \n",
    "    inp_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, inp_emb)}\n",
    "    \n",
    "if context_less:\n",
    "    # Creating Context Less Embedding Dicts\n",
    "    if not seq_class:\n",
    "        context_less_dicts = []\n",
    "        for layer in context_less_emb:\n",
    "            context_less_dicts.append({token[:-1]: vec for token, vec in zip(tokens, layer)})\n",
    "\n",
    "\n",
    "####################################################\n",
    "# Detecting F-Term Pairs with Supposed Similarities\n",
    "####################################################\n",
    "df=pd.read_csv(\"data/f-terms.csv\", index_col=0)\n",
    "\n",
    "#subset with \"material\" in the viewpoint description\n",
    "df[\"vp\"]=df.theme+\"/\"+df.viewpoint\n",
    "df[\"fterm\"]=df.theme+\"/\"+df.number\n",
    "df2=df.copy()\n",
    "\n",
    "#f-term descriptions are searched for the following materials\n",
    "materials_list=[\". Metal\", \". Wood\", \". Polymer\", \". Cutting\", \". Bleaching\", \". Adhes\"]\n",
    "materials_f_terms={}\n",
    "for material in materials_list:\n",
    "    materials_f_terms[material]=[df2[df2.label.str.startswith(material, na=False)].theme.unique(), df2[df2.label.str.startswith(material, na=False)].fterm.values]\n",
    "\n",
    "#pairs of two materials with fterms in same viewpoints are created\n",
    "material_combinations={}\n",
    "for mat1 in materials_list:\n",
    "    for mat2 in materials_list:\n",
    "        \n",
    "        if mat1==mat2: continue\n",
    "        if mat2[2:]+\"_\"+mat1[2:] in materials_f_terms: continue\n",
    "        \n",
    "        #filtering shared f-terms with both materials in the label description\n",
    "        \n",
    "        unique_fterms_mat1 = list(set(materials_f_terms[mat1][1]) - set(materials_f_terms[mat2][1]))\n",
    "        unique_fterms_mat2 = list(set(materials_f_terms[mat2][1]) - set(materials_f_terms[mat1][1]))\n",
    "\n",
    "\n",
    "        vp_mat1= set([x[:5]for x in unique_fterms_mat1])\n",
    "        vp_mat2= set([x[:5]for x in unique_fterms_mat2])\n",
    "        shared_vp=[x for x in vp_mat1 if x in vp_mat2]\n",
    "        fterm_pairs=[]\n",
    "        for vp in shared_vp:\n",
    "            fterm_pairs.append([vp,\n",
    "                                [fterm for fterm in unique_fterms_mat1 if fterm.startswith(vp)],\n",
    "                                [fterm for fterm in unique_fterms_mat2 if fterm.startswith(vp)]])\n",
    "            \n",
    "            \n",
    "        material_combinations[mat1[2:]+\"_\"+mat2[2:]]=fterm_pairs\n",
    "\n",
    "material_combinations.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d51a6d-d64b-4d92-9332-faef9646003f",
   "metadata": {},
   "source": [
    "# Searching for \"Triangle\" Combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b07c1b-192a-4940-bf6c-5678d61fda22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253 Generating combinations for Theme:  2C518\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'. Metal. Wood. Adhes': [('4E093/QA04,', '4E093/JA01,', '4E093/GC01,'),\n",
       "  ('4E093/QA04,', '4E093/HA01,', '4E093/GC01,')],\n",
       " '. Metal. Polymer. Adhes': [('4J001/DC09,', '4J001/EB72,', '4J001/JB45,'),\n",
       "  ('4J001/DC09,', '4J001/EB72,', '4J001/JA18,'),\n",
       "  ('4J001/DC09,', '4J001/EB71,', '4J001/JB45,'),\n",
       "  ('4J001/DC09,', '4J001/EB71,', '4J001/JA18,'),\n",
       "  ('4J001/DC09,', '4J001/EC81,', '4J001/JB45,'),\n",
       "  ('4J001/DC09,', '4J001/EC81,', '4J001/JA18,'),\n",
       "  ('4J001/DC09,', '4J001/GB07,', '4J001/JB45,'),\n",
       "  ('4J001/DC09,', '4J001/GB07,', '4J001/JA18,'),\n",
       "  ('4J001/DC09,', '4J001/HA03,', '4J001/JB45,'),\n",
       "  ('4J001/DC09,', '4J001/HA03,', '4J001/JA18,'),\n",
       "  ('4J001/DC09,', '4J001/EA41,', '4J001/JB45,'),\n",
       "  ('4J001/DC09,', '4J001/EA41,', '4J001/JA18,'),\n",
       "  ('4J001/DC09,', '4J001/ED61,', '4J001/JB45,'),\n",
       "  ('4J001/DC09,', '4J001/ED61,', '4J001/JA18,')],\n",
       " '. Metal. Wood. Cutting': [('2B150/DH01,', '2B150/CA31,', '2B150/BE02,'),\n",
       "  ('2B150/ED09,', '2B150/CA31,', '2B150/BE02,')],\n",
       " '. Metal. Cutting. Bleaching': [('4L055/GA40,', '4L055/BE04,', '4L055/CB41,'),\n",
       "  ('4L055/GA40,', '4L055/CH05,', '4L055/CB41,')],\n",
       " '. Wood. Cutting. Bleaching': [('4L055/AA01,', '4L055/BE04,', '4L055/CB41,'),\n",
       "  ('4L055/AA01,', '4L055/CH05,', '4L055/CB41,')],\n",
       " '. Wood. Cutting. Adhes': [('4L055/AA01,', '4L055/BE04,', '4L055/AH37,'),\n",
       "  ('4L055/AA01,', '4L055/BE04,', '4L055/GA42,'),\n",
       "  ('4L055/AA01,', '4L055/CH05,', '4L055/AH37,'),\n",
       "  ('4L055/AA01,', '4L055/CH05,', '4L055/GA42,')],\n",
       " '. Wood. Bleaching. Adhes': [('4L055/AA01,', '4L055/CB41,', '4L055/AH37,'),\n",
       "  ('4L055/AA01,', '4L055/CB41,', '4L055/GA42,')],\n",
       " '. Cutting. Bleaching. Adhes': [('2B200/EC21,', '2B200/HB15,', '2B200/EE13,'),\n",
       "  ('2B200/FA27,', '2B200/HB15,', '2B200/EE13,')],\n",
       " '. Wood. Polymer. Adhes': [('4J011/CA04,', '4J011/RA14,', '4J011/WA06,'),\n",
       "  ('4J011/CA04,', '4J011/NB04,', '4J011/WA06,'),\n",
       "  ('4J011/CA04,', '4J011/PC15,', '4J011/WA06,'),\n",
       "  ('4J011/CA04,', '4J011/VA03,', '4J011/WA06,'),\n",
       "  ('4J011/CA04,', '4J011/XA03,', '4J011/WA06,'),\n",
       "  ('4J011/CA04,', '4J011/MA20,', '4J011/WA06,')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################\n",
    "# Calculating the needed combinations\n",
    "theme_f_term_dict = {}\n",
    "for f_term in f_term_tokens:\n",
    "    theme = f_term.split('/')[0]\n",
    "    vp = f_term[:8]\n",
    "    # Creating a dict entry for the theme\n",
    "    try: \n",
    "        _ = theme_f_term_dict[theme]\n",
    "    except KeyError:\n",
    "        theme_f_term_dict[theme] = {}\n",
    "\n",
    "    # Creating a dict entry for the viewpoint\n",
    "\n",
    "    try:\n",
    "        # The first dict call will def. work the second may work if the vp-dict entry \n",
    "        # was already made. If it works the theme is appended to the viewpoint dict\n",
    "        theme_f_term_dict[theme][vp].append(f_term)\n",
    "    except KeyError:\n",
    "        theme_f_term_dict[theme][vp] = []\n",
    "\n",
    "# All possible triangular \"Material\" Combinations\n",
    "materials_list=[\". Metal\", \". Wood\", \". Polymer\", \". Cutting\", \". Bleaching\", \". Adhes\"]\n",
    "tri_combs = [[f1, f2, f3] for i1, f1 in enumerate(materials_list[:], start=1) for i2, f2 in enumerate(materials_list[i1:], start=1) for f3 in materials_list[i2+i1:]]\n",
    "\n",
    "#####################################################################################\n",
    "# Searching for the combinations in the theme_f_term_dict\n",
    "all_combs = {}\n",
    "for i, (theme, theme_vp_dict) in enumerate(theme_f_term_dict.items()):\n",
    "    print(i, 'Generating combinations for Theme: ', theme, end='\\r')\n",
    "    \n",
    "    all_f_terms = []\n",
    "    [all_f_terms.extend(f_terms) for f_terms in theme_vp_dict.values()]\n",
    "    # generating the idices of the f-terms in all_f_terms which start with a \"material\" from the materials_list\n",
    "    single_hits = {material: [f_term for f_term in all_f_terms if number_dict[f_term[:-1]].lower().startswith(material.lower())] for material in materials_list}\n",
    "\n",
    "    # Extracting the material hits for each combination\n",
    "    combinations = {comb[0]+comb[1]+comb[2]: [single_hits[comb[0]], single_hits[comb[1]], single_hits[comb[2]]] for comb in tri_combs}\n",
    "    # Generating the F-Term combinations\n",
    "    combinations = {key: [(f1, f2, f3) for f1 in f_terms[0] for f2 in f_terms[1] for f3 in f_terms[2]] for key, f_terms in combinations.items()} \n",
    "    # Dropping empty combinations\n",
    "    combinations = {key: [ft_comb for ft_comb in value if len(set([f_term[:8] for f_term in ft_comb])) == 3] for key, value in combinations.items()}\n",
    "    # Ignoring Themes with no combinations\n",
    "    combinations = {key: value for key, value in combinations.items() if len(value) != 0}\n",
    "    #  Dropping Combinations within the same viewpoint\n",
    "    if len(combinations) == 0:\n",
    "        continue\n",
    "\n",
    "    all_combs = all_combs | combinations\n",
    "\n",
    "# Dropping combinations with just one sample \n",
    "all_combs = {key: value for key, value in all_combs.items() if len(value) > 1}\n",
    "all_combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f35cd7-b169-4061-8c18-0f36e23ea2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['4E093', '2E093', '4E093']\n",
    "set(a)\n",
    "len(set(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7449a5b8-78f0-41a3-b68d-df9877a2c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253 Generating combinations for Theme:  2C518\r"
     ]
    }
   ],
   "source": [
    "# Generating the differences\n",
    "emb_dict = out_emb_dict\n",
    "\n",
    "for i, (theme, theme_vp_dict) in enumerate(theme_f_term_dict.items()):\n",
    "    print(i, 'Generating combinations for Theme: ', theme, end='\\r')\n",
    "    combinations = []\n",
    "    for vp, f_terms in theme_vp_dict.items():\n",
    "        keys = [key for key in theme_vp_dict.keys() if key != vp]\n",
    "        # This one liner basically concatenates all f_term lists except the f_term list of the current viewpoint\n",
    "        out_vp_f_terms = [f_term for f_term_list in [theme_vp_dict[key] for key in keys] for f_term in f_term_list]    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5685ba-7bac-4417-be8c-cabd8ff92740",
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme, combinations in theme_f_term_dict.items():\n",
    "    try:\n",
    "        print(theme_dict[theme], len(combinations))\n",
    "    except KeyError:\n",
    "        print('KeyError', theme_dict.keys())\n",
    "    \n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc2c93-9884-4f5d-9cb0-f3b7d91f1ec0",
   "metadata": {},
   "source": [
    "# Plotting the Material Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba0089-9caa-4186-98df-99ea059e2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating vectors for all material combinations\n",
    "emb_dict = out_emb_dict\n",
    "\n",
    "combination_desc = []\n",
    "vector_diffs = []\n",
    "themes = []\n",
    "theme_desc = []\n",
    "numbers = []\n",
    "desc = []\n",
    "color_ints = []\n",
    "\n",
    "for i, l  in enumerate(material_combinations.items()):\n",
    "    key, item = l\n",
    "    for theme_comb in item:\n",
    "        theme, mat_1, mat_2 = theme_comb\n",
    "        for f_term_1 in mat_1:\n",
    "            for f_term_2 in mat_2:\n",
    "                try: \n",
    "                    vec_1 = emb_dict[f_term_1]\n",
    "                    vec_2 = emb_dict[f_term_2]\n",
    "                    theme_desc.append(theme_dict[theme])\n",
    "                    desc.append(number_dict[f_term_2] + ' - ' + number_dict[f_term_1])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                    \n",
    "                diff = vec_2 - vec_1\n",
    "                combination_desc.append(key)\n",
    "                vector_diffs.append(diff)\n",
    "                themes.append(theme)\n",
    "                numbers.append(f_term_2 + ' - ' + f_term_1)\n",
    "                color_ints.append(i)\n",
    "        \n",
    "len(combination_desc), len(theme_desc), len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701851f-a1ff-41ba-ae23-6b6456b6f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_diffs = np.stack(vector_diffs, 0)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, random_state=69) \n",
    "tsne_rep = tsne.fit_transform(vector_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294cf77f-bd07-456e-b2f2-2d74f9d936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_palette = bokeh.palettes.viridis(30)\n",
    "color_palette = bokeh_palette\n",
    "\n",
    "colors = [color_palette[c%30] for c in color_ints]\n",
    "\n",
    "datasource_diff = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = tsne_rep[:,0],\n",
    "            y = tsne_rep[:,1],\n",
    "            combination =  combination_desc,\n",
    "            themes=themes,\n",
    "            theme_desc = theme_desc,\n",
    "            numbers = numbers,\n",
    "            desc = desc,\n",
    "            colors = colors\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover_tsne = HoverTool(tooltips='<div style=\"font-size: 12px;\"><b>Combination: </b>  @combination<br><b>Theme:</b> @themes<br><b>Theme Description:</b> @theme_desc<br><b>Numbers:</b> @numbers<br><b>Description:</b> @desc</div>', mode='mouse')\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(width=1500, height=1500, tools=tools_tsne, title='Material Combintation Differences')\n",
    "    \n",
    "plot_tsne.circle('x', 'y', size=8, fill_color='colors', \n",
    "                     alpha=0.7, line_width=0, source=datasource_diff, name=\"Material Combination Differences\")\n",
    "\n",
    "show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd251359",
   "metadata": {},
   "source": [
    "# Generating in Theme Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1937958-0947-4228-ad72-c10cd8ba14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = out_emb_dict\n",
    "#layer_n = 13\n",
    "#emb = context_less_dicts[layer_n]\n",
    "# Dictionary that contains all differences and additional information for each material combination\n",
    "differences = {}\n",
    "# Huge dict, that contains the difference vectors as keys and \n",
    "diffs_dict = {}\n",
    "\n",
    "for i, combination in enumerate(material_combinations.items()):\n",
    "    key, item = combination\n",
    "    print(i, key, end='\\r')\n",
    "    # Checking if the reverse combination was already calculated\n",
    "    current_material_keys = []\n",
    "    for current_key in differences.keys():\n",
    "      \n",
    "        current_material_keys.append(current_key.split('_'))\n",
    "    combination_keys = key.split('_')\n",
    "    combination_keys.reverse()\n",
    "    if combination_keys in current_material_keys:\n",
    "        continue\n",
    "\n",
    "    # Calculating the differences and storing them in a list of dict\n",
    "    combinations_list = []\n",
    "    for theme_comb in item:\n",
    "        theme, mat_1, mat_2 = theme_comb\n",
    "        for f_term_1 in mat_1:\n",
    "            for f_term_2 in mat_2:\n",
    "                try: \n",
    "                    vec_1 = emb[f_term_1]\n",
    "                    vec_2 = emb[f_term_2]\n",
    "                    diff = vec_2 - vec_1\n",
    "                    diff = normalize(np.array([diff]))[0]                   \n",
    "                    sample_dict = {}\n",
    "                    sample_dict['Vector'] = diff\n",
    "                    sample_dict['Theme'] = theme\n",
    "                    sample_dict['F-Term 1'] = f_term_1\n",
    "                    sample_dict['F-Term 2'] = f_term_2\n",
    "                    diffs_dict[sys.intern(str(np.sum(diff)))] = [theme, f_term_1, f_term_2]\n",
    "                    combinations_list.append(sample_dict)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "    # Now the differences are computed and stored in a list of dicts\n",
    "            \n",
    "    differences[key] = combinations_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272c58f-bf90-4456-b04b-863390c3b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairing the vectors to compare\n",
    "\n",
    "max_comb = 100000000\n",
    "sqr_max_comb = int(max_comb**0.5)\n",
    "in_combination_comparisons = {}\n",
    "\n",
    "for i, [keys, samples] in enumerate(differences.items()):\n",
    "    # Adding the comparison of the material combination with itself to the comparisons\n",
    "    in_combination_comparisons[keys] = {'Vectors A': [], \n",
    "                                        'Vectors B': [],\n",
    "                                        'Themes': [],\n",
    "                                        'F-Terms A': [],\n",
    "                                        'F-Terms B': []}\n",
    "    print(i, keys, len(samples), end = '\\r')\n",
    "    samples_2 = list(samples)\n",
    "    random.shuffle(samples_2)\n",
    "    s1 = 0 \n",
    "    while len(samples_2) > 0:\n",
    "        s1 += 1\n",
    "        s2 = 0\n",
    "        sample_a = samples_2.pop(0)\n",
    "        random.shuffle(samples_2)\n",
    "        for sample_b in samples_2:\n",
    "\n",
    "            if s2 == sqr_max_comb:\n",
    "                break\n",
    "            # Ignoring combinations with matching Viewpoints\n",
    "            if sample_a['Theme'] == sample_b['Theme']:\n",
    "                continue\n",
    "            # Debugging remove later \n",
    "            ##################################\n",
    "            themea = sample_a['Theme']\n",
    "            themeb = sample_b['Theme']\n",
    "            theme_a = themea.split('/')[0]\n",
    "            theme_b = themeb.split('/')[0]\n",
    "            if theme_a == theme_b:\n",
    "                continue\n",
    "            ###################################\n",
    "            in_combination_comparisons[keys]['Vectors A'].append(torch.tensor(np.array([sample_a['Vector']])))\n",
    "            in_combination_comparisons[keys]['Vectors B'].append(torch.tensor(np.array([sample_b['Vector']])))\n",
    "            in_combination_comparisons[keys]['Themes'].append([sample_a['Theme'], sample_a['Theme']])\n",
    "            in_combination_comparisons[keys]['F-Terms A'].append([sample_a['F-Term 1'], sample_a['F-Term 2']])\n",
    "            in_combination_comparisons[keys]['F-Terms B'].append([sample_b['F-Term 1'], sample_b['F-Term 2']])\n",
    "            s2 += 1\n",
    "        if s1 == sqr_max_comb:\n",
    "            break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f0f9a-17f1-4626-af5d-d87dee3c8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Cosine Similarities within all combinations\n",
    "cos = torch.nn.CosineSimilarity(dim =1)\n",
    "bad_keys = []\n",
    "min_comb = 10\n",
    "\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    print(' '*1000, end='\\r')\n",
    "    print(i, key, end = '\\r')\n",
    "    try:\n",
    "        similarities = cos(torch.cat(combination_dict['Vectors A'], 0), torch.cat(combination_dict['Vectors B'], 0))\n",
    "        # Dropping combinations with low sample counts\n",
    "        if len(similarities) < min_comb:\n",
    "            print('Low number of samples in:', key, len(similarities), end='\\r')\n",
    "            bad_keys.append(key)\n",
    "            continue\n",
    "        combination_dict['Cosine Similarities'] = np.array(similarities)\n",
    "        # Creating a sorted index of the similariteis to compare the best ones with other combinations\n",
    "        combination_dict['Sort idx'] = np.argsort(np.array(similarities))[::-1]\n",
    "        \n",
    "        \n",
    "    except RuntimeError:\n",
    "        print(' '*1000, end='\\r')\n",
    "        print('Empty combination dictionary found, dropping  it!', key, end='\\r')\n",
    "        bad_keys.append(key)\n",
    "        continue\n",
    "    \n",
    "\n",
    "for key in bad_keys:\n",
    "    in_combination_comparisons.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f61fd-4aaa-4082-9c7b-567fdbd88ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best vectors \n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    top_100 = np.array(combination_dict['Cosine Similarities'])\n",
    "    idx = combination_dict['Sort idx'][:100]\n",
    "    top_100 = top_100[idx]\n",
    "    # Top 100 vectors a\n",
    "    top_vectors = np.concatenate(combination_dict['Vectors A'], 0)[idx]\n",
    "    top_vectors = np.concatenate([np.concatenate(combination_dict['Vectors B'], 0)[idx], top_vectors])\n",
    "    # dropping duplicates\n",
    "    top_vectors = np.unique(top_vectors, axis = 0)    \n",
    "    combination_dict['Best Vectors']  = top_vectors\n",
    "    print(' '*1000, end='\\r')\n",
    "    print(i, key, len(top_vectors), end='\\r')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872ada0-7951-4f9c-b786-fd34cd1d392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the cosine similarities for out of combination comparisons\n",
    "\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    print(' '*1000, end='\\r')\n",
    "    print(i, key, end=' \\r')\n",
    "    \n",
    "    # Extracting all unique vectors from other material combinations, which do not share a material with the material \n",
    "    # Combination to be checked\n",
    "    all_vectors = []\n",
    "    material_1, material_2 = key.split('_')\n",
    "    for key2, samples in differences.items():\n",
    "        \n",
    "        materials2 = key2.split('_')\n",
    "        # Ignoring material combinations with matching materials \n",
    "        if material_1 in materials2 or material_2 in materials2:\n",
    "            continue\n",
    "        # Ignoring material combinations which do not appear in the in_combination_comparisons keys\n",
    "        if not key2 in [k for k in in_combination_comparisons.keys()]:\n",
    "            continue\n",
    "        \n",
    "        for sample_dict in samples:\n",
    "            all_vectors.append(sample_dict['Vector'])\n",
    "\n",
    "    if len(all_vectors) == 0: continue\n",
    "    \n",
    "    all_vectors = np.stack(all_vectors, 0)\n",
    "    all_vectors = np.unique(all_vectors, axis=0)\n",
    "    \n",
    "    # vectors_a = vectors from top_vectors, vectors_b = vectors from other material combinations to compare the top_vectors with.\n",
    "    vectors_a , vectors_b = [], []\n",
    "    top_vectors = combination_dict['Best Vectors'][:50]\n",
    "\n",
    "    for top_vector in top_vectors:\n",
    "        vp, _ ,_ = diffs_dict[str(np.sum(top_vector))]\n",
    "        for vector in all_vectors:\n",
    "            vp2, _, _ = diffs_dict[str(np.sum(vector))]\n",
    "            # Skipping vectors with the same viewpoints\n",
    "            if vp2 == vp:\n",
    "                continue\n",
    "            ##### Debugging remove later\n",
    "            theme_a = vp.split('/')[0]\n",
    "            theme_b = vp2.split('/')[0]\n",
    "            if theme_a == theme_b:\n",
    "                continue\n",
    "            #############################\n",
    "            vectors_a.append(top_vector)\n",
    "            vectors_b.append(vector)\n",
    "    \n",
    "    vectors_a = np.stack(vectors_a, 0)\n",
    "    vectors_b = np.stack(vectors_b, 0)\n",
    "   \n",
    "    similarities = cos(torch.tensor(vectors_a), torch.tensor(vectors_b)).numpy()\n",
    "    combination_dict['Out of Comb Simis'] = similarities\n",
    "    # Just to generate the Matrices of the top combinations\n",
    "    combination_dict['Out of Comb Vec A'] = vectors_a\n",
    "    combination_dict['Out of Comb Vec B'] = vectors_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6efe1-6a1b-42c4-9718-40b2436bc79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_n = f'In Theme Combinations 10 min comb'\n",
    "# Plotting all out of comb similarities vs all in comb similarities (combined)\n",
    "all_in_comb = []\n",
    "all_out_comb = []\n",
    "for comb_dict in in_combination_comparisons.values():\n",
    "    all_in_comb.extend(comb_dict['Cosine Similarities'])\n",
    "    all_out_comb.extend(comb_dict['Out of Comb Simis'])\n",
    "\n",
    "all_in_comb = np.array(all_in_comb)\n",
    "all_out_comb = np.array(all_out_comb)\n",
    "mean_in = np.mean(all_in_comb)\n",
    "mean_out = np.mean(all_out_comb)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=[10, 10])\n",
    "\n",
    "\n",
    "axs.hist(all_in_comb, bins=500, alpha=0.75)\n",
    "axs.hist(all_out_comb, bins=500, alpha=0.75)\n",
    "axs.axvline(mean_in , color='red', linestyle='dashed', linewidth=1)\n",
    "axs.axvline(mean_out, color='magenta', linestyle='dashed', linewidth=1)\n",
    "axs.text(mean_in + 0.01, axs.get_ylim()[1] * 0.9, f'Mean: {mean_in:.6f}', color='grey')\n",
    "axs.text(mean_in + 0.01, axs.get_ylim()[1] * 0.925, f'Out of Comb Mean: {mean_out:.6f}', color='grey')\n",
    "axs.set_title(f'Layer {layer_n} Embedding 1 one 1 Similarities Best Vector Elements {model_name} {checkpoint}')\n",
    "plt.show()\n",
    "fig.savefig(f'{model_folder}/{layer_n}  {model_name} {checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe01109-a3b9-466b-99a9-2bfdcdb9c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the out of comb similarities\n",
    "\n",
    "fig, axs = plt.subplots(nrows = len(in_combination_comparisons), ncols=1, figsize=[10, 200])\n",
    "for i, (key, combination_dict) in enumerate(in_combination_comparisons.items()):\n",
    "    print(i, end='\\r')\n",
    "    simis = np.array(combination_dict['Cosine Similarities'])\n",
    "    out_simis = np.array(combination_dict['Out of Comb Simis'])\n",
    "    # Pruning the out of comb similarities to the same length as the similarities\n",
    "    out_simis = out_simis\n",
    "    \n",
    "    mean_top_100 = np.mean(np.sort(simis)[-100:])\n",
    "    out_mean_top_100 = np.mean(np.sort(out_simis)[-100:])\n",
    "    \n",
    "    axs[i].hist(simis, bins=50, alpha=.75)\n",
    "    axs[i].hist(out_simis[:len(simis)], bins=50, alpha=.75)\n",
    "    axs[i].set_title(key)\n",
    "    mean =  np.mean(simis)\n",
    "    mean_out = np.mean(out_simis)\n",
    "  \n",
    "    axs[i].axvline(mean, color='red', linestyle='dashed', linewidth=1)\n",
    "    axs[i].axvline(mean_out, color='magenta', linestyle='dashed', linewidth=1)\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.9, f'Mean: {mean:.6f}', color='grey')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.925, f'Out of Comb Mean: {mean_out:.6f}', \n",
    "                color='green' if mean > mean_out else 'red')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.875, f'Number of Comparisons: {len(simis)}', color='grey')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.85, f'Mean of Top 100: {mean_top_100:.6f}', color='grey')\n",
    "    axs[i].text(mean + 0.01, axs[i].get_ylim()[1] * 0.825, f'Mean of Top 100 Out of Comb: {out_mean_top_100:.6f}', \n",
    "                color='green' if mean_top_100 > out_mean_top_100 else 'red')\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f3fc6-c9e1-4046-82ee-54926ee93634",
   "metadata": {},
   "source": [
    "# Searching in all In Theme Combinations, Disregarding In Viewpoint Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a461bf-9f42-420a-b769-8c09fc47ab82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21,685,138 in theme combinations\n"
     ]
    }
   ],
   "source": [
    "# Calculating the needed combinations\n",
    "theme_f_term_dict = {}\n",
    "for f_term in f_term_tokens:\n",
    "    theme = f_term.split('/')[0]\n",
    "    vp = f_term[:8]\n",
    "    # Creating a dict entry for the theme\n",
    "    try: \n",
    "        _ = theme_f_term_dict[theme]\n",
    "    except KeyError:\n",
    "        theme_f_term_dict[theme] = {}\n",
    "\n",
    "    # Creating a dict entry for the viewpoint\n",
    "\n",
    "    try:\n",
    "        # The first dict call will def. work the second may work if the vp-dict entry \n",
    "        # was already made. If it works the theme is appended to the viewpoint dict\n",
    "        theme_f_term_dict[theme][vp].append(f_term)\n",
    "    except KeyError:\n",
    "        theme_f_term_dict[theme][vp] = []\n",
    "        \n",
    "\n",
    "n_combs = 0\n",
    "for vps in theme_f_term_dict.values():\n",
    "    n_f_terms = np.sum([len(f_terms) for f_terms in vps.values()])\n",
    "    for vp, f_terms in vps.items():\n",
    "        #print(len(f_terms), n_f_terms)\n",
    "        vp_comb = (n_f_terms - len(f_terms)) * len(f_terms)\n",
    "        n_combs += vp_comb\n",
    "        #print(vp_comb)\n",
    "print(f'There are {n_combs:,} in theme combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84339a67-6855-40ce-a328-76ac259254a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912 Generating Differences for Theme:  5F849\r"
     ]
    }
   ],
   "source": [
    "# Generating the differences\n",
    "all_diffs = []\n",
    "all_desc = []\n",
    "emb_dict = out_emb_dict\n",
    "\n",
    "for i, (theme, theme_dict) in enumerate(theme_f_term_dict.items()):\n",
    "    print(i, 'Generating Differences for Theme: ', theme, end='\\r')\n",
    "    for vp, f_terms in theme_dict.items():\n",
    "        keys = [key for key in theme_dict.keys() if key != vp]\n",
    "        # This one liner basically concatenates all f_term lists except the f_term list of the current viewpoint\n",
    "        out_vp_f_terms = [f_term for f_term_list in [theme_dict[key] for key in keys] for f_term in f_term_list]    \n",
    "        # generating the differences\n",
    "        for f_term1 in f_terms:\n",
    "            for f_term2 in out_vp_f_terms:\n",
    "                diff = emb_dict[f_term2[:-1]] - emb_dict[f_term1[:-1]]\n",
    "                diff = normalize(np.array([diff]))[0]\n",
    "                all_diffs.append(diff)\n",
    "                all_desc.append([f_term1, f_term2])\n",
    "\n",
    "all_diffs = np.array(all_diffs)\n",
    "\n",
    "print(all_diffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb732a-deaa-4247-b657-23d61fe92bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_theme(f_term_pairs, theme):\n",
    "    '''\n",
    "    Drops all f_term pairs that contain a certain theme.\n",
    "\n",
    "    returns the cleaned f_term_pairs and a boolean index\n",
    "    '''\n",
    "\n",
    "    idx = [f_term_pair[0][:5] != theme for f_term_pair in f_term_pairs]\n",
    "    f_term_pairs = [f_term_pair for f_term_pair, b in zip(f_term_pairs, idx) if b]\n",
    "    return f_term_pairs, idx\n",
    "\n",
    "\n",
    "def find_pairs(f_term_pairs, key_a, key_b, exact=False):\n",
    "    '''\n",
    "    Returns the indices of the f_term_pairs which contain key_a in the first f-term description and key_b in the description of the second f_term\n",
    "    '''\n",
    "    f_term_desc = [[number_dict[pair[0][:-1]], number_dict[pair[1][:-1]]] for pair in f_term_pairs]\n",
    "    if exact:\n",
    "        matches = [bool(pair[0].startswith(key_a) * pair[1].startswith(key_b)) for pair in f_term_desc]\n",
    "    else:\n",
    "        matches = [bool((key_a in pair[0]) * (key_b in pair[1])) for pair in f_term_desc]\n",
    "        # preventing one F-Term of the pair contains both F-Terms\n",
    "        reverse_matches = [bool((key_b in pair[0]) + (key_a in pair[1])) for pair in f_term_desc]\n",
    "        matches = [bool(match and not r_match) for match, r_match in zip(matches, reverse_matches)]\n",
    "    idx = np.arange(len(f_term_pairs))\n",
    "    idx = idx[matches]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8a8be-526d-419f-b8c0-64d71dd5d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = [\". metal\", \". adhes\"]\n",
    "\n",
    "comb_matches_idx = find_pairs(all_desc, *combination)\n",
    "comb_desc, comb_diffs = [all_desc[i] for i in comb_matches_idx], all_diffs[comb_matches_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e065e-8a8d-4e8f-91c3-463aa57364a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hits = 0\n",
    "total_searched = 0\n",
    "total_priori_s = 0\n",
    "total_priori_hits = 0\n",
    "n = 100\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "for query_vec, desc in zip(comb_diffs, comb_desc):\n",
    "    query_vec = torch.tensor(query_vec).unsqueeze(0)\n",
    "    similarities = torch.tensor([cos(torch.tensor(vec), query_vec) for vec in all_diffs[:]])\n",
    "    sorted = torch.argsort(similarities).numpy()[::-1]\n",
    "\n",
    "    guesses = []\n",
    "    # Also calculating the hits in a theme to be able to calculate how many hits there could be in total\n",
    "    theme_guesses = []\n",
    "    i = 0\n",
    "    # Iterating over all top combinations and removing the in theme combinations until at least n valid comparisons are found\n",
    "    while len(guesses) < n:\n",
    "        # Extracting the top -n f-term descriptions\n",
    "        chunk = sorted[i*n:(i+1)*n]\n",
    "        chunk_desc = [all_desc[i] for i in chunk]\n",
    "        # Removing all in theme Combinations\n",
    "        chunk_clean, idx = drop_theme(chunk_desc, desc[0][:5])\n",
    "        # idx is a boolean index\n",
    "        \n",
    "        idx = np.array([not i for i in idx])\n",
    "        theme_chunk = [i for i, b in zip(chunk_desc, idx) if b] \n",
    "        theme_guesses.extend([theme_chunk] if sum(idx)==-1 else theme_chunk)\n",
    "        guesses.extend(chunk_clean)\n",
    "                             \n",
    "    guesses = guesses[:n]\n",
    "    hits = len(find_pairs(guesses, *combination))\n",
    "    theme_hits = len(find_pairs(theme_guesses, *combination))\n",
    "    \n",
    "    total_hits += hits\n",
    "    total_searched += n\n",
    "    total_priori_s += len(all_desc) - len(theme_guesses)\n",
    "    total_priori_hits += len(comb_matches_idx) - theme_hits\n",
    "\n",
    "    print(f' Current Hits: {hits} Current Targets: {len(comb_matches_idx) - theme_hits}; Overall: a priori: {total_priori_hits*100/total_priori_s:.5f}%, a posteriori: {total_hits*100/total_searched:.5f}%; Query Description {number_dict[desc[0][:-1]]}     {number_dict[desc[1][:-1]]}', end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ea8af-1449-4e1a-b651-57f9718eff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([True, False, False])\n",
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35009bb-9b04-4ddb-8808-66423d6046de",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\". Metal\", \". Wood\", \". Polymer\", \". Cutting\", \". Bleaching\", \". Adhes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c80d24-684b-477d-b95d-e9df9f74cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084db30-4527-443a-a20d-49eddcabb4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
