{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83334be5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c66194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e49a8a3-ffc5-430a-b17a-66dcd8e4e12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/bin/python3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80fbf8",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/gal_125_1'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# The folder at which the training progress will be logged\n",
    "log_folder = 'data/models/gal_125_1/logs'\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "default_dtype = torch.float16\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cuda:0'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "n_f_terms = None # Will be calculated\n",
    "\n",
    "# Training parameters!\n",
    "output_dir=model_folder\n",
    "num_train_epochs=3\n",
    "per_device_train_batch_size=10\n",
    "per_device_eval_batch_size=10\n",
    "save_strategy=\"epoch\"\n",
    "logging_strategy=\"steps\"\n",
    "evaluation_strategy=\"steps\"\n",
    "eval_steps = 1000000 # eval_steps = number of samples until evaluation is performed.\n",
    "eval_steps = int(eval_steps/per_device_train_batch_size)\n",
    "learning_rate=5e-5\n",
    "weight_decay=0.0\n",
    "seed = 42\n",
    "\n",
    "# This that could improve performance\n",
    "dataloader_num_workers = 4\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "torch_compile = True\n",
    "# V-Ram reduction only if default_dtype= float32\n",
    "fp16=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f20e8",
   "metadata": {},
   "source": [
    "# Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a pretrained Tokenizer for the galactica model and adds an additional token for each F-Term\n",
    "tokenizer = get_tokenizer(dump_dir)\n",
    "\n",
    "# The Tokenizer contained initially 50000 Tokens which are stored as the vocab-size.\n",
    "# The vocab_size attribute is not updated when the additional tokens are added to the tokenizer\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16f301",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples in train 6385601\n",
    "# Samples in val 1596401\n",
    "\n",
    "class JapPatDataset(Dataset):\n",
    "    \"\"\"Dataset containing Japanese patents and their F-Term classification\"\"\"\n",
    "    def __init__(self, data_folder, tokenizer):\n",
    "        \"\"\"\n",
    "        data_folder: path to folder containing the text samples\n",
    "        tokenizer: tokenizer instance with added additional Tokens for F-Terms\n",
    "        \"\"\"\n",
    "        super(Dataset).__init__()\n",
    "        self.data_folder = data_folder\n",
    "        # This has to be manually set to the ammount of files in the 'dataset_samples' folder. Calculating the number of files in this folder would take forever.\n",
    "        # A to low number would lead to samples missing from the dataset.\n",
    "        # A to high number would raise a FileNotFound error.\n",
    "        self.l = len(os.listdir(data_folder))\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with open(f'{self.data_folder}/{idx}.txt', 'r', encoding='utf-8') as f:\n",
    "                item = f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError\n",
    "        \n",
    "        # Tokenizing the item \n",
    "        # The Tokenizer will return a dict with the encoded text as 'input_ids', \n",
    "        # a mask which shows the tokens types this will not be needed for our applications\n",
    "        # and a mask for the attention mechanism as 'attention_mask' The attention mask will be needed to indicate, that the \n",
    "        # model should not attend to <pad> tokens.\n",
    "        return self.tokenizer(item)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JapPatDataset(f'{dataset_folder}/train', tokenizer)\n",
    "validation_dataset = JapPatDataset(f'{dataset_folder}/validation', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pretrained model is loaded from Huggingface.\n",
    "# The token-embedding is expanded for all f-terms and the output embeddings is compleatly replaced by a F-Term classification head.\n",
    "model = load_and_modify_model(base_model_name, default_dtype, tensor_parallel, num_gpus, n_f_terms, default_device)\n",
    "print(f'The model interprets token-index {model.config.bos_token_id} as the beginning of a sequence and {model.config.eos_token_id} as the end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f51647-ad3d-4ff0-ac02-2930ca7f3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Text\n",
    "text = 'Good morning Mr.'\n",
    "# Convert text to tokens\n",
    "tokens  = tokenizer(text, return_tensors='pt').input_ids\n",
    "print(f'Output of Tokenizer: {tokens}')\n",
    "# Model generating the predicted output tokens\n",
    "out = model.generate(tokens.to(default_device), max_length=30)\n",
    "# Decoding the tokens\n",
    "\n",
    "out = tokenizer.decode(out[0])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3553e0",
   "metadata": {},
   "source": [
    "# Creating the Trainer Class by Subclassing from Huggingface-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e64f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing the Huggingface Trainer class to use custome code to calculate the loss\n",
    "# The labels used for the loss are generated and the labels for the text tokens are set to -100 to ignore their loss,\n",
    "# because the modified model can't predict text-tokens\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool=False):\n",
    "        \"\"\"\n",
    "        model: model which should be trained.\n",
    "        inputs: A padded batch of samples from the dataset.\n",
    "        return_outputs: Indicates if the whole output of the model is returned or not.\n",
    "        \"\"\"\n",
    "        # Removing the token_type_ids because we don't need them\n",
    "        try:\n",
    "            inputs.pop('token_type_ids')\n",
    "        except KeyError:\n",
    "            pass\n",
    "        labels = inputs['input_ids'].clone()\n",
    "        # Generating the labels, because the model can only predict F-Terms but also can interpret Text-Tokens as input, \n",
    "        # The maximum token idx is 50000 higher than the maximum output_idx\n",
    "        labels = labels - 50000\n",
    "        # All text tokens have token_idx below 50000 after substracting 50000 they are negative and \n",
    "        # are now set to -100 to ignore them when the loss is computed\n",
    "        labels[labels<0] = -100\n",
    "        # generating the output of the model\n",
    "        # It is a dict of 'loss', 'logits' and 'past_key_values'\n",
    "        outputs = model(**inputs, output_attentions=False, output_hidden_states=False, return_dict=True, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        print('loss', loss)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a1ee8-3dcc-49c5-b031-39b98671e103",
   "metadata": {},
   "source": [
    "# Finding the Optimal Batch-Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f15410-1a5e-4a44-9a05-96b9514704c8",
   "metadata": {},
   "source": [
    "# Extracting the number of tokens per sample\n",
    "\n",
    "file_indices = np.arange(train_dataset.l)\n",
    "empty_df = pd.DataFrame(file_indices, columns=['Files'])\n",
    "lengths_df = dd.from_pandas(empty_df, chunksize=100000)\n",
    "\n",
    "def load_file(index):\n",
    "    with open(f'{train_dataset.data_folder}/{index}.txt', 'r', encoding='utf-8') as f:\n",
    "        txt = f.read()\n",
    "    tokens = train_dataset.tokenizer(txt)\n",
    "    return len(tokens['input_ids'])\n",
    "\n",
    "lengths_df['Lengths'] = lengths_df['Files'].map(load_file)\n",
    "lengths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ff2bb2-9e22-447b-a199-598bfb34dca2",
   "metadata": {},
   "source": [
    "# Finding the longest sample\n",
    "n_tokens = 0\n",
    "max_l = 0\n",
    "for i, l in enumerate(lengths_df['Lengths']):\n",
    "    n_tokens += l\n",
    "    if l > max_l:\n",
    "        max_l = l\n",
    "    if i%1000 == 0:\n",
    "        print(f'Processed {i} samples, maximum found lengths = {max_l}, number of total tokens = {n_tokens}', end= '\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0d8d7-b213-469c-993d-a541afad70b6",
   "metadata": {},
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, l):\n",
    "        self.l = 100\n",
    "        self.sample = {'input_ids': [int(x) for x in np.random.randint(50000, 400000, size=l)],  'attention_mask': [1 for _ in range(l)]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sample\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614efa6-73c7-42f3-967c-48b8d6c29af2",
   "metadata": {},
   "source": [
    "max_l = 1279"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a9e7c-306b-4c89-b58a-9e38ffd92e58",
   "metadata": {},
   "source": [
    "# Iterating over increcingly higher batch-sizes untill the maximum is found, starting batch_size is 100\n",
    "dummy_ds = DummyDataset(max_l)\n",
    "for batch_size in range(4, 1, -1):\n",
    "    print(f'Testing batch_size {batch_size}', end='\\r')\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,          # output directory\n",
    "        num_train_epochs=1,              # total # of training epochs\n",
    "        per_device_train_batch_size=batch_size,    # batch size per device during training\n",
    "        save_strategy=save_strategy,\n",
    "        logging_strategy=logging_strategy,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay)\n",
    "\n",
    "    trainer = CustomTrainer(model=model, args=training_args, train_dataset=dummy_ds, eval_dataset=dummy_ds, data_collator=DataCollatorWithPadding(tokenizer, return_tensors='pt'))\n",
    "\n",
    "    try: \n",
    "        trainer.train()\n",
    "    except OutOfMemoryError:\n",
    "        continue\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f4614-fea2-4416-b417-19da3de9ad42",
   "metadata": {},
   "source": [
    "batch_size = 10\n",
    "\n",
    "for n_tokens in range(450, 1000, 10):\n",
    "    print(f'Maximum number of tokens per sample for batchsize {batch_size} = {n_tokens}')\n",
    "\n",
    "    max_l = n_tokens\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    dummy_ds = DummyDataset(max_l)\n",
    "    training_args = TrainingArguments(\n",
    "            output_dir=output_dir,          # output directory\n",
    "            num_train_epochs=1,              # total # of training epochs\n",
    "            per_device_train_batch_size=batch_size,    # batch size per device during training\n",
    "            save_strategy=save_strategy,\n",
    "            logging_strategy=logging_strategy,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay)\n",
    "    \n",
    "    trainer = CustomTrainer(model=model, args=training_args, train_dataset=dummy_ds, eval_dataset=dummy_ds, data_collator=DataCollatorWithPadding(tokenizer, return_tensors='pt'))\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5c4ea-7fd9-42e7-8a7f-64052fae873c",
   "metadata": {},
   "source": [
    "max_bs_for_1279 = 3\n",
    "maximum tokens per sample for batch size 4 = 1034\n",
    "maximum tokens per sample for batch size 5 = 864\n",
    "maximum tokens per sample for batch size 6 = 740\n",
    "maximum tokens per sample for batch size 7 = 650\n",
    "maximum tokens per sample for batch size 8 = 570 \n",
    "maximum tokens per sample for batch size 9 = 520\n",
    "maximum tokens per sample for batch size 10 = 470"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd386c2",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017669a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TrainingArguments class is a class which stores multiple parameters for the Custom-trainer of the model.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,          \n",
    "    logging_dir=log_folder,\n",
    "    num_train_epochs=num_train_epochs,              # total # of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size,    # batch size per device during training\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    save_strategy=save_strategy,\n",
    "    logging_strategy=logging_strategy,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    seed=seed,\n",
    "    dataloader_num_workers=dataloader_num_workers, \n",
    "    fp16=fp16,\n",
    "    torch_compile=torch_compile\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8425ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=validation_dataset, data_collator=DataCollatorWithPadding(tokenizer, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211aeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a280c92-eb78-4264-82ff-a426ebc6b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(python.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9052a7-8ed6-4491-a70f-0e091413a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed909568",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305f94a-9263-492e-82bc-c0a81abba605",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = 'data/models/gal_125_1/logs'\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "default_dtype = torch.float16\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cuda:0'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "n_f_terms = None # Will be calculated\n",
    "\n",
    "# Training parameters!\n",
    "output_dir=model_folder\n",
    "num_train_epochs=3\n",
    "per_device_train_batch_size=10\n",
    "per_device_eval_batch_size=10\n",
    "save_strategy=\"epoch\"\n",
    "logging_strategy=\"steps\"\n",
    "evaluation_strategy=\"steps\"\n",
    "eval_steps = 1000000 # eval_steps = number of samples until evaluation is performed.\n",
    "eval_steps = int(eval_steps/per_device_train_batch_size)\n",
    "learning_rate=5e-5\n",
    "weight_decay=0.0\n",
    "seed = 42\n",
    "\n",
    "# This that could improve performance\n",
    "dataloader_num_workers = 4\n",
    "torch_compile = True\n",
    "# V-Ram reduction\n",
    "fp16=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
