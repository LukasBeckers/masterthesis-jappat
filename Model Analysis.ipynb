{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e5a328-7461-4e78-8862-96de705dcbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 16:33:35.339058: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-24 16:33:35.445359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-24 16:33:36.033717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3.10.0 (default, Jul 12 2023, 08:49:30) [GCC 12.2.0]',\n",
       " '/home/worker/.pyenv/versions/3.10.0/bin/python3.10')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "sys.version, sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f064cd55-6c4c-4d0d-a4a7-9faf5e9ae877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just calculate needed'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices = ['calculate all', 'ask for userinput', 'just calculate needed']\n",
    "calculation_profile =  choices[2]\n",
    "calculation_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e8bd44-8e8c-41f8-8086-8f093520c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_125_1'\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.bfloat16\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cpu'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b06129-7179-43e2-8e3c-e8d8c2543571",
   "metadata": {},
   "source": [
    "# Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd265196-d3eb-4e46-be5a-97860017c369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadede Tokenizer from serialized instance!\n",
      "There are 378165 different F-Terms in the whole Dataset!\n"
     ]
    }
   ],
   "source": [
    "if calculation_profile == choices[0]:\n",
    "    i = 'y'\n",
    "elif calculation_profile == choices[1]:  \n",
    "    i = input(\"This creates a new tokenizer instance and saves it, if you want to proceed write y: \")\n",
    "else:\n",
    "    i = 'n'\n",
    "\n",
    "if i != 'y' and os.path.isfile(f'{tokenizer_folder}/tokenizer.json'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    print('Loadede Tokenizer from serialized instance!')    \n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "    \n",
    "else:\n",
    "    # Loads a pretrained Tokenizer for the galactica model and adds an additional token for each F-Term\n",
    "    tokenizer = get_tokenizer(dump_dir)\n",
    "    \n",
    "    # The Tokenizer contained initially 50000 Tokens which are stored as the vocab-size.\n",
    "    # The vocab_size attribute is not updated when the additional tokens are added to the tokenizer\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    tokenizer.save_pretrained(tokenizer_folder)\n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4cb1b-87a3-42d7-9cc8-358c19b7d40b",
   "metadata": {},
   "source": [
    "# Loading The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4e3dfa-af05-4a7a-a86b-35db53103afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-140000', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                           device_map=None, max_memory={'cpu': psutil.virtual_memory().available})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc43873f-570a-4ecd-b4ce-117b4e24e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapPatDataset(Dataset):\n",
    "    \"\"\"Dataset containing Japanese patents and their F-Term classification\"\"\"\n",
    "    def __init__(self, data_folder, tokenizer):\n",
    "        \"\"\"\n",
    "        data_folder: path to folder containing the text samples\n",
    "        tokenizer: tokenizer instance with added additional Tokens for F-Terms\n",
    "        \"\"\"\n",
    "        super(Dataset).__init__()\n",
    "        self.data_folder = data_folder\n",
    "        # This has to be manually set to the ammount of files in the 'dataset_samples' folder. Calculating the number of files in this folder would take forever.\n",
    "        # A to low number would lead to samples missing from the dataset.\n",
    "        # A to high number would raise a FileNotFound error.\n",
    "        self.l = len(os.listdir(data_folder))\n",
    "        #self.l = 10000\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with open(f'{self.data_folder}/{idx}.txt', 'r', encoding='utf-8') as f:\n",
    "                item = f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError\n",
    "        \n",
    "        # Tokenizing the item \n",
    "        # The Tokenizer will return a dict with the encoded text as 'input_ids', \n",
    "        # a mask which shows the tokens types this will not be needed for our applications\n",
    "        # and a mask for the attention mechanism as 'attention_mask' The attention mask will be needed to indicate, that the \n",
    "        # model should not attend to <pad> tokens.\n",
    "        \n",
    "        output = self.tokenizer(item)  \n",
    "        output.pop('token_type_ids')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40cdb3b7-5ccb-4f6a-92a6-b8913b37d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JapPatDataset(f'{dataset_folder}/train', tokenizer)\n",
    "validation_dataset = JapPatDataset(f'{dataset_folder}/validation', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d298f0-5c60-4911-a9fa-a2c977818163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_accurracy(batch, model, top_k=1):\n",
    "    \"\"\"\n",
    "    Classic prediction accuracy metric. \n",
    "    This function should be applied to a batch of samples,\n",
    "    which were tokenized by a tokenizer instance.\n",
    "    This function returns the procentual accuracy metric as well as the total number of correct predictions\n",
    "    and the total number of predictions in this batch\n",
    "\n",
    "    :batch: batch of samples from validation dataset\n",
    "    :model: model which should be testet\n",
    "    :top_k: top k predictions which should be investigated for a correct result.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(**batch, output_hidden_states=False, return_dict=True)['logits']\n",
    "\n",
    "    input_ids = batch['input_ids']\n",
    "    labels = input_ids[:, 1:]\n",
    "    print(input_ids.shape, labels.shape)\n",
    "    print(logits)\n",
    "    \n",
    "\n",
    "class Batch_DataLoader():\n",
    "    \"\"\"\n",
    "    This class converts a dataset to a iterable dataloader, which loads padded patches of data.   \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset, \n",
    "                 batchsize=10,\n",
    "                 datacollator=DataCollatorWithPadding(tokenizer, return_tensors='pt')):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.l = len(dataset)//batchsize + 1\n",
    "        self.datacollator = datacollator\n",
    "        self.current = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = [self.dataset[i] for i in range(self.current, self.current+self.batchsize)]\n",
    "        batch = self.datacollator(batch)\n",
    "        self.current += self.batchsize\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583a2b13-d64d-4c5f-bc1e-b03539e853ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 319]) torch.Size([100, 318])\n",
      "tensor([[[ 1.3770e-01,  1.7266e+00, -3.2031e-01,  ..., -1.8047e+00,\n",
      "          -2.6406e+00, -2.3750e+00],\n",
      "         [ 9.7500e+00, -2.6719e+00, -2.0625e+00,  ...,  6.2109e-01,\n",
      "           5.7031e-01,  8.5547e-01],\n",
      "         [ 9.8750e+00, -2.2969e+00, -1.6875e+00,  ...,  7.7734e-01,\n",
      "           8.1250e-01,  8.5156e-01],\n",
      "         ...,\n",
      "         [-1.4844e+00,  1.2969e+00,  1.3428e-02,  ..., -1.2266e+00,\n",
      "          -2.5938e+00, -2.6250e+00],\n",
      "         [-1.4844e+00,  1.2969e+00,  1.3428e-02,  ..., -1.2266e+00,\n",
      "          -2.5938e+00, -2.6250e+00],\n",
      "         [-1.4844e+00,  1.2969e+00,  1.3428e-02,  ..., -1.2266e+00,\n",
      "          -2.5938e+00, -2.6250e+00]],\n",
      "\n",
      "        [[ 1.3770e-01,  1.7266e+00, -3.2031e-01,  ..., -1.8047e+00,\n",
      "          -2.6406e+00, -2.3750e+00],\n",
      "         [ 9.7500e+00, -2.6719e+00, -2.0625e+00,  ...,  6.2109e-01,\n",
      "           5.7031e-01,  8.5547e-01],\n",
      "         [ 9.8750e+00, -2.2969e+00, -1.6875e+00,  ...,  7.7734e-01,\n",
      "           8.1250e-01,  8.5156e-01],\n",
      "         ...,\n",
      "         [-1.0156e+00,  3.0859e-01, -1.0000e+00,  ..., -3.4531e+00,\n",
      "          -4.5938e+00, -4.5312e+00],\n",
      "         [-1.0156e+00,  3.0859e-01, -1.0000e+00,  ..., -3.4531e+00,\n",
      "          -4.5938e+00, -4.5312e+00],\n",
      "         [-1.0156e+00,  3.0859e-01, -1.0000e+00,  ..., -3.4531e+00,\n",
      "          -4.5938e+00, -4.5312e+00]],\n",
      "\n",
      "        [[ 1.3770e-01,  1.7266e+00, -3.2031e-01,  ..., -1.8047e+00,\n",
      "          -2.6406e+00, -2.3750e+00],\n",
      "         [ 9.7500e+00, -2.6719e+00, -2.0625e+00,  ...,  6.2109e-01,\n",
      "           5.7031e-01,  8.5547e-01],\n",
      "         [ 9.8750e+00, -2.2969e+00, -1.6875e+00,  ...,  7.7734e-01,\n",
      "           8.1250e-01,  8.5156e-01],\n",
      "         ...,\n",
      "         [ 1.8828e+00,  9.8047e-01,  9.2969e-01,  ..., -4.0312e+00,\n",
      "          -5.6562e+00, -5.4375e+00],\n",
      "         [ 1.8828e+00,  9.8047e-01,  9.2969e-01,  ..., -4.0312e+00,\n",
      "          -5.6562e+00, -5.4375e+00],\n",
      "         [ 1.8828e+00,  9.8047e-01,  9.2969e-01,  ..., -4.0312e+00,\n",
      "          -5.6562e+00, -5.4375e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.3770e-01,  1.7266e+00, -3.2031e-01,  ..., -1.8047e+00,\n",
      "          -2.6406e+00, -2.3750e+00],\n",
      "         [ 1.8000e+01, -2.4688e+00, -2.6562e+00,  ...,  3.8086e-01,\n",
      "           4.7070e-01,  2.9102e-01],\n",
      "         [ 1.8000e+01, -2.2812e+00, -2.5312e+00,  ...,  3.6133e-01,\n",
      "           4.0039e-01,  1.9043e-01],\n",
      "         ...,\n",
      "         [-2.0938e+00,  1.1953e+00,  3.0078e-01,  ..., -2.2969e+00,\n",
      "          -2.4219e+00, -3.5469e+00],\n",
      "         [-2.0938e+00,  1.1953e+00,  3.0078e-01,  ..., -2.2969e+00,\n",
      "          -2.4219e+00, -3.5469e+00],\n",
      "         [-2.0938e+00,  1.1953e+00,  3.0078e-01,  ..., -2.2969e+00,\n",
      "          -2.4219e+00, -3.5469e+00]],\n",
      "\n",
      "        [[ 1.3770e-01,  1.7266e+00, -3.2031e-01,  ..., -1.8047e+00,\n",
      "          -2.6406e+00, -2.3750e+00],\n",
      "         [ 1.8000e+01, -2.4688e+00, -2.6562e+00,  ...,  3.8086e-01,\n",
      "           4.7070e-01,  2.9102e-01],\n",
      "         [ 1.8000e+01, -2.2812e+00, -2.5312e+00,  ...,  3.6133e-01,\n",
      "           4.0039e-01,  1.9043e-01],\n",
      "         ...,\n",
      "         [-3.6406e+00, -1.0703e+00, -2.2344e+00,  ..., -4.6875e+00,\n",
      "          -4.3438e+00, -3.6094e+00],\n",
      "         [-3.6406e+00, -1.0703e+00, -2.2344e+00,  ..., -4.6875e+00,\n",
      "          -4.3438e+00, -3.6094e+00],\n",
      "         [-3.6406e+00, -1.0703e+00, -2.2344e+00,  ..., -4.6875e+00,\n",
      "          -4.3438e+00, -3.6094e+00]],\n",
      "\n",
      "        [[ 1.3770e-01,  1.7266e+00, -3.2031e-01,  ..., -1.8047e+00,\n",
      "          -2.6406e+00, -2.3750e+00],\n",
      "         [ 1.4875e+01, -2.6250e+00, -2.8125e+00,  ...,  6.3672e-01,\n",
      "           8.3984e-01,  5.1953e-01],\n",
      "         [ 1.2438e+01, -2.5781e+00, -2.8594e+00,  ...,  4.3359e-01,\n",
      "           6.1719e-01,  5.1953e-01],\n",
      "         ...,\n",
      "         [ 2.6562e+00,  1.3828e+00,  8.3984e-01,  ..., -2.8438e+00,\n",
      "          -2.9219e+00, -3.2344e+00],\n",
      "         [ 2.6562e+00,  1.3828e+00,  8.3984e-01,  ..., -2.8438e+00,\n",
      "          -2.9219e+00, -3.2344e+00],\n",
      "         [ 2.6562e+00,  1.3828e+00,  8.3984e-01,  ..., -2.8438e+00,\n",
      "          -2.9219e+00, -3.2344e+00]]])\n"
     ]
    }
   ],
   "source": [
    "validation_loader = Batch_DataLoader(validation_dataset, 100)\n",
    "\n",
    "for batch in validation_loader:\n",
    "    classic_accurracy(batch, model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23698986-6609-4f22-b652-cb3ac3f80582",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dump_dir}/full_descriptions.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3b731-b097-4479-8c3d-a89fbecb83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = validation_dataset[1]\n",
    "test_sample_text = tokenizer.decode(test_sample['input_ids'])\n",
    "test_sample_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed323fa3-ceaa-4fb9-9e35-daa98d4db4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 4\n",
    "input_ids=torch.tensor([test_sample['input_ids'][:-x]])\n",
    "attention_mask=torch.tensor([test_sample['attention_mask'][:-x]])\n",
    "print(input_ids.shape, input_ids)\n",
    "print('vocab_size', model.config.vocab_size)\n",
    "output = model(input_ids, attention_mask, output_attentions=False, output_hidden_states=False, return_dict=True)\n",
    "logits = output['logits']\n",
    "indices = torch.max(logits, dim=-1)\n",
    "indices = indices[1]\n",
    "indices += 50000\n",
    "indices, tokenizer.decode(test_sample['input_ids'][-x]), full_descriptions_dict[ tokenizer.decode(test_sample['input_ids'][-x])[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d6db1-1b76-471c-a8b9-802a2bd07758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282a0ae-56c6-420b-8ae8-b256752926f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_token = tokenizer.decode(indices[0][-1])\n",
    "targ_token = tokenizer.decode(test_sample['input_ids'][-x])\n",
    "\n",
    "print('Predicted Token: ', pred_token, 'Target Token: ', targ_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58d8e5-1ebb-4e78-aef8-b3d42b9607dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Abstract: ')\n",
    "print(tokenizer.decode(test_sample['input_ids'][:-x]))\n",
    "print(' ')\n",
    "\n",
    "print('Predicted Description: ')\n",
    "print(full_descriptions_dict[pred_token[:-1]])\n",
    "print(' ')\n",
    "print('Target Description: ')\n",
    "print(full_descriptions_dict[targ_token[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28a4ce-7470-470b-9576-75bc2699ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=torch.nn.Softmax()\n",
    "r = s(logits)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcc341-680a-4396-b4c6-5834b2979e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab15c95-6e55-40b3-af35-309d54e4b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244a118-b4b9-435f-856d-ccebed9778cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
