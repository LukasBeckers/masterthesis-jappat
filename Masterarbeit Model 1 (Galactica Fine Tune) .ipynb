{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06f7d17-99b2-42f3-a893-89bfb407dfc6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this notebook the galactica model 'mini' is altred for F-Term prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c3215",
   "metadata": {},
   "source": [
    "# Notitzen\n",
    "\n",
    "- man kann dem Model direkt embeddede tokens übergeben. Als key-word argument im foreward pass.\n",
    "- - somit vielleich embedding der label möglich\n",
    "    \n",
    "- es gibt eine classe von meta in die man (vermutlich) Galactica reinladen kann, die speziell für sequenz klassifizierung gedacht ist.\n",
    "\n",
    "- Die unter Klasse von OPTFor... ist PreTrainedmodel hier kann man die input embeddings definieren.\n",
    "\n",
    "- Man kann model in 8 bit laden key-word: load_in_8bit\n",
    "\n",
    "- Man kann mit tokenizer.add_tokens([token1,..]) neue tokens zum vocab des tokenizers hinzufügen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272e44f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "781922b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from Masterarbeit_utils import model_utils, dataset_utils\n",
    "import psutil\n",
    "import torch\n",
    "import inspect\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, OPTForSequenceClassification\n",
    "\n",
    "default_dtype = torch.float16\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero in the model configuration\n",
    "#default_device = 'cuda:0'\n",
    "default_device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac715f",
   "metadata": {},
   "source": [
    "# Downloading the Naked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f91f2f6-11d8-4aaf-8166-9efbec23fa16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A dict to map the correct model urls\n",
    "HF_MAPPING = {\n",
    "    \"mini\": (\"facebook/galactica-125m\", torch.float32),\n",
    "    \"base\": (\"facebook/galactica-1.3b\", torch.float32),\n",
    "    \"standard\": (\"facebook/galactica-6.7b\", torch.float32),\n",
    "    \"large\": (\"facebook/galactica-30b\", torch.float32),\n",
    "    \"huge\": (\"facebook/galactica-120b\", torch.float16)\n",
    "}\n",
    "\n",
    "# Configuration of the model\n",
    "model_name = 'mini'\n",
    "dtype = default_dtype\n",
    "tensor_parallel = False\n",
    "device_map = None\n",
    "# Set to zero if you use the cpu as default device\n",
    "num_gpus = 1\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "    default_dtype = torch.float32\n",
    "    dtype = default_dtype\n",
    "\n",
    "# All new torch objects will have this dtype\n",
    "torch.set_default_dtype(default_dtype)\n",
    "# Analyzing the system (code by huggingface)\n",
    "max_memory = {}\n",
    "if num_gpus > 0 and not tensor_parallel:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "         _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "# Loading the model form web / from cache\n",
    "model = OPTForCausalLM.from_pretrained(HF_MAPPING[model_name][0], torch_dtype=dtype, low_cpu_mem_usage=True, device_map=device_map, max_memory=max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355f38b",
   "metadata": {},
   "source": [
    "# Loading the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32739e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_MAPPING[model_name][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ab49f",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2af3edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Tokenizer: tensor([[34848, 16810, 14782,    36]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Good morning Mr. H. S. (1920), \"The Greatest Man in the World\", The New York'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Text\n",
    "text = 'Good morning Mr.'\n",
    "# Convert text to tokens\n",
    "tokens  = tokenizer(text, return_tensors='pt').input_ids\n",
    "print(f'Output of Tokenizer: {tokens}')\n",
    "# Model generating the predicted output tokens\n",
    "out = model.generate(tokens.to(default_device), max_length=30)\n",
    "# Decoding the tokens\n",
    "\n",
    "out = tokenizer.decode(out[0])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6128e7",
   "metadata": {},
   "source": [
    "# Extract Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e673cdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model uses a nn.Embeddings instance as token embedding. \n",
      "\n",
      "It has a dict-size of <50000>.\n",
      "It has a embedding dimension of <768>\n",
      "and a padding index of <1>.\n",
      "\n",
      "The weights have a dtype of <torch.float32>\n",
      "and are on device <cpu>\n"
     ]
    }
   ],
   "source": [
    "token_embedding = model.get_input_embeddings()\n",
    "\n",
    "print(f'''The model uses a nn.Embeddings instance as token embedding. \n",
    "\n",
    "It has a dict-size of <{token_embedding.num_embeddings}>.\n",
    "It has a embedding dimension of <{token_embedding.embedding_dim}>\n",
    "and a padding index of <{token_embedding.padding_idx}>.\n",
    "\n",
    "The weights have a dtype of <{token_embedding.weight.dtype}>\n",
    "and are on device <{token_embedding.weight.device}>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463fe5b",
   "metadata": {},
   "source": [
    "# Creating a Custom Token and F-Term Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de907a",
   "metadata": {},
   "source": [
    "### Using the weights from the original embeddig and replacing the weigths of a larger embedding instance partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d405a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(original_embedding: torch.nn.Embedding, n_f_terms: int, dtype: torch.dtype=default_dtype) -> torch.nn.Embedding:\n",
    "    \"\"\"\n",
    "    This function takes the original_embedding instance of an OPT model, \n",
    "        (nn.Embedding instance).\n",
    "    and the number of f-terms it should embedd (n_f_terms) and creates a new embedding which has \n",
    "    new weights for all f_terms stacked ontop of the old weigths used for the original tokens\n",
    "    \n",
    "    returns: torch.nn.Embedding\n",
    "    \"\"\"\n",
    "    # calculating parameters for the new embedding instance\n",
    "    embedding_dim = original_embedding.embedding_dim\n",
    "    num_embeddings = original_embedding.num_embeddings + n_f_terms\n",
    "    padding_idx = original_embedding.padding_idx\n",
    "    \n",
    "    # creating new embedding (compleately untrained)\n",
    "    embedding = torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx)\n",
    "    # extracting the weigths of the original pretrained embeddign\n",
    "    old_weights = original_embedding.weight\n",
    "    new_weights = embedding.weight\n",
    "    \n",
    "    # replacing a chunk of the new parameters with te old parameters to retain the ability to encode natrual language tokens\n",
    "    embedding.weight = torch.nn.Parameter(\n",
    "                        torch.cat([old_weights.clone().to(default_device),\n",
    "                                   new_weights[original_embedding.num_embeddings:].clone().to(default_device)],\n",
    "                                  0))\n",
    "    return embedding\n",
    "    \n",
    "new_embeddings = create_embedding(token_embedding, 360000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d00381",
   "metadata": {},
   "source": [
    "### Adding the new embedding to the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "780d6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Good morning Mr.\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Output:Good morning Mr. H. S. (1920), \"The Greatest Man in the World\", The New York \n",
      "\n",
      "\n",
      "======================================================================================================\n",
      "Prompt \"translated\" by model:  cake quadr alga Rivers\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Output: MatchClinical’s, and other studies).\n",
      "\n",
      "Abstract: 1010101).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract: [START_REF] The effect of the \"Biologically-Dependent Gene Expression Profiles in the Regulation\n"
     ]
    }
   ],
   "source": [
    "# Replacing the old embedding instance with the new embedding instance in the model instance\n",
    "model.set_input_embeddings(new_embeddings)\n",
    "\n",
    "# testing using known (natural language) tokens\n",
    "out = model.generate(tokens, max_length=30)\n",
    "out = tokenizer.decode(out[0])\n",
    "print(f'Prompt: {text}\\n\\n----------------------------------------------------\\n\\nOutput:{out}', '\\n\\n')\n",
    "print('======================================================================================================')\n",
    "\n",
    "# Testing using new unknown tokens (including tokens reserved for f-terms) \n",
    "random_tokens = torch.randint(410000, [1, 50])\n",
    "random_out = model.generate(random_tokens, max_length=100)\n",
    "random_input_out = tokenizer.decode(random_out[0][:30])\n",
    "random_generated_out = tokenizer.decode(random_out[0][30:])\n",
    "print(f'Prompt \"translated\" by model: {random_input_out}\\n\\n----------------------------------------------------\\n\\nOutput: {random_generated_out}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e202dd9",
   "metadata": {},
   "source": [
    "# Creating a Custom Classification-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5659911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The old classification head is an instance of <class 'torch.nn.modules.linear.Linear'>,\n",
      "it has <768> input features \n",
      "and <50000> output features.\n",
      "The weights have a dtype of <torch.float32>\n",
      "and are on device <cpu>\n"
     ]
    }
   ],
   "source": [
    "# extracting the old classification head from the model\n",
    "old_classification_head = model.get_output_embeddings()\n",
    "# analyzing the old classification head\n",
    "print(f'''\n",
    "The old classification head is an instance of {type(old_classification_head)},\n",
    "it has <{old_classification_head.in_features}> input features \n",
    "and <{old_classification_head.out_features}> output features.\n",
    "The weights have a dtype of <{old_classification_head.weight.dtype}>\n",
    "and are on device <{old_classification_head.weight.device}>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "206e2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_classification_head(n_f_terms: int, model_dim:int) -> torch.nn.Linear:\n",
    "    \"\"\"\n",
    "    Creates a new classification head for the model\n",
    "    \n",
    "    This classification head will be a new linear layer with 'model_dim' input features and 'n_f_terms' output features\n",
    "    \"\"\"\n",
    "    return torch.nn.Linear(in_features=model_dim, out_features=n_f_terms, bias=False).to(default_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bbde525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The new classification head has <768> input features\n",
      "and <360000> output features.\n",
      "\n",
      "Its weights are in dtype <torch.float32>\n",
      "and on device <cpu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the new classification head\n",
    "new_classification_head = create_new_classification_head(360000, 768)\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "The new classification head has <{new_classification_head.in_features}> input features\n",
    "and <{new_classification_head.out_features}> output features.\n",
    "\n",
    "Its weights are in dtype <{new_classification_head.weight.dtype}>\n",
    "and on device <{new_classification_head.weight.device}>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3361c",
   "metadata": {},
   "source": [
    "### Adding the new classification head to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d55a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classification_head(_model: OPTForCausalLM, classification_head: torch.nn.Linear) -> OPTForCausalLM:\n",
    "    \"\"\"\n",
    "    This function implements the new classification head to the pretrained model.\n",
    "    \n",
    "    _model: Instanciated OPTForCausalLM model\n",
    "    classificaiton_head: New classification head for the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # changing the configuration of the model\n",
    "    vocab_size = classification_head.out_features\n",
    "    _model.config.vocab_size = vocab_size\n",
    "    _model.model.decoder.vocab_size = vocab_size\n",
    "    _model.num_labels = vocab_size\n",
    "    _model.config.num_labels = vocab_size\n",
    "    \n",
    "    # adding the classification head to the model\n",
    "    _model.set_output_embeddings(classification_head)\n",
    "    return _model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab6dba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input tokens shape: <torch.Size([1, 50])>\n",
      "Output predictions shape: <torch.Size([1, 50, 360000])>\n"
     ]
    }
   ],
   "source": [
    "# Replacing the old with the new classification head\n",
    "model = add_classification_head(model, new_classification_head)\n",
    "\n",
    "# Testing the model\n",
    "random_tokens = random_tokens.clone()\n",
    "x = model.get_input_embeddings()\n",
    "\n",
    "# cloning the tokens (could lead to cuda error otherwise)\n",
    "ip = random_tokens.clone()\n",
    "\n",
    "# generating the output\n",
    "random_out = model(ip, return_dict=1)\n",
    "\n",
    "print(f'''\n",
    "Input tokens shape: <{random_tokens.shape}>\n",
    "Output predictions shape: <{random_out['logits'].shape}>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8127f-5e22-4597-9956-ae9f5a0e7924",
   "metadata": {},
   "source": [
    "# End of file rest are just small experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfdba1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 360000]) torch.Size([1, 50])\n",
      "torch.Size([1, 49, 360000])\n",
      "torch.Size([1, 49, 360000]) torch.Size([1, 49])\n",
      "tensor([[-0.1596,  0.7767,  0.4020,  ..., -1.0422,  0.2109,  0.1856],\n",
      "        [-0.0162,  1.0445,  0.4784,  ..., -0.7289,  0.6067,  0.1513],\n",
      "        [ 0.0064,  0.8758,  0.4820,  ..., -0.8266,  0.6701,  0.1259],\n",
      "        ...,\n",
      "        [-0.1477,  0.8794,  0.4060,  ..., -0.7622,  0.6516,  0.0300],\n",
      "        [-0.1836,  0.7232,  0.1063,  ..., -0.6491,  0.6695,  0.3145],\n",
      "        [-0.0996,  0.9570,  0.3922,  ..., -0.8469,  0.5693,  0.0639]],\n",
      "       grad_fn=<ViewBackward0>) tensor([112899, 240034,  39225, 228259, 267777,   9282, 228947,  33788,  12377,\n",
      "        196954, 342823,    739, 251748, 144978,  14862,   9919, 341779,  42158,\n",
      "         57064, 137387, 187279,  75908,  84301, 236928, 106148, 332870, 196292,\n",
      "        326631, 322462,  23459, 342544, 268594,  21288, 260231, 111515, 115507,\n",
      "        211587,  25907, 197215, 182135, 200160, 223017, 191950, 220580,  51563,\n",
      "         48921, 106237, 193738,  49740])\n",
      "torch.Size([49, 360000]) torch.Size([49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(12.9256, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreating the loss function from the transformers module\n",
    "# Shift so that tokens < n predict n\n",
    "logits = random_out['logits']\n",
    "labels = torch.randint(360000, [1, 50])\n",
    "\n",
    "print(logits.shape, labels.shape)\n",
    "\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "print(shift_logits.shape)\n",
    "\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "print(shift_logits.shape, shift_labels.shape)\n",
    "\n",
    "# Flatten the tokens\n",
    "\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "print(shift_logits.view(-1, 360000), shift_labels.view(-1))\n",
    "print(shift_logits.view(-1, 360000).shape, shift_labels.view(-1).shape)\n",
    "loss = loss_fct(shift_logits.view(-1, 360000).type(torch.float32), shift_labels.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e46d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=30)\n",
    "\n",
    "for p in model.model.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddf32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.model.parameters():\n",
    "    #print(torch.mean(p).item())\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ccfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db906c41",
   "metadata": {},
   "source": [
    "# Probleme und Fragen\n",
    "\n",
    "- wird der Loss im Model berechnet, oder lasse ich mir die hidden states ausgeben und berechne den loss separat?\n",
    "    - vermutlich besser separat\n",
    "- definiere ich das Token embedding einfach neu (im Model oder gebe ich bereits embeddete Tokens in das Model?\n",
    "\n",
    "- Wie blockiere ich den Loss für meine Input sequenz?\n",
    "\n",
    "- Welche Kombination an start_sentence, stop_sentence, padding_tokens soll ich verwenden?\n",
    "    - Links oder rechts Padding?\n",
    "         - Vermutlich links\n",
    "         \n",
    "\n",
    "- Speichere ich das gesammte Datenset in Token-Form?\n",
    "    - gepadded oder nicht?\n",
    "        - padding während der batch-Erstellung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b611bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
