{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83334be5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c66194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:40:27.669118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-03 14:40:28.643455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, OPTForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80fbf8",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/gal_125_1'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "default_dtype = torch.float16\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cuda:0'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "n_f_terms = None # Will be calculated\n",
    "\n",
    "# Training parameters!\n",
    "\n",
    "output_dir=model_folder\n",
    "num_train_epochs=10\n",
    "per_device_train_batch_size=2\n",
    "save_strategy=\"epoch\"\n",
    "logging_strategy=\"epoch\"\n",
    "evaluation_strategy=\"epoch\"\n",
    "learning_rate=2e-4\n",
    "weight_decay=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f20e8",
   "metadata": {},
   "source": [
    "# Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b85e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 391452 different F-Terms in the whole Dataset!\n"
     ]
    }
   ],
   "source": [
    "# Loads a pretrained Tokenizer for the galactica model and adds an additional token for each F-Term\n",
    "tokenizer = get_tokenizer(dump_dir)\n",
    "\n",
    "# The Tokenizer contained initially 50000 Tokens which are stored as the vocab-size.\n",
    "# The vocab_size attribute is not updated when the additional tokens are added to the tokenizer\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16f301",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3b2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples in train 6385601\n",
    "# Samples in val 1596401\n",
    "\n",
    "class JapPatDataset(Dataset):\n",
    "    \"\"\"Dataset containing Japanese patents and their F-Term classification\"\"\"\n",
    "    def __init__(self, data_folder, tokenizer):\n",
    "        \"\"\"\n",
    "        data_folder: path to folder containing the text samples\n",
    "        tokenizer: tokenizer instance with added additional Tokens for F-Terms\n",
    "        \"\"\"\n",
    "        super(Dataset).__init__()\n",
    "        self.data_folder = data_folder\n",
    "        # This has to be manually set to the ammount of files in the 'dataset_samples' folder. Calculating the number of files in this folder would take forever.\n",
    "        # A to low number would lead to samples missing from the dataset.\n",
    "        # A to high number would raise a FileNotFound error.\n",
    "        self.l = 7984000 \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with open(f'{self.data_folder}/{idx}.txt', 'r', encoding='utf-8') as f:\n",
    "                item = f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError\n",
    "        \n",
    "        # Tokenizing the item \n",
    "        # The Tokenizer will return a dict with the encoded text as 'input_ids', \n",
    "        # a mask which shows the tokens types this will not be needed for our applications\n",
    "        # and a mask for the attention mechanism as 'attention_mask' The attention mask will be needed to indicate, that the \n",
    "        # model should not attend to <pad> tokens.\n",
    "        return self.tokenizer(item)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e4e3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: {'input_ids': [0, 70, 6314, 48486, 48, 2294, 1008, 286, 921, 353, 559, 345, 281, 1308, 1596, 404, 7445, 315, 281, 35044, 2481, 281, 20198, 312, 6862, 286, 20198, 321, 1262, 1336, 748, 281, 5675, 2194, 15508, 36, 41024, 2383, 21314, 48, 68, 305, 632, 1487, 10180, 243, 45, 343, 13542, 301, 286, 5675, 2194, 15508, 243, 40, 6068, 3039, 301, 281, 1470, 243, 44, 34, 835, 18774, 377, 286, 3577, 1311, 299, 286, 5675, 2194, 15508, 243, 40, 36, 381, 35044, 243, 42, 343, 7884, 2481, 286, 20198, 243, 39, 312, 286, 1336, 343, 7312, 1262, 748, 388, 299, 286, 5675, 2194, 15508, 243, 40, 34, 891, 286, 10180, 243, 45, 343, 6165, 18774, 36, 2263, 34, 286, 5675, 2194, 15508, 243, 40, 4159, 18214, 7270, 34, 891, 491, 6165, 17426, 2644, 36, 2093, 34, 286, 20198, 243, 39, 6165, 17426, 2644, 363, 286, 10180, 243, 45, 34, 891, 1308, 35, 9671, 2656, 2831, 343, 1616, 36, 50, 387, 401, 35, 11689, 2022, 52, 53060, 53061, 53062, 53063, 53064, 53065, 53066, 53067, 53068, 53069, 53070, 53071, 53072, 53073, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "dataset = JapPatDataset(dataset_folder, tokenizer)\n",
    "print(f'Example: {dataset[100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175990ac",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e185d4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.generation_logits_process'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m _CONFIG_FOR_DOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPTConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple, Union\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration_logits_process\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     EncoderNoRepeatNGramLogitsProcessor,\n\u001b[1;32m      7\u001b[0m     ExponentialDecayLengthPenalty,\n\u001b[1;32m      8\u001b[0m     ForcedBOSTokenLogitsProcessor,\n\u001b[1;32m      9\u001b[0m     ForcedEOSTokenLogitsProcessor,\n\u001b[1;32m     10\u001b[0m     ForceTokensLogitsProcessor,\n\u001b[1;32m     11\u001b[0m     HammingDiversityLogitsProcessor,\n\u001b[1;32m     12\u001b[0m     InfNanRemoveLogitsProcessor,\n\u001b[1;32m     13\u001b[0m     LogitNormalization,\n\u001b[1;32m     14\u001b[0m     LogitsProcessorList,\n\u001b[1;32m     15\u001b[0m     MinLengthLogitsProcessor,\n\u001b[1;32m     16\u001b[0m     NoBadWordsLogitsProcessor,\n\u001b[1;32m     17\u001b[0m     NoRepeatNGramLogitsProcessor,\n\u001b[1;32m     18\u001b[0m     PrefixConstrainedLogitsProcessor,\n\u001b[1;32m     19\u001b[0m     RepetitionPenaltyLogitsProcessor,\n\u001b[1;32m     20\u001b[0m     SuppressTokensAtBeginLogitsProcessor,\n\u001b[1;32m     21\u001b[0m     SuppressTokensLogitsProcessor,\n\u001b[1;32m     22\u001b[0m     TemperatureLogitsWarper,\n\u001b[1;32m     23\u001b[0m     TopKLogitsWarper,\n\u001b[1;32m     24\u001b[0m     TopPLogitsWarper,\n\u001b[1;32m     25\u001b[0m     TypicalLogitsWarper,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomOPTForCausalLM\u001b[39;00m(OPTForCausalLM):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreedy_search\u001b[39m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     32\u001b[0m         input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         synced_gpus: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[GreedySearchOutput, torch\u001b[38;5;241m.\u001b[39mLongTensor]:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.generation_logits_process'"
     ]
    }
   ],
   "source": [
    "from transformers.utils import replace_return_docstrings\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "_CONFIG_FOR_DOC = \"OPTConfig\"\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.generation_logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    ")\n",
    "\n",
    "\n",
    "class CustomOPTForCausalLM(OPTForCausalLM):\n",
    "    def greedy_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: Optional[bool] = False,\n",
    "        **model_kwargs,) -> Union[GreedySearchOutput, torch.LongTensor]:\n",
    "            r\"\"\"\n",
    "            Generates sequences of token ids for models with a language modeling head using **greedy decoding** and can be\n",
    "            used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
    "\n",
    "            Parameters:\n",
    "                input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                    The sequence used as a prompt for the generation.\n",
    "                logits_processor (`LogitsProcessorList`, *optional*):\n",
    "                    An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
    "                    used to modify the prediction scores of the language modeling head applied at each generation step.\n",
    "                stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
    "                    An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
    "                    used to tell if the generation loop should stop.\n",
    "\n",
    "                max_length (`int`, *optional*, defaults to 20):\n",
    "                    **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
    "                    tokens. The maximum length of the sequence to be generated.\n",
    "                pad_token_id (`int`, *optional*):\n",
    "                    The id of the *padding* token.\n",
    "                eos_token_id (`int`, *optional*):\n",
    "                    The id of the *end-of-sequence* token.\n",
    "                output_attentions (`bool`, *optional*, defaults to `False`):\n",
    "                    Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                    returned tensors for more details.\n",
    "                output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
    "                    Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                    for more details.\n",
    "                output_scores (`bool`, *optional*, defaults to `False`):\n",
    "                    Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
    "                return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
    "                    Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "                synced_gpus (`bool`, *optional*, defaults to `False`):\n",
    "                    Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
    "                model_kwargs:\n",
    "                    Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n",
    "                    If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
    "\n",
    "            Return:\n",
    "                [`~generation_utils.GreedySearchDecoderOnlyOutput`], [`~generation_utils.GreedySearchEncoderDecoderOutput`]\n",
    "                or `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "                [`~generation_utils.GreedySearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
    "                `return_dict_in_generate=True` or a [`~generation_utils.GreedySearchEncoderDecoderOutput`] if\n",
    "                `model.config.is_encoder_decoder=True`.\n",
    "\n",
    "            Examples:\n",
    "\n",
    "            ```python\n",
    "            >>> from transformers import (\n",
    "            ...     AutoTokenizer,\n",
    "            ...     AutoModelForCausalLM,\n",
    "            ...     LogitsProcessorList,\n",
    "            ...     MinLengthLogitsProcessor,\n",
    "            ...     StoppingCriteriaList,\n",
    "            ...     MaxLengthCriteria,\n",
    "            ... )\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "            >>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
    "            >>> model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "            >>> input_prompt = \"It might be possible to\"\n",
    "            >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList(\n",
    "            ...     [\n",
    "            ...         MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),\n",
    "            ...     ]\n",
    "            ... )\n",
    "            >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "\n",
    "            >>> outputs = model.greedy_search(\n",
    "            ...     input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
    "            ... )\n",
    "\n",
    "            >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n",
    "            ```\"\"\"\n",
    "            # init values\n",
    "            logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "            stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "            if max_length is not None:\n",
    "                warnings.warn(\n",
    "                    \"`max_length` is deprecated in this function, use\"\n",
    "                    \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "            pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "            eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "            output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "            output_hidden_states = (\n",
    "                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "            )\n",
    "            return_dict_in_generate = (\n",
    "                return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "            )\n",
    "\n",
    "            # init attention / hidden states / scores tuples\n",
    "            scores = () if (return_dict_in_generate and output_scores) else None\n",
    "            decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "            cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "            decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "            # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "            if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "                encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "                encoder_hidden_states = (\n",
    "                    model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "                )\n",
    "\n",
    "            # keep track of which sequences are already finished\n",
    "            unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "\n",
    "            this_peer_finished = False  # used by synced_gpus only\n",
    "            while True:\n",
    "                if synced_gpus:\n",
    "                    # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
    "                    # The following logic allows an early break if all peers finished generating their sequence\n",
    "                    this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "                    # send 0.0 if we finished, 1.0 otherwise\n",
    "                    dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "                    # did all peers finish? the reduced sum will be 0.0 then\n",
    "                    if this_peer_finished_flag.item() == 0.0:\n",
    "                        break\n",
    "\n",
    "                # prepare model inputs\n",
    "                model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "                # forward pass to get next token\n",
    "                outputs = self(\n",
    "                    **model_inputs,\n",
    "                    return_dict=True,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                )\n",
    "\n",
    "                if synced_gpus and this_peer_finished:\n",
    "                    continue  # don't waste resources running the code we don't need\n",
    "\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "                # pre-process distribution\n",
    "                next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "                print(f'next_token_scores: {next_token_scores}')\n",
    "\n",
    "                # Store scores, attentions and hidden_states when required\n",
    "                if return_dict_in_generate:\n",
    "                    if output_scores:\n",
    "                        scores += (next_tokens_scores,)\n",
    "                    if output_attentions:\n",
    "                        decoder_attentions += (\n",
    "                            (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                        )\n",
    "                        if self.config.is_encoder_decoder:\n",
    "                            cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "                    if output_hidden_states:\n",
    "                        decoder_hidden_states += (\n",
    "                            (outputs.decoder_hidden_states,)\n",
    "                            if self.config.is_encoder_decoder\n",
    "                            else (outputs.hidden_states,)\n",
    "                        )\n",
    "\n",
    "                # argmax\n",
    "                next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "                # finished sentences should have their next token be a padding token\n",
    "                if eos_token_id is not None:\n",
    "                    if pad_token_id is None:\n",
    "                        raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "                    next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "                # update generated ids, model inputs, and length for next step\n",
    "                input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "                model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                    outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "                )\n",
    "\n",
    "                # if eos_token was found in one sentence, set sentence to finished\n",
    "                if eos_token_id is not None:\n",
    "                    unfinished_sequences = unfinished_sequences.mul((next_tokens != eos_token_id).long())\n",
    "\n",
    "                # stop when each sentence is finished, or if we exceed the maximum length\n",
    "                if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
    "                    if not synced_gpus:\n",
    "                        break\n",
    "                    else:\n",
    "                        this_peer_finished = True\n",
    "\n",
    "            if return_dict_in_generate:\n",
    "                if self.config.is_encoder_decoder:\n",
    "                    return GreedySearchEncoderDecoderOutput(\n",
    "                        sequences=input_ids,\n",
    "                        scores=scores,\n",
    "                        encoder_attentions=encoder_attentions,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        decoder_attentions=decoder_attentions,\n",
    "                        cross_attentions=cross_attentions,\n",
    "                        decoder_hidden_states=decoder_hidden_states,\n",
    "                    )\n",
    "                else:\n",
    "                    return GreedySearchDecoderOnlyOutput(\n",
    "                        sequences=input_ids,\n",
    "                        scores=scores,\n",
    "                        attentions=decoder_attentions,\n",
    "                        hidden_states=decoder_hidden_states,\n",
    "                    )\n",
    "            else:\n",
    "                return input_ids\n",
    "\n",
    "    \n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`OPTTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n",
    "                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n",
    "\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "\n",
    "        >>> model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "        >>> tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        print('Forward-Call !!!!!')\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        logits = self.lm_head(outputs[0]).contiguous()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf545e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokenizer(dump_dir):\n",
    "    \n",
    "    # Loading the dict containing all unique f-terms in the datase\n",
    "    with open(f'{dump_dir}/f_terms_in_ds_dir.pk', 'rb') as f:\n",
    "        f_term_dict = pk.load(f)\n",
    "        \n",
    "    # Loading a dict, which contains all uniqe f-terms with crawled definitions\n",
    "    with open(f'{dump_dir}/f_term_dict.pk', 'rb') as f:\n",
    "        definitions = pk.load(f)\n",
    "        \n",
    "    # Loading the original tokenizer for the galactica model\n",
    "    tokenizer = load_pretrained_Tokenizer('mini')\n",
    "    \n",
    "    # Checking for which f-term form the dataset a f-term definition is present\n",
    "    exceptions = {}\n",
    "    exceptions_l = 0\n",
    "    for i, key in enumerate(f_term_dict.keys()):\n",
    "        try: \n",
    "            _ = definitions[key]\n",
    "            exceptions[key] = 0\n",
    "        except KeyError:\n",
    "            exceptions[key] = 1\n",
    "            exceptions_l += 1\n",
    "    \n",
    "    unique_tokens = [key +',' for key, value in exceptions.items() if value ==0] \n",
    "    tokenizer.add_tokens(unique_tokens)\n",
    "    # Adding the start_sequence, end_sequence and padding tokens to the tokenizer\n",
    "    tokenizer.pad_token = '<pad>'\n",
    "    tokenizer.bos_token = '<s>'\n",
    "    tokenizer.eos_token = '</s>'\n",
    "    tokenizer.bos_token_id = 0\n",
    "    tokenizer.eos_token_id = 2\n",
    "    tokenizer.pad_token_id = 1\n",
    "    tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    \tsingle=tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    \tspecial_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id), (tokenizer.bos_token, tokenizer.bos_token_id)],\n",
    "\t)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_name: str, dtype: torch.dtype, tensor_parallel: bool, num_gpus: int) -> OPTForCausalLM:\n",
    "    \"\"\"\n",
    "    Loads a pretrained model in the OPT structure\n",
    "\n",
    "    :return: OPTForCausalLM with pretrained weights\n",
    "    \"\"\"\n",
    "    if num_gpus > 1:\n",
    "        tensor_parallel = True\n",
    "\n",
    "    # will probably never need a device map\n",
    "    device_map=None\n",
    "\n",
    "    # A dict to map the correct model urls\n",
    "    HF_MAPPING = {\n",
    "        \"mini\": (\"facebook/galactica-125m\", torch.float32),\n",
    "        \"base\": (\"facebook/galactica-1.3b\", torch.float32),\n",
    "        \"standard\": (\"facebook/galactica-6.7b\", torch.float32),\n",
    "        \"large\": (\"facebook/galactica-30b\", torch.float32),\n",
    "        \"huge\": (\"facebook/galactica-120b\", torch.float16)}\n",
    "\n",
    "    # Analyzing the system (code by huggingface)\n",
    "    max_memory = {}\n",
    "    if num_gpus > 0 and not tensor_parallel:\n",
    "        # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "        for i in range(num_gpus):\n",
    "            _ = torch.tensor([0], device=i)\n",
    "        for i in range(num_gpus):\n",
    "            max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "        device_map = \"auto\"\n",
    "    max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "    # Loading the model form web / from cache\n",
    "    model = CustomOPTForCausalLM.from_pretrained(HF_MAPPING[model_name][0], torch_dtype=dtype, low_cpu_mem_usage=True,\n",
    "                                           device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrained_Tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    :param model_name:  Name of the matching pretrained model\n",
    "    :return:            Tokenizer matching to the pretrained model\n",
    "    \"\"\"\n",
    "\n",
    "    # A dict to map the correct model urls\n",
    "    HF_MAPPING = {\n",
    "        \"mini\": (\"facebook/galactica-125m\", torch.float32),\n",
    "        \"base\": (\"facebook/galactica-1.3b\", torch.float32),\n",
    "        \"standard\": (\"facebook/galactica-6.7b\", torch.float32),\n",
    "        \"large\": (\"facebook/galactica-30b\", torch.float32),\n",
    "        \"huge\": (\"facebook/galactica-120b\", torch.float16)}\n",
    "\n",
    "    return AutoTokenizer.from_pretrained(HF_MAPPING[model_name][0])\n",
    "\n",
    "\n",
    "def extract_embedding(model):\n",
    "    \"\"\"\n",
    "    :param model:  Loaded Pretrained model\n",
    "    :return:       Token embeddings\n",
    "    \"\"\"\n",
    "    return model.get_input_embeddings()\n",
    "\n",
    "\n",
    "def create_embedding(original_embedding: torch.nn.Embedding, n_f_terms: int, device: str) -> torch.nn.Embedding:\n",
    "    \"\"\"\n",
    "    This function takes the original_embedding instance of an OPT model,\n",
    "        (nn.Embedding instance).\n",
    "    and the number of f-terms it should embedd (n_f_terms) and creates a new embedding which has\n",
    "    new weights for all f_terms stacked ontop of the old weigths used for the original tokens\n",
    "\n",
    "    returns: torch.nn.Embedding\n",
    "    \"\"\"\n",
    "    # calculating parameters for the new embedding instance\n",
    "    embedding_dim = original_embedding.embedding_dim\n",
    "    num_embeddings = original_embedding.num_embeddings + n_f_terms\n",
    "    padding_idx = original_embedding.padding_idx\n",
    "\n",
    "    # creating new embedding (compleately untrained)\n",
    "    embedding = torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx)\n",
    "    # extracting the weigths of the original pretrained embeddign\n",
    "    old_weights = original_embedding.weight\n",
    "    new_weights = embedding.weight\n",
    "\n",
    "    # replacing a chunk of the new parameters with te old parameters\n",
    "    # to retain the ability to encode natrual language tokens\n",
    "    embedding.weight = torch.nn.Parameter(\n",
    "        torch.cat([old_weights.clone().to(device),\n",
    "                   new_weights[original_embedding.num_embeddings:].clone().to(device)],\n",
    "                  0))\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def modify_embeddings(model: OPTForCausalLM, n_f_terms: int, device: str) -> OPTForCausalLM:\n",
    "    original_embeddings = extract_embedding(model)\n",
    "    new_embeddings = create_embedding(original_embeddings, n_f_terms, device)\n",
    "    # Replacing the old embedding instance with the new embedding instance in the model instance\n",
    "    model.set_input_embeddings(new_embeddings)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_new_classification_head(n_f_terms: int, model_dim: int, dtype: torch.dtype, device: str) -> torch.nn.Linear:\n",
    "    \"\"\"\n",
    "    Creates a new classification head for the model\n",
    "\n",
    "    This classification head will be a new linear layer with 'model_dim' input features and 'n_f_terms' output features\n",
    "    \"\"\"\n",
    "    print(device)\n",
    "    return torch.nn.Linear(in_features=model_dim, out_features=n_f_terms, bias=False).to(device)\n",
    "\n",
    "\n",
    "def add_classification_head(_model: OPTForCausalLM, classification_head: torch.nn.Linear) -> OPTForCausalLM:\n",
    "    \"\"\"\n",
    "    This function implements the new classification head to the pretrained model.\n",
    "\n",
    "    _model: Instanciated OPTForCausalLM model\n",
    "    classificaiton_head: New classification head for the model\n",
    "    \"\"\"\n",
    "\n",
    "    # changing the configuration of the model\n",
    "    vocab_size = classification_head.out_features\n",
    "    _model.config.vocab_size = vocab_size\n",
    "    _model.model.decoder.vocab_size = vocab_size\n",
    "    _model.num_labels = vocab_size\n",
    "    _model.config.num_labels = vocab_size\n",
    "\n",
    "    # adding the classification head to the model\n",
    "    _model.set_output_embeddings(classification_head)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def change_classification_head(model: OPTForCausalLM, n_f_terms: int, dtype: torch.dtype, device: str):\n",
    "    \"\"\"\n",
    "    :param model:       Model which classification head should be changed\n",
    "    :param n_f_terms:   Number of different F-terms in dataset\n",
    "    :param dtype:       dtype of the model\n",
    "    :return:            OPTForCausalLM with changed classification head\n",
    "    \"\"\"\n",
    "    emb = extract_embedding(model)\n",
    "    model_dim = emb.embedding_dim\n",
    "    classification_head = create_new_classification_head(n_f_terms, model_dim, dtype, device)\n",
    "    return add_classification_head(model, classification_head)\n",
    "\n",
    "\n",
    "def load_and_modify_model(model_name: str,\n",
    "                          dtype: torch.dtype,\n",
    "                          tensor_parallel: bool,\n",
    "                          num_gpus: int,\n",
    "                          n_f_terms,\n",
    "                          device: str) -> OPTForCausalLM:\n",
    "    \"\"\"\n",
    "    This function loads a pretrained OPT model and modifies it for F-Term prediction\n",
    "\n",
    "    :param model_name:      Name of the pretrained model to download\n",
    "    :param dtype:           DType of the model parameters\n",
    "    :param tensor_parallel: Switch to turn on model paralelization\n",
    "    :param num_gpus:        Number of GPUs the model should run on\n",
    "    :param n_f_terms:       Number of F-terms the model should be able to encode and predict\n",
    "    :param device:          Device on which the model should be loaded\n",
    "    :return:                Modified OPT model \n",
    "    \"\"\"\n",
    "    model = load_pretrained_model(model_name, dtype, tensor_parallel, num_gpus)\n",
    "    model = modify_embeddings(model, n_f_terms, device)\n",
    "    model = change_classification_head(model, n_f_terms, dtype, device)\n",
    "    return model\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pretrained model is loaded from Huggingface.\n",
    "# The token-embedding is expanded for all f-terms and the output embeddings is compleatly replaced by a F-Term classification head.\n",
    "model = load_and_modify_model(base_model_name, default_dtype, tensor_parallel, num_gpus, n_f_terms, default_device)\n",
    "print(f'The model interprets token-index {model.config.bos_token_id} as the beginning of a sequence and {model.config.eos_token_id} as the end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'A novel method to '\n",
    "inputs = tokenizer(i, return_tensors='pt')\n",
    "inputs.pop('token_type_ids')\n",
    "out = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_beams, model.config.length_penalty, model.config.early_stopping, model.config.num_beam_groups, model.config.do_sample, model.config.num_return_sequences, model.config.output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.logits_processer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3553e0",
   "metadata": {},
   "source": [
    "# Creating the Trainer Class by Subclassing from Huggingface-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e64f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing the Huggingface Trainer class to use custome code to calculate the loss\n",
    "# The labels used for the loss are generated and the labels for the text tokens are set to -100 to ignore their loss,\n",
    "# because the modified model can't predict text-tokens\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool=False):\n",
    "        \"\"\"\n",
    "        model: model which should be trained.\n",
    "        inputs: A padded batch of samples from the dataset.\n",
    "        return_outputs: Indicates if the whole output of the model is returned or not.\n",
    "        \"\"\"\n",
    "        # Removing the token_type_ids because we don't need them\n",
    "        inputs.pop('token_type_ids')\n",
    "        labels = inputs['input_ids'].clone()\n",
    "        # Generating the labels, because the model can only predict F-Terms but also can interpret Text-Tokens as input, \n",
    "        # The maximum token idx is 50000 higher than the maximum output_idx\n",
    "        labels = labels - 50000\n",
    "        # All text tokens have token_idx below 50000 after substracting 50000 they are negative and \n",
    "        # are now set to -100 to ignore them when the loss is computed\n",
    "        labels[labels<0] = -100\n",
    "        # generating the output of the model\n",
    "        # It is a dict of 'loss', 'logits' and 'past_key_values'\n",
    "        outputs = model(**inputs, output_attentions=False, output_hidden_states=False, return_dict=True, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd386c2",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017669a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TrainingArguments class is a class which stores multiple parameters for the Custom-trainer of the model.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,          # output directory\n",
    "    num_train_epochs=num_train_epochs,              # total # of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size,    # batch size per device during training\n",
    "    save_strategy=save_strategy,\n",
    "    logging_strategy=logging_strategy,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8425ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(model=model, args=training_args, train_dataset=dataset, data_collator=DataCollatorWithPadding(tokenizer, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211aeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed909568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
