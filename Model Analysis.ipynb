{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e5a328-7461-4e78-8862-96de705dcbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 11:34:20.039738: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-21 11:34:20.066497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 11:34:20.732007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3.10.0 (default, Jul 12 2023, 08:49:30) [GCC 12.2.0]',\n",
       " '/home/worker/.pyenv/versions/3.10.0/bin/python3.10')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "sys.version, sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f064cd55-6c4c-4d0d-a4a7-9faf5e9ae877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just calculate needed'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices = ['calculate all', 'ask for userinput', 'just calculate needed']\n",
    "calculation_profile =  choices[2]\n",
    "calculation_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e8bd44-8e8c-41f8-8086-8f093520c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_125_aug_1'#\n",
    "checkpoint = 148000\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.bfloat16\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cpu'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b06129-7179-43e2-8e3c-e8d8c2543571",
   "metadata": {},
   "source": [
    "# Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd265196-d3eb-4e46-be5a-97860017c369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenizer from serialized instance!\n",
      "There are 378166 different F-Terms in the whole Dataset!\n"
     ]
    }
   ],
   "source": [
    "if calculation_profile == choices[0]:\n",
    "    i = 'y'\n",
    "elif calculation_profile == choices[1]:  \n",
    "    i = input(\"This creates a new tokenizer instance and saves it, if you want to proceed write y: \")\n",
    "else:\n",
    "    i = 'n'\n",
    "\n",
    "if i != 'y' and os.path.isfile(f'{tokenizer_folder}/tokenizer.json'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    print('Loaded Tokenizer from serialized instance!')    \n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "    \n",
    "else:\n",
    "    # Loads a pretrained Tokenizer for the galactica model and adds an additional token for each F-Term\n",
    "    tokenizer = get_tokenizer(dump_dir)\n",
    "    \n",
    "    # The Tokenizer contained initially 50000 Tokens which are stored as the vocab-size.\n",
    "    # The vocab_size attribute is not updated when the additional tokens are added to the tokenizer\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    tokenizer.save_pretrained(tokenizer_folder)\n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4cb1b-87a3-42d7-9cc8-358c19b7d40b",
   "metadata": {},
   "source": [
    "# Loading The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4e3dfa-af05-4a7a-a86b-35db53103afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "         _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                           device_map=device_map, max_memory=max_memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe502a-540d-4b83-b8cc-c17ac1a51bfb",
   "metadata": {},
   "source": [
    "# Loading the Datasets and the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc43873f-570a-4ecd-b4ce-117b4e24e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapPatDataset(Dataset):\n",
    "    \"\"\"Dataset containing Japanese patents and their F-Term classification\"\"\"\n",
    "    def __init__(self, data_folder, tokenizer):\n",
    "        \"\"\"\n",
    "        data_folder: path to folder containing the text samples\n",
    "        tokenizer: tokenizer instance with added additional Tokens for F-Terms\n",
    "        \"\"\"\n",
    "        super(Dataset).__init__()\n",
    "        self.data_folder = data_folder\n",
    "        # This has to be manually set to the ammount of files in the 'dataset_samples' folder. Calculating the number of files in this folder would take forever.\n",
    "        # A to low number would lead to samples missing from the dataset.\n",
    "        # A to high number would raise a FileNotFound error.\n",
    "        self.l = len(os.listdir(data_folder)) - 1\n",
    "        #self.l = 10000\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with open(f'{self.data_folder}/{idx}.txt', 'r', encoding='utf-8') as f:\n",
    "                item = f.read()\n",
    "        except FileNotFoundError:\n",
    "            \n",
    "            raise FileNotFoundError\n",
    "        \n",
    "        # Tokenizing the item \n",
    "        # The Tokenizer will return a dict with the encoded text as 'input_ids', \n",
    "        # a mask which shows the tokens types this will not be needed for our applications\n",
    "        # and a mask for the attention mechanism as 'attention_mask' The attention mask will be needed to indicate, that the \n",
    "        # model should not attend to <pad> tokens.\n",
    "        \n",
    "        output = self.tokenizer(item)  \n",
    "        output.pop('token_type_ids')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40cdb3b7-5ccb-4f6a-92a6-b8913b37d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = JapPatDataset(f'{dataset_folder}/train', tokenizer)\n",
    "validation_dataset = JapPatDataset(f'{dataset_folder}/validation', tokenizer)\n",
    "\n",
    "# Loading a dict that contains the definitions of the f-terms\n",
    "with open(f'{dump_dir}/full_descriptions.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1dd923-2657-4994-8f6e-e50946180dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05310c6e-564d-444a-8509-7e8d57d2b258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>To provide a vehicle riding-together support system that encourages a user who desires to ride in a vehicle to adjust desired time of boarding and a desired location of the boarding, and improve formability of matching between the user and a riding-together vehicle.SOLUTION: A vehicle riding-together support system, which supports allocation of a user and a riding-together vehicle, includes: a condition information acquisition unit that acquires, with respect to the riding-together vehicle, desired condition information including a desired boarding location and a desired boarding time zone desired by the user from a user terminal device; a reservation information acquisition unit that acquires and stores reservation reception records indicating a reserved boarding location, a reserved boarding date, and a reserved boarding time zone for each of allocated reservation requests transmitted from the user terminal device; and a difficulty calculation unit that calculates, based on the above stored past reservation reception record, for each of the predetermined locations in an area within a predetermined distance range including the desired boarding place, an index value of an index indicating difficulty of establishing the above allocation for each predetermined time zone including the desired boarding time zone.SELECTED DRAWING: Figure 2',\n",
       " '                         ',\n",
       " '5L049/CC42,<END F-TERMS>')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = validation_dataset[10]\n",
    "test_text= tokenizer.decode(test_sample['input_ids'])\n",
    "test_abstract, f_term_text = test_text.split('<START F-TERMS>')\n",
    "test_abstract, '                         ',f_term_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "784e415c-596b-4173-9375-b5e349da1cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 5L049/CC42 Target 5L049/CC42\n",
      "Prediction <END F-TERMS> Target <END F-TERMS>\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, model, tokenizer, max_pred_tokens=10, decode=True):\n",
    "    # adding the Start F-Term Token to the prompt to beginng the prediction of F-Terms\n",
    "    prompt += '<START F-TERMS>'\n",
    "\n",
    "    # Converting the prompt to tokens\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    tokenized = tokenizer(prompt, return_tensors='pt')\n",
    "    prompt_tokens = tokenized['input_ids'][:,:-1]\n",
    "    attention_mask = tokenized['attention_mask'][:, :-1]\n",
    "\n",
    "    # Generating the F-Terms\n",
    "    current_token = -100\n",
    "    predictions = []\n",
    "    while current_token != eos_token_id and len(predictions) < max_pred_tokens:\n",
    "\n",
    "        # Model Call\n",
    "        output = model(prompt_tokens, attention_mask, output_attentions=False, output_hidden_states=False, return_dict=True)\n",
    "        logits = output['logits']\n",
    "        # torch.max function returns values and indices, we are just interested in the indices.\n",
    "        indices = torch.max(logits, dim=-1)[1]\n",
    "        current_token = indices[0, -1]\n",
    "        # To make the predictions match the indices in the tokenizer we must add 50000 to the prediction, because the output does not have the 50000 text-tokens\n",
    "        current_token += 50000\n",
    "        predictions.append(current_token)\n",
    "        # Adding the prediction to the input sequence to predict the new token.\n",
    "        prompt_tokens = torch.cat([prompt_tokens, indices[:, -1:]], -1)\n",
    "        # Attention mask has to be updates as well\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask[:,-1:]], -1)\n",
    "    if decode:\n",
    "        predictions = tokenizer.decode(predictions)\n",
    "        return predictions\n",
    "    else: \n",
    "        return predictions\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "predictions = generate(test_abstract, model, tokenizer, 10)\n",
    "\n",
    "for p, t in zip(predictions.split(','), f_term_text.split(',')):\n",
    "    print('Prediction', p, 'Target', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f9dad93-0b54-46b8-9ed2-f8ce61e35327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(280161)] [280161]\n",
      "[tensor(60800), tensor(60250), tensor(60800)] [155058, 153792, 105462]\n",
      "[tensor(63153)] [63153]\n",
      "[tensor(169129)] [169129]\n",
      "[tensor(69303), tensor(235553), tensor(308035)] [69303, 341263, 142136]\n",
      "[tensor(63153)] [63153]\n",
      "[tensor(51833), tensor(51833), tensor(51833), tensor(50001)] [108983, 51414, 53081, 134214, 51417, 51420]\n",
      "[tensor(63153)] [212834]\n",
      "[tensor(243157), tensor(210845), tensor(68055)] [150502, 150503, 172404]\n",
      "[tensor(60193), tensor(67618), tensor(119354), tensor(50001)] [60193, 67618, 219924, 73794, 67621, 73795, 119354, 113993, 160767, 160768]\n",
      "[tensor(293105)] [293105]\n",
      "[tensor(74052)] [74052]\n",
      "[tensor(89206)] [108382]\n",
      "[tensor(55000)] [66673]\n",
      "[tensor(212567), tensor(61641)] [85713, 81345]\n",
      "[tensor(280161)] [280161]\n",
      "[tensor(100471), tensor(100471)] [60207, 100471]\n",
      "[tensor(139556), tensor(112353), tensor(50001)] [126494, 139556, 178896, 265603, 112353]\n",
      "[tensor(96758), tensor(96758), tensor(96758), tensor(115131)] [85159, 53466, 72205, 53468]\n",
      "[tensor(70887)] [165679]\n",
      "[tensor(69303), tensor(233830), tensor(235553), tensor(308035)] [69303, 226537, 219449, 233830]\n",
      "[tensor(87807)] [70866]\n",
      "[tensor(73838), tensor(73837), tensor(73838), tensor(73838), tensor(50001)] [82791, 114385, 128068, 106479, 155058, 153792]\n",
      "[tensor(109405), tensor(50001)] [102851, 142313, 164311, 109405]\n",
      "[tensor(50394), tensor(55046), tensor(50001)] [50394, 55046, 67536]\n",
      "[tensor(69300), tensor(69299), tensor(69303), tensor(69301), tensor(50001)] [69299, 69300, 123036, 69301, 106465, 69303, 212432, 386496, 263016]\n",
      "[tensor(123042), tensor(69303), tensor(123043), tensor(50001)] [69303, 327474, 267616, 123042, 123043, 362093, 167243, 240298]\n",
      "[tensor(73740)] [77170]\n",
      "[tensor(55225), tensor(85020), tensor(55225), tensor(69932)] [57252, 69932, 55226, 69933]\n",
      "[tensor(280161)] [234491]\n",
      "[tensor(60193), tensor(67618), tensor(50001)] [60193, 188560, 67618, 219924, 119354, 119355, 160768, 90948]\n",
      "[tensor(64328), tensor(64328), tensor(64328), tensor(55249), tensor(64328), tensor(55249), tensor(55975), tensor(55975), tensor(55975)] [63306, 64328, 87672, 55244, 55246, 63310, 81575, 55974, 55975]\n",
      "[tensor(165703), tensor(91606)] [235986, 217140]\n",
      "[tensor(58739), tensor(58740), tensor(59399), tensor(86777), tensor(50001)] [64293, 58739, 58740, 68196, 60596, 85512]\n",
      "[tensor(95811), tensor(50001)] [95811, 241963]\n",
      "[tensor(192987), tensor(50001)] [227865, 192987, 369514]\n",
      "[tensor(58740), tensor(58739), tensor(58735), tensor(50001)] [60199, 58735, 59399, 133013, 87656, 92108, 58739, 59402, 60596]\n",
      "[tensor(96128), tensor(96128), tensor(96128)] [154800, 88142, 105462]\n",
      "[tensor(60725), tensor(159127)] [148727, 65532]\n",
      "[tensor(63608), tensor(55369), tensor(63608), tensor(50001)] [80131, 64808, 55369, 63608, 137993, 77518, 86926, 77519]\n",
      "[tensor(60250), tensor(60800), tensor(60250), tensor(60800)] [58457, 60800, 60367, 58723]\n",
      "[tensor(59327), tensor(59327)] [59332, 135710]\n",
      "[tensor(410931), tensor(409091), tensor(410931), tensor(423293), tensor(360626), tensor(410931), tensor(50001)] [410931, 360622, 343767, 360625, 360626, 360627, 412763]\n",
      "[tensor(69303), tensor(212432), tensor(50001)] [148277, 266673, 69299, 69300, 123036, 69301, 55151, 118744, 118748, 69303, 212432, 166318, 338646, 212433, 219449, 207867]\n",
      "[tensor(69303), tensor(233830), tensor(226537), tensor(50001)] [69303, 219449, 266238, 308035]\n",
      "[tensor(50394), tensor(51291), tensor(50001)] [50394, 51291, 51295]\n",
      "[tensor(50832), tensor(50830), tensor(52896), tensor(52894), tensor(50001)] [52894, 50830, 52896, 50832, 133311, 63574]\n",
      "[tensor(103509), tensor(54397), tensor(50001)] [85464, 52894, 50830, 52896, 50832, 61443, 58485, 61446, 103509, 54397, 103516, 149928, 243971, 149929, 103517]\n",
      "[tensor(77949)] [70366]\n",
      "[tensor(62809), tensor(81208), tensor(62809), tensor(81208), tensor(62809), tensor(50001)] [60187, 60189, 60190, 219923, 119352, 160764, 160765, 60193, 60195, 60196, 219924, 119354, 160767, 160768, 106381, 81208, 81209, 81422, 97040, 120899, 97041, 55281]\n",
      "[tensor(104934)] [215114]\n",
      "[tensor(58740), tensor(58739), tensor(59399), tensor(56612), tensor(59399), tensor(50001)] [89626, 56612, 68195, 73220, 58739, 58740, 60596]\n",
      "[tensor(154377), tensor(50001)] [154375, 154377]\n",
      "[tensor(70351), tensor(70360), tensor(70351), tensor(70360)] [74254, 94586, 65563, 94592]\n",
      "[tensor(83069), tensor(83069), tensor(50001)] [83069, 103041, 83076]\n",
      "[tensor(63513), tensor(110555), tensor(50001)] [65283, 110555, 63515, 62276, 55611, 128924, 62281]\n",
      "[tensor(281426), tensor(50001)] [234491, 306216]\n",
      "[tensor(80390), tensor(104247), tensor(104247), tensor(104244), tensor(80390), tensor(50001)] [93308, 78069, 80391, 106730, 104244, 109548, 106731, 104246, 104249, 223233, 143617, 143618, 146614, 109552]\n",
      "[tensor(88861), tensor(88861), tensor(73979), tensor(50001)] [149627, 100824, 72344, 100828, 73979]\n",
      "[tensor(58304)] [176723]\n",
      "[tensor(248737)] [248737]\n",
      "[tensor(93308), tensor(93309), tensor(106730), tensor(93309), tensor(106730), tensor(50001)] [93308, 106730, 133230, 104247, 109550, 104251]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predictions, f_terms)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, test_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(validation_dataset):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#print(i, end='\\r')\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mgeneration_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m, in \u001b[0;36mgeneration_accuracy\u001b[0;34m(sample, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m abstract, f_terms_text \u001b[38;5;241m=\u001b[39m sample_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<START F-TERMS>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m f_terms \u001b[38;5;241m=\u001b[39m tokenizer(f_terms_text)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabstract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf_terms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions, f_terms)\n",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, model, tokenizer, max_pred_tokens, decode)\u001b[0m\n\u001b[1;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_token \u001b[38;5;241m!=\u001b[39m eos_token_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;241m<\u001b[39m max_pred_tokens:\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Model Call\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# torch.max function returns values and indices, we are just interested in the indices.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:930\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    927\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 930\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    944\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:696\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    688\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    689\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    690\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:326\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    323\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    334\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:171\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    168\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# get key, value proj\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# reuse k,v, cross_attentions\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generation_accuracy(sample, model):\n",
    "    \"\"\"\n",
    "    \n",
    "    Lets the model generate f-Terms for a abstact and then computes the accuracy of the predicted F-Terms\n",
    "    \"\"\"\n",
    "    # Remove <s> and <END F-TERMS> Tokens and convert sample to text\n",
    "    sample_text = tokenizer.decode(sample['input_ids'][1:-1])\n",
    "    abstract, f_terms_text = sample_text.split('<START F-TERMS>')\n",
    "    f_terms = tokenizer(f_terms_text)['input_ids'][1:-1]\n",
    "\n",
    "    predictions = generate(abstract, model, tokenizer, len(f_terms), decode=False)\n",
    "    print(predictions, f_terms)\n",
    "\n",
    "for i, test_batch in enumerate(validation_dataset):\n",
    "    #print(i, end='\\r')\n",
    "    generation_accuracy(test_batch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4c091-86f0-4845-b314-9a4a3d082684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b91ec-f7fc-4ebd-93b4-da779747db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, test_sample in enumerate(validation_dataset):\n",
    "    test_text= tokenizer.decode(test_sample['input_ids'])\n",
    "    test_abstract, f_term_text = test_text.split('<START F-TERMS>')\n",
    "\n",
    "    predictions = generate(test_abstract, model, tokenizer, 10)\n",
    "   \n",
    "    for p, t in zip(predictions.split(','), f_term_text.split(',')):\n",
    "        print(p, t)\n",
    "        \n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d298f0-5c60-4911-a9fa-a2c977818163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_accurracy(batch, model, top_k=1):\n",
    "    \"\"\"\n",
    "    Classic prediction accuracy metric. \n",
    "    This function should be applied to a batch of samples,\n",
    "    which were tokenized by a tokenizer instance.\n",
    "    This function returns the procentual accuracy metric as well as the total number of correct predictions\n",
    "    and the total number of predictions in this batch\n",
    "\n",
    "    :batch: batch of samples from validation dataset\n",
    "    :model: model which should be testet\n",
    "    :top_k: top k predictions which should be investigated for a correct result.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(**batch, output_hidden_states=False, return_dict=True)['logits']\n",
    "\n",
    "    input_ids = batch['input_ids']\n",
    "    # Removing first label\n",
    "    labels = input_ids[:, 1:]\n",
    "    # removing last prediction\n",
    "    logits = logits[:, :-1]\n",
    "    # Sorting the logits to get the predictions orderd from highest to lowest\n",
    "    _, preds = torch.sort(logits, dim=-1, descending=True)\n",
    "    # Dropping all text predictions and labels, keeping just the predictions and labels for f-terms\n",
    "    token_threshold = 50000 # tokens with an ids_value below the token_threshold are removed\n",
    "    preds = preds[labels > token_threshold]\n",
    "    # only taking the top k predictions:\n",
    "    preds = preds[:,:top_k]\n",
    "    # the predictions are missing the 50000 text tokens so the predictions have to be increased by 50000\n",
    "    preds += 50000\n",
    "    labels = labels[labels > token_threshold]\n",
    "    n_preds = labels.shape[0] # number of values that are to be predicted\n",
    "\n",
    "    # expanding the labels to the same size as the predictions\n",
    "    labels = labels.unsqueeze(dim=-1)\n",
    "    labels = labels.expand(-1, top_k)\n",
    "    n_correct = len(labels[labels == preds])\n",
    "    accuracy = 100*n_correct/n_preds\n",
    "    return accuracy, n_correct, n_preds\n",
    "\n",
    "class Batch_DataLoader():\n",
    "    \"\"\"\n",
    "    This class converts a dataset to a iterable dataloader, which loads padded patches of data.   \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset, \n",
    "                 batchsize=10,\n",
    "                 datacollator=DataCollatorWithPadding(tokenizer, return_tensors='pt')):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.l = len(dataset)//batchsize + 1\n",
    "        self.datacollator = datacollator\n",
    "        self.current = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = [self.dataset[i] for i in range(self.current, self.current+self.batchsize)]\n",
    "        batch = self.datacollator(batch)\n",
    "        self.current += self.batchsize\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a2b13-d64d-4c5f-bc1e-b03539e853ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_on_dataset(dataset, model, top_k=1, batch_size=10):\n",
    "    loader = Batch_DataLoader(dataset, batch_size)\n",
    "    n_pred = 0\n",
    "    n_corr = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        acc, corr, pred = classic_accurracy(batch, model, top_k)\n",
    "        n_pred += pred\n",
    "        n_corr += corr\n",
    "        print(f'batch_acc: {acc:.2f}%, total_acc: {100*n_corr/n_pred:.2f}% batch {i}/{len(loader)}', end ='\\r')\n",
    "    return n_pred, n_corr\n",
    "    \n",
    "\n",
    "n_pred, n_corr = accuracy_on_dataset(validation_dataset, model, top_k=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23698986-6609-4f22-b652-cb3ac3f80582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244a118-b4b9-435f-856d-ccebed9778cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
