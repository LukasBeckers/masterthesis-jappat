{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9a8f9-369c-4fed-b65b-2e9b37c4062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils_agg import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "\n",
    "# Site-Packages\n",
    "import itertools\n",
    "import difflib\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bokeh\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from cProfile import Profile\n",
    "from pstats import SortKey, Stats\n",
    "\n",
    "# Dimension reduction algorithms\n",
    "#from cuml.manifold import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from scipy.fft import fft, fftfreq\n",
    "# Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import export_png\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1974c-cce1-4ca3-8afd-fc2ad038662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_125_agg_aug_1'\n",
    "checkpoint = 100_000\n",
    "# If True normalization is applied to the embeddings\n",
    "norm = True\n",
    "seq_class = False\n",
    "context_less = False\n",
    "\n",
    "if seq_class:\n",
    "    # Importing code for sequence classification\n",
    "    from Masterarbeit_utils.model_utils_seq_class import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "    from transformers import OPTForSequenceClassification\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/agg_dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cuda:0'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "if seq_class:\n",
    "    model = OPTForSequenceClassification.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "else:\n",
    "    model = OPTForCausalLM.from_pretrained(f'{model_folder}/checkpoint-{checkpoint}', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)\n",
    "\n",
    "###########################\n",
    "# Loading the Tokenizer\n",
    "###########################\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "print('Loaded Tokenizer from serialized instance!')    \n",
    "print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "###########################\n",
    "# Loading Descriptions\n",
    "###########################\n",
    "with open(f'{dump_dir}/agg_themes_descriptions.pk', 'rb') as f:\n",
    "    theme_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_viewpoints_descriptions.pk', 'rb') as f:\n",
    "    viewpoint_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_numbers_descriptions.pk', 'rb') as f:\n",
    "    number_dict = pk.load(f)\n",
    "with open(f'{dump_dir}/agg_full_descriptions.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)\n",
    "\n",
    "\n",
    "###########################\n",
    "# Extracting the Embeddings\n",
    "###########################\n",
    "\n",
    "# Extracting the classification Head weights\n",
    "inp_emb = model.get_input_embeddings()\n",
    "\n",
    "if not seq_class:\n",
    "    #Embeddings if the model is not a sequence classification model\n",
    "    out_emb = model.get_output_embeddings()\n",
    "    out_emb = next(out_emb.parameters()).to('cpu').detach().numpy()[2:]\n",
    "    inp_emb = inp_emb(torch.arange(len(tokenizer))).to('cpu').detach().numpy()[50002:]\n",
    "\n",
    "    if context_less:\n",
    "        # Extracting context less embeddings\n",
    "        if not os.path.isfile(f'{model_folder}/context_less_emb{checkpoint}.pk'):\n",
    "            print('Calculating context less embeddings!')\n",
    "            context_less_emb = [[] for _ in range(13)]\n",
    "            for i in range(len(tokenizer)):\n",
    "                print(i, end='\\r')\n",
    "                out = model(input_ids= torch.tensor([[i]]), attention_mask = torch.tensor([[1]]), output_hidden_states=True)\n",
    "                \n",
    "                out = out.hidden_states\n",
    "                for i, k in enumerate(out):\n",
    "                    context_less_emb[i].append(k.to('cpu').detach().numpy())\n",
    "            with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'wb') as f:\n",
    "                pk.dump(context_less_emb, f)\n",
    "        else:\n",
    "            print('Loading context less embeddings from disk')\n",
    "            with open(f'{model_folder}/context_less_emb{checkpoint}.pk', 'rb') as f:\n",
    "                context_less_emb = pk.load(f)\n",
    "        \n",
    "        # Combining context less embeddings of a layer to a single tensor\n",
    "        for i, layer in enumerate(context_less_emb):\n",
    "            layer = [e[0] for e in layer]\n",
    "            layer = np.concatenate(layer, 0)\n",
    "            context_less_emb[i] = layer\n",
    "\n",
    "else: \n",
    "    # embeddings if the model is a Sequence Classifier\n",
    "    inp_emb = inp_emb(torch.arange(50000)).to('cpu').detach().numpy()\n",
    "    out_emb = model.score.weight\n",
    "    out_emb.to('cpu').detach().numpy()\n",
    "    \n",
    "\n",
    "## Normalizing the embeddings \n",
    "def normalize(tensor):\n",
    "    if norm:\n",
    "        return torch.nn.functional.normalize(torch.tensor(tensor), p=2).numpy()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "out_emb = normalize(out_emb)\n",
    "inp_emb = normalize(inp_emb)\n",
    "if not seq_class:\n",
    "    if context_less:\n",
    "        context_less_emb = [normalize(layer) for layer in context_less_emb]\n",
    "\n",
    "# Extracting the matching F_terms for the weights and creating lists with the defintions\n",
    "tokens = [tokenizer.decode(i) for i in range(len(tokenizer))]\n",
    "f_term_tokens = tokens[50002:]\n",
    "\n",
    "# Creating  a dict with f-Terms and their embedding vectors:\n",
    "out_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, out_emb)}\n",
    "ft_emb_dict = {key: np.abs(fft(value)) for key, value in out_emb_dict.items()}\n",
    "if seq_class:\n",
    "    inp_emb_dict = {token[:-1]: vec for token, vec in zip(tokens[:50000], inp_emb)}\n",
    "else: \n",
    "    inp_emb_dict = {token[:-1]: vec for token, vec in zip(f_term_tokens, inp_emb)}\n",
    "    \n",
    "# Creating Context Less Embedding Dicts\n",
    "if context_less:\n",
    "    if not seq_class:\n",
    "        context_less_dicts = []\n",
    "        for layer in context_less_emb:\n",
    "            context_less_dicts.append({token[:-1]: vec for token, vec in zip(tokens, layer)})\n",
    "    \n",
    "####################################################\n",
    "# Detecting F-Term Triangles with Supposed Similarities\n",
    "####################################################\n",
    "df=pd.read_csv(\"data/f-terms.csv\", index_col=0)\n",
    "\n",
    "#subset with \"material\" in the viewpoint description\n",
    "df[\"vp\"]=df.theme+\"/\"+df.viewpoint\n",
    "df[\"fterm\"]=df.theme+\"/\"+df.number\n",
    "df2=df.copy()\n",
    "# Aggregating the F-Terms\n",
    "\n",
    "agg_rows = [row for i, row in df2.iterrows() if row.fterm in out_emb_dict.keys()]\n",
    "df2 = pd.DataFrame(agg_rows)\n",
    "\n",
    "#f-term descriptions are searched for the following materials\n",
    "materials_list=[\". Metal\", \". Wood\", \". Polymer\", \". Glass\", \". Wool\", \". Cutting\", \". Bleaching\", \". Adhes\", \". Heat insulat\", \". Heat radiation\", \". Cooling\", \". Insulat\", \". Coating\", \". Rubber\", \". Rota\", \". Compress\", \". Bolt\", \"Welding\" ]\n",
    "materials_f_terms={}\n",
    "for material in materials_list:\n",
    "    materials_f_terms[material]=[df2[df2.label.str.contains(material.lower()[2:], na=False, case=False)].theme.unique(), df2[df2.label.str.contains(material.lower()[2:], na=False, case=False)].fterm.values]\n",
    "    \n",
    "    \n",
    "#pairs of two materials with fterms in same viewpoints are created\n",
    "material_combinations={}\n",
    "for mat1 in materials_list:\n",
    "    for mat2 in materials_list:\n",
    "        for mat3 in materials_list:\n",
    "            \n",
    "            if len(set([mat1,mat2,mat3]))!=3: continue\n",
    "            # filtering shared f-terms with both materials in the label description\n",
    "            unique_fterms_mat1 = list(set(materials_f_terms[mat1][1]) - set(list(materials_f_terms[mat2][1]) + list(materials_f_terms[mat3][1])))\n",
    "            unique_fterms_mat2 = list(set(materials_f_terms[mat2][1]) - set(list(materials_f_terms[mat1][1]) + list(materials_f_terms[mat3][1])))\n",
    "            unique_fterms_mat3 = list(set(materials_f_terms[mat3][1]) - set(list(materials_f_terms[mat1][1]) + list(materials_f_terms[mat2][1])))\n",
    "\n",
    "            theme_mat1= set([x[:5]for x in unique_fterms_mat1])\n",
    "            theme_mat2= set([x[:5]for x in unique_fterms_mat2])\n",
    "            theme_mat3= set([x[:5]for x in unique_fterms_mat3])\n",
    "            \n",
    "            shared_theme=[x for x in theme_mat1 if x in theme_mat2 and x in theme_mat3]\n",
    "            fterm_pairs=[]\n",
    "            for theme in shared_theme:\n",
    "                fterm_pairs.append([theme,\n",
    "                                   [fterm for fterm in unique_fterms_mat1 if fterm.startswith(theme)],\n",
    "                                   [fterm for fterm in unique_fterms_mat2 if fterm.startswith(theme)],\n",
    "                                   [fterm for fterm in unique_fterms_mat3 if fterm.startswith(theme)]])\n",
    "\n",
    "\n",
    "            material_combinations[mat1[2:]+\"_\"+mat2[2:]+\"_\"+mat3[2:]]=fterm_pairs\n",
    "\n",
    "# Dropping empty material combinations\n",
    "material_combinations = {key: value for key, value in material_combinations.items() if len(value) == 4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294874e3-c326-4bbe-ac15-f6f959487326",
   "metadata": {},
   "source": [
    "# New Approach: In viewpoint search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068b356-bd73-40bc-9f11-458db6017054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a hirachical dict with all F-Terms sorted in them \n",
    "f_term_dict = {}\n",
    "for f_term in f_term_tokens:\n",
    "    theme = f_term.split('/')[0]\n",
    "    vp = f_term[:8]\n",
    "    # Creating a dict entry for the theme\n",
    "    try: \n",
    "        _ = f_term_dict[theme]\n",
    "    except KeyError:\n",
    "        f_term_dict[theme] = {}\n",
    "\n",
    "    # Creating a dict entry for the viewpoint\n",
    "    try:\n",
    "        # The first dict call will def. work the second may work if the vp-dict entry \n",
    "        # was already made. If it works the theme is appended to the viewpoint dict\n",
    "        f_term_dict[theme][vp].append(f_term)\n",
    "    except KeyError:\n",
    "        f_term_dict[theme][vp] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bd72a-b162-475d-aaf2-8d3e9436fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Using String Similarity to find Semantically matching Viewpoints or F-Termsl\n",
    "\n",
    "def sort_string_list(string_list, chunk_size=100000000, greedy_steps=10):\n",
    "    '''\n",
    "    string_list = list of strings which should be sorted by their string similarity\n",
    "    \n",
    "    returns a the list with all strings sorted by their string similarity.              \n",
    "    '''\n",
    "    # We need at least one step to collect the best match\n",
    "    greedy_steps += 1\n",
    "        \n",
    "    string_list2 = list(string_list)\n",
    "    current_string = string_list2.pop(0)\n",
    "    sorted_list = [current_string]\n",
    "    # First sorting step has to be dones outside the loop to start the loop with pretr\n",
    "    string_similarities = [difflib.SequenceMatcher(None, string, current_string).ratio() for string in string_list2]\n",
    "    # Sorting the similarities\n",
    "    idxs = list(np.argsort(string_similarities))\n",
    "    # The best matching strings will be added to the output.\n",
    "    # The last added string will be used as search string in the next iteration\n",
    "    string_list2 = [string_list2[i] for i in idxs[::-1]]\n",
    "    similar_strings, string_list2 = string_list2[:chunk_size] , string_list2[chunk_size:]\n",
    "    while len(sorted_list) < len(string_list):\n",
    "        print(' '*1000, end='\\r')\n",
    "        print(len(sorted_list), len(string_list2), current_string, end='\\r')\n",
    "        # Calculating the string similarities\n",
    "        string_similarities = [difflib.SequenceMatcher(None, string, current_string).ratio() for string in similar_strings]\n",
    "        # Sorting the similarities\n",
    "        idxs = list(np.argsort(string_similarities))\n",
    "        # The best matching strings will be added to the output.\n",
    "        # The last added string will be used as search string in the next iteration\n",
    "        similar_strings = [similar_strings[i] for i in idxs[::-1]]\n",
    "        for _ in range(greedy_steps):\n",
    "            try:\n",
    "                current_string = similar_strings.pop(0)\n",
    "                sorted_list.append(current_string)\n",
    "            except IndexError: \n",
    "                continue\n",
    "        for _ in range(greedy_steps):\n",
    "            # replenishing the similar strings list with new strings from the whole list\n",
    "            try:\n",
    "                similar_strings.append(string_list2.pop(0))   \n",
    "            except IndexError:\n",
    "                continue\n",
    "                    \n",
    "    return sorted_list\n",
    "        \n",
    "\n",
    "# Sorting Viewpoints by string similarity\n",
    "all_vp = [vp for theme, t_dict in f_term_dict.items() for vp in t_dict.keys()]\n",
    "all_vp_desc = [viewpoint_dict[vp] for vp in all_vp]\n",
    "#sorted_vp = sort_string_list(all_vp_desc)\n",
    "\n",
    "all_f_terms = number_dict.keys()\n",
    "all_f_term_desc = number_dict.values()\n",
    "\n",
    "sorted_f_terms = sort_string_list(all_f_term_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf109d-5549-4292-904d-74d257cc8fd7",
   "metadata": {},
   "source": [
    "# Create all Possible Viewpoint triangles (without using string similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a186f-2108-4d59-971a-2251774139b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a hirachical dict with all F-Terms sorted in them \n",
    "f_term_dict = {}\n",
    "for f_term in f_term_tokens:\n",
    "    theme = f_term.split('/')[0]\n",
    "    vp = f_term[:8]\n",
    "    # Creating a dict entry for the theme\n",
    "    try: \n",
    "        _ = f_term_dict[theme]\n",
    "    except KeyError:\n",
    "        f_term_dict[theme] = {}\n",
    "\n",
    "    # Creating a dict entry for the viewpoint\n",
    "    try:\n",
    "        # The first dict call will def. work the second may work if the vp-dict entry \n",
    "        # was already made. If it works the theme is appended to the viewpoint dict\n",
    "        f_term_dict[theme][vp].append(f_term)\n",
    "    except KeyError:\n",
    "        f_term_dict[theme][vp] = []\n",
    "\n",
    "# Searching for triangles of identical viewpoints in differend themes\n",
    "# 3 for triangles, higher combinations of f-terms should also be possible with this algorithm\n",
    "edges = 3\n",
    "# Minimum number of themes in which a viewpoint triangle must appear.\n",
    "min_themes = 6\n",
    "all_combs = []\n",
    "for theme, t_dict in f_term_dict.items():\n",
    "    vps = [viewpoint_dict[vp] for vp in t_dict.keys()]\n",
    "    # creating the combinations\n",
    "    combs = list(itertools.combinations(vps, edges))\n",
    "    # dropping eventual combinations with duplicates\n",
    "    combs = [frozenset(comb) for comb in combs if len(frozenset(comb)) == edges]\n",
    "    # dropping duplicate combinations or combinations with permutations\n",
    "    combs = list(set(combs))\n",
    "    all_combs.extend(combs)\n",
    "\n",
    "all_vp_tri_desc = dict(Counter(all_combs))\n",
    "all_vp_tri_desc = {key: value for key, value in all_vp_tri_desc.items() if value >= min_themes}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30b38e-1336-4d41-80de-0da0d0e1d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating all possilbe viewpoints from the viewpoint description desc triangles.\n",
    "all_vp_tri={}\n",
    "\n",
    "for triangle in all_vp_tri_desc.keys():\n",
    "    all_vp_tri[triangle] = []\n",
    "    for theme, t_dict in f_term_dict.items():\n",
    "        matching_vps = [vp for vp in t_dict.keys() if viewpoint_dict[vp] in list(triangle)]\n",
    "        if len(matching_vps) != 3:\n",
    "            continue\n",
    "        all_vp_tri[triangle].append(matching_vps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc55a9-2638-4158-8fa4-16498767b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating all trarget differences\n",
    "\n",
    "# Herachical dict in the following order triangle -> themes -> diffs_a, diffs_b \n",
    "target_diffs ={}\n",
    "emb = out_emb_dict\n",
    "print(len(all_vp_tri))\n",
    "\n",
    "for i, (tri, vps) in enumerate(all_vp_tri.items()):\n",
    "    print(i, end='\\r')\n",
    "    target_diffs[tri] = {}\n",
    "    for vp_tri in vps:\n",
    "        \n",
    "        f_terms = [f_term_dict[vp[:5]][vp] for vp in vp_tri]\n",
    "        combinations = list(itertools.product(*f_terms))\n",
    "        # dropping all duplicate combinations\n",
    "        combinations = list(set([frozenset(comb) for comb in combinations if len(frozenset(comb)) == 3]))\n",
    "        \n",
    "        diffs_a = {(list(comb)[0], list(comb)[1]): normalize(np.array([emb[list(comb)[1][:-1]] - emb[list(comb)[0][:-1]]])) for comb in combinations}\n",
    "        diffs_b = {(list(comb)[0], list(comb)[2]): normalize(np.array([emb[list(comb)[2][:-1]] - emb[list(comb)[0][:-1]]])) for comb in combinations}\n",
    "        \n",
    "        theme = vp_tri[0][:5]\n",
    "        target_diffs[tri][theme] = [diffs_a, diffs_b]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58d610-116a-4da0-aadb-3c0d7493987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating all Differences\n",
    "\n",
    "all_diffs = {}\n",
    "for i, (theme, t_dict) in enumerate(f_term_dict.items()):\n",
    "    print(i, theme, end='\\r')\n",
    "    theme_f_terms = []\n",
    "    [theme_f_terms.extend(f_term) for f_term in t_dict.values()]\n",
    "\n",
    "    combinations = list(itertools.combinations(theme_f_terms, 2))\n",
    "\n",
    "    # Filtering the combinations from all unwanted combinations\n",
    "    # 1 Dropping F-Term combinations within the same viewpoint\n",
    "    combinations = [(a, b) for a, b in combinations if viewpoint_dict[a[:8]] != viewpoint_dict[b[:8]]]\n",
    "    # Generating the difference vectors for each combination\n",
    "    diffs = {pair: normalize(np.array([emb[pair[1][:-1]]-emb[pair[0][:-1]]])) for pair in combinations}\n",
    "    all_diffs = all_diffs | diffs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f70ca-6a59-411b-8093-f498eaf2cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(exclude_theme):\n",
    "    diffs = {comb: diff for comb, diff in all_diffs.items() if comb[0][:5] != exclude_theme}\n",
    "    return diffs\n",
    "\n",
    "for t in f_term_dict.keys():\n",
    "    break\n",
    "\n",
    "print(t)\n",
    "\n",
    "len(all_diffs), len(create_subset(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f30dac-e67c-450f-8dff-6f3b0cc471cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search \n",
    "n_search = 100_000_000_000\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "for tri, themes_dict in target_diffs.items():\n",
    "    print(f'''\n",
    "    {list(tri)[0]}\n",
    "    \n",
    "    {list(tri)[1]} \n",
    "    \n",
    "    {list(tri)[2]}    ''')\n",
    "    for theme, (diffs_a, diffs_b) in themes_dict.items():\n",
    "        #print(theme, len(diffs_a), end='\\r')\n",
    "        \n",
    "        \n",
    "        search_diffs = create_subset(theme)\n",
    "        potential_hits_a = {key: value for t, (d_a, d_b) in themes_dict.items() for key, value in d_a.items() if t != theme}\n",
    "        potential_hits_b = {key: value for t, (d_a, d_b) in themes_dict.items() for key, value in d_b.items() if t != theme}\n",
    "        \n",
    "        best_simis_a = []\n",
    "        best_combs_a = []\n",
    "        qv_a = next(iter(diffs_a.values()))\n",
    "     \n",
    "        for i, (comb, diff) in enumerate(search_diffs.items()):\n",
    "            if i%1000 ==0:\n",
    "                print('iteration',f'{i:,}',  end='\\r')\n",
    "            simi = cos(torch.tensor(qv_a), torch.tensor(diff))\n",
    "\n",
    "            if len(best_simis_a) < n_search:\n",
    "                best_simis_a.append(simi.item())\n",
    "                best_combs_a.append(comb)\n",
    "            else:\n",
    "                lowest_idx = np.argmin(best_simis_a)\n",
    "                lowest_simi = best_simis_a[lowest_idx]\n",
    "                if lowest_simi < simi:\n",
    "                    best_simis_a[lowest_idx] = simi.item()\n",
    "                    best_combs_a[lowest_idx] = comb\n",
    "\n",
    "        best_simis_b = []\n",
    "        best_combs_b = []\n",
    "        \n",
    "        qv_b = next(iter(diffs_b.values()))\n",
    "        for i, (comb, diff) in enumerate(search_diffs.items()):\n",
    "            if i%1000 ==0:\n",
    "                print('iteration', f'{i:,}',  end='\\r')\n",
    "            simi = cos(torch.tensor(qv_b), torch.tensor(diff))\n",
    "\n",
    "            if len(best_simis_b) < n_search:\n",
    "                best_simis_b.append(simi.item())\n",
    "                best_combs_b.append(comb)\n",
    "            else:\n",
    "                lowest_idx = np.argmin(best_simis_b)\n",
    "                lowest_simi = best_simis_b[lowest_idx]\n",
    "                if lowest_simi < simi:\n",
    "                    best_simis_b[lowest_idx] = simi.item()\n",
    "                    best_combs_b[lowest_idx] = comb\n",
    "\n",
    "        idx_a = np.argsort(best_simis_a)[::-1]\n",
    "        idx_b = np.argsort(best_simis_b)[::-1]\n",
    "        n =10_000\n",
    "        best_simis_a = [best_simis_a[i] for i in idx_a][:n]\n",
    "        best_simis_b = [best_simis_b[i] for i in idx_b][:n]\n",
    "        best_combs_a = [best_combs_a[i] for i in idx_a][:n]\n",
    "        best_combs_b = [best_combs_b[i] for i in idx_b][:n]\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3804111-ba7e-425c-b13b-e9047e287466",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_simis_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab62ff6-df37-47b9-823e-9dcf813eca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_a = np.argsort(best_simis_a)\n",
    "idx_b = np.argsort(best_simis_b)\n",
    "\n",
    "simis_a_sorted = [best_simis_a[i] for i in idx_a]\n",
    "simis_b_sorted = [best_simis_b[i] for i in idx_b]\n",
    "\n",
    "comb_a = [best_combs_a[i] for i in idx_a]\n",
    "comb_b = [best_combs_b[i] for i in idx_b]\n",
    "\n",
    "themes_a = [c[0][:5] for c in comb_a]\n",
    "themes_b = [c[0][:5] for c in comb_b]\n",
    "\n",
    "vps_a = [c[0][:8] for c in comb_a] \n",
    "vps_b = [c[0][:8] for c in comb_b] \n",
    "\n",
    "# a = set(vps_a)\n",
    "# b = set(vps_b)\n",
    "\n",
    "# z = a.intersection(b)\n",
    "# for vp in list(a):\n",
    "#     t = vp[:5]\n",
    "#     if t in themes_dict.keys():\n",
    "#         print(vp)\n",
    "\n",
    "# print(len(z), len(a))\n",
    "# print(themes_dict.keys())\n",
    "# ######################################\n",
    "\n",
    "for tri, themes_dict in target_diffs.items():\n",
    "    break\n",
    "    for theme, (diffs_a, diffs_b) in themes_dict.items():\n",
    "        break\n",
    "print(themes_dict.keys())\n",
    "\n",
    "found_tri = []\n",
    "\n",
    "for c in comb_a:\n",
    "    vp_a = c[0][:8]\n",
    "    if vp_a in vps_b:\n",
    "        for c2 in comb_b: \n",
    "            vp_b = c2[0][:8]\n",
    "            if vp_b == vp_a:\n",
    "                vp_a = frozenset([f_term[:8] for f_term in c])\n",
    "                vp_b = frozenset([f_term[:8] for f_term in c2])\n",
    "                found_tri.extend([vp_a, vp_b])\n",
    "\n",
    "targets = []\n",
    "\n",
    "for theme, (diffs_a, diffs_b) in themes_dict.items():\n",
    "    for comb_a, comb_b in zip(diffs_a.keys(), diffs_b.keys()):\n",
    "\n",
    "        vp_a = frozenset([f_term[:8] for f_term in comb_a])\n",
    "        vp_b = frozenset([f_term[:8] for f_term in comb_b])\n",
    "        targets.extend([vp_a, vp_b])\n",
    "\n",
    "\n",
    "found_tri = list(set(found_tri))\n",
    "target_tri = list(set(targets))\n",
    "\n",
    "print(len(targets), len(found_tri))\n",
    "correct_tri = []\n",
    "for tri in found_tri:\n",
    "    if tri in targets:\n",
    "        correct_tri.append(tri)\n",
    "print(correct_tri)\n",
    "print(f'Apriori probability = {100*len(targets)/len(search_diffs)}%')\n",
    "print(f'Aposteriori probability = {100*len(correct_tri)/len(found_tri)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbef72-aedb-4cb2-9f2c-aab5bc8d0d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0eeaf-643d-451c-b2dd-06dc6305ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Substet with all themes that contain a 'purpose' viewpoint, a 'use' viewpoint and a 'structure' viewpoint\n",
    "min_simi = 0.58\n",
    "subset = {}\n",
    "for theme, t_dict in f_term_dict.items():\n",
    "    subset[theme] = {}\n",
    "    vps = [viewpoint_dict[vp] for vp in t_dict.keys()]\n",
    "    \n",
    "    purpose_simis = [difflib.SequenceMatcher(None, vp, 'purpose').ratio() for vp in vps]\n",
    "    use_simis = [difflib.SequenceMatcher(None, vp, 'use').ratio() for vp in vps]\n",
    "    structure_simis = [difflib.SequenceMatcher(None, vp, 'structure').ratio() for vp in vps]\n",
    "\n",
    "    purpose_idx = np.argmax(purpose_simis)\n",
    "    use_idx = np.argmax(use_simis)\n",
    "    structure_idx = np.argmax(structure_simis)\n",
    "\n",
    "    if len(set([purpose_idx, structure_idx, use_idx])) < 3:\n",
    "        # all viewpoints need to be different\n",
    "        subset.pop(theme)\n",
    "        continue\n",
    "\n",
    "    if purpose_simis[purpose_idx] > min_simi and use_simis[use_idx] > min_simi and structure_simis[structure_idx] > min_simi:\n",
    "        vps = list(t_dict.keys())\n",
    "        subset[theme][vps[purpose_idx]] = t_dict[vps[purpose_idx]]\n",
    "        subset[theme][vps[use_idx]] = t_dict[vps[use_idx]]\n",
    "        subset[theme][vps[structure_idx]] = t_dict[vps[structure_idx]]\n",
    "\n",
    "    else:\n",
    "        subset.pop(theme)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3181a-1d5e-408c-90e7-c41815ddd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in subset.items():\n",
    "    print(k, ':')\n",
    "    for k2 in v:\n",
    "        print(viewpoint_dict[k2])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddd86e",
   "metadata": {},
   "source": [
    "# Triangels in Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = out_emb_dict\n",
    "# Generating F-Term triangles form the material combination and dropping in viewpoint comparisons\n",
    "f_term_combinations = {key: [(f1, f2, f3) for f1 in values[1] for f2 in values[2] for f3 in values[3] if len(set([f1[:8], f2[:8], f3[:8]])) == 3] for key, theme_values in material_combinations.items() for values in theme_values}\n",
    "# Dropping empty combinations and duplicates\n",
    "f_term_combinations = {tuple(key.split('_')): value for key, value in f_term_combinations.items() if len(value) >=3}\n",
    "f_term_combinations = {key: value for i, (key, value) in enumerate(f_term_combinations.items()) if i%2==0} \n",
    "\n",
    "# Generating differences\n",
    "f_term_combinations = {key: {(combination[0], combination[1], combination[0], combination[2]): [normalize(np.array([emb_dict[combination[1][:]] - emb_dict[combination[0][:]]]))[0], normalize(np.array([emb_dict[combination[2][:]] - emb_dict[combination[0][:]]]))[0]] for combination in combinations} for key, combinations in f_term_combinations.items()}\n",
    "\n",
    "f_term_combinations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Calculating all in theme combinations\n",
    "theme_f_term_dict = {}\n",
    "for f_term in f_term_tokens:\n",
    "    theme = f_term.split('/')[0]\n",
    "    vp = f_term[:8]\n",
    "    # Creating a dict entry for the theme\n",
    "    try: \n",
    "        _ = theme_f_term_dict[theme]\n",
    "    except KeyError:\n",
    "        theme_f_term_dict[theme] = {}\n",
    "\n",
    "    # Creating a dict entry for the viewpoint\n",
    "\n",
    "    try:\n",
    "        # The first dict call will def. work the second may work if the vp-dict entry \n",
    "        # was already made. If it works the theme is appended to the viewpoint dict\n",
    "        theme_f_term_dict[theme][vp].append(f_term)\n",
    "    except KeyError:\n",
    "        theme_f_term_dict[theme][vp] = []\n",
    "        \n",
    "\n",
    "n_combs = 0\n",
    "for vps in theme_f_term_dict.values():\n",
    "    n_f_terms = np.sum([len(f_terms) for f_terms in vps.values()])\n",
    "    for vp, f_terms in vps.items():\n",
    "        #print(len(f_terms), n_f_terms)\n",
    "        vp_comb = (n_f_terms - len(f_terms)) * len(f_terms)\n",
    "        n_combs += vp_comb\n",
    "        #print(vp_comb)\n",
    "print(f'There are {n_combs:,} in theme combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0ee0a-1590-4fed-945a-c1a4bf11c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the differences\n",
    "all_diffs = []\n",
    "all_desc = []\n",
    "emb_dict = out_emb_dict\n",
    "\n",
    "for i, (theme, t_dict) in enumerate(theme_f_term_dict.items()):\n",
    "    print(i, 'Generating Differences for Theme: ', theme, end='\\r')\n",
    "    for vp, f_terms in t_dict.items():\n",
    "        keys = [key for key in t_dict.keys() if key != vp]\n",
    "        # This one liner basically concatenates all f_term lists except the f_term list of the current viewpoint\n",
    "        out_vp_f_terms = [f_term for f_term_list in [t_dict[key] for key in keys] for f_term in f_term_list]    \n",
    "        # generating the differences\n",
    "        for f_term1 in f_terms:\n",
    "            for f_term2 in out_vp_f_terms:\n",
    "                diff = emb_dict[f_term2[:-1]] - emb_dict[f_term1[:-1]]\n",
    "                diff = normalize(np.array([diff]))[0]\n",
    "                all_diffs.append(diff)\n",
    "                all_desc.append([f_term1, f_term2])\n",
    "                \n",
    "all_diffs = np.array(all_diffs)\n",
    "print(all_diffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d36a6-d10a-4923-9b42-cd630a394505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_theme(f_term_pairs, theme):\n",
    "    '''\n",
    "    Drops all f_term pairs that contain a certain theme.\n",
    "\n",
    "    returns the cleaned f_term_pairs and a boolean index\n",
    "    '''\n",
    "\n",
    "    idx = [f_term_pair[0][:5] != theme for f_term_pair in f_term_pairs]\n",
    "    f_term_pairs = [f_term_pair for f_term_pair, b in zip(f_term_pairs, idx) if b]\n",
    "    return f_term_pairs, idx\n",
    "\n",
    "\n",
    "def find_pairs(f_term_pairs, key_a, key_b, exact=False):\n",
    "    '''\n",
    "    Returns the indices of the f_term_pairs which contain key_a in the first f-term description and key_b in the description of the second f_term\n",
    "    '''\n",
    "    f_term_desc = [[number_dict[pair[0][:-1]], number_dict[pair[1][:-1]]] for pair in f_term_pairs]\n",
    "    if exact:\n",
    "        matches = [bool(pair[0].lower().startswith(key_a.lower) * pair[1].lower().startswith(key_b).lower()) for pair in f_term_desc]\n",
    "    else:\n",
    "        matches = [bool((key_a in pair[0]) * (key_b in pair[1])) for pair in f_term_desc]\n",
    "        # preventing one F-Term of the pair contains both F-Terms\n",
    "        reverse_matches = [bool((key_b in pair[0]) + (key_a in pair[1])) for pair in f_term_desc]\n",
    "        matches = [bool(match and not r_match) for match, r_match in zip(matches, reverse_matches)]\n",
    "    idx = np.arange(len(f_term_pairs))\n",
    "    idx = idx[matches]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ceeae-0f89-4305-b9e0-d2f325ae222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for similar triangles in all 1 one 1 in theme (out of viewpoint) combinations\n",
    "\n",
    "for test_triangle, test_targets in f_term_combinations.items():\n",
    "    print(test_triangle)\n",
    "    for f_terms, vecs in test_targets.items():\n",
    "        print(f_terms[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8655b9-1ea2-43af-9dbf-0d71c4e9cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = [\". metal\", \". adhes\"]\n",
    "\n",
    "comb_matches_idx = find_pairs(all_desc, *combination)\n",
    "comb_desc, comb_diffs = [all_desc[i] for i in comb_matches_idx], all_diffs[comb_matches_idx]\n",
    "\n",
    "total_hits = 0\n",
    "total_searched = 0\n",
    "total_priori_s = 0\n",
    "total_priori_hits = 0\n",
    "n = 100\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "for query_vec, desc in zip(comb_diffs, comb_desc):\n",
    "    query_vec = torch.tensor(query_vec).unsqueeze(0)\n",
    "    similarities = torch.tensor([cos(torch.tensor(vec), query_vec) for vec in all_diffs[:]])\n",
    "    sorted = torch.argsort(similarities).numpy()[::-1]\n",
    "\n",
    "    guesses = []\n",
    "    # Also calculating the hits in a theme to be able to calculate how many hits there could be in total\n",
    "    theme_guesses = []\n",
    "    i = 0\n",
    "    # Iterating over all top combinations and removing the in theme combinations until at least n valid comparisons are found\n",
    "    while len(guesses) < n:\n",
    "        # Extracting the top -n f-term descriptions\n",
    "        chunk = sorted[i*n:(i+1)*n]\n",
    "        chunk_desc = [all_desc[i] for i in chunk]\n",
    "        # Removing all in theme Combinations\n",
    "        chunk_clean, idx = drop_theme(chunk_desc, desc[0][:5])\n",
    "        # idx is a boolean index\n",
    "        \n",
    "        idx = np.array([not i for i in idx])\n",
    "        theme_chunk = [i for i, b in zip(chunk_desc, idx) if b] \n",
    "        theme_guesses.extend([theme_chunk] if sum(idx)==-1 else theme_chunk)\n",
    "        guesses.extend(chunk_clean)\n",
    "                             \n",
    "    guesses = guesses[:n]\n",
    "    hits = len(find_pairs(guesses, *combination))\n",
    "    theme_hits = len(find_pairs(theme_guesses, *combination))\n",
    "    \n",
    "    total_hits += hits\n",
    "    total_searched += n\n",
    "    total_priori_s += len(all_desc) - len(theme_guesses)\n",
    "    total_priori_hits += len(comb_matches_idx) - theme_hits\n",
    "\n",
    "    print(f' Current Hits: {hits} Current Targets: {len(comb_matches_idx) - theme_hits}; Overall: a priori: {total_priori_hits*100/total_priori_s:.5f}%, a posteriori: {total_hits*100/total_searched:.5f}%; Query Description {number_dict[desc[0][:-1]]}     {number_dict[desc[1][:-1]]}', end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713f699-457d-4295-8d4d-e81e791c4072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
