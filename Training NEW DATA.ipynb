{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c66194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 22:15:58.888108: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-24 22:15:58.909190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-24 22:15:59.342541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3.10.0 (default, Jul 12 2023, 08:49:30) [GCC 12.2.0]',\n",
       " '/home/worker/.pyenv/versions/3.10.0/bin/python',\n",
       " '4.27.1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only for Models with F-Term aggregation\n",
    "from Masterarbeit_utils.model_utils_agg_NEW_DATA import get_tokenizer, load_and_modify_model, load_pretrained_Tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import random\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "sys.version, sys.executable, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5b4422-53ef-47fc-b32e-70125677471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just calculate needed'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices = ['calculate all', 'ask for userinput', 'just calculate needed']\n",
    "calculation_profile =  choices[2]\n",
    "calculation_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80fbf8",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_1300_new_1'\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/agg_dataset_samples_new'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B\n",
    "'''\n",
    "base_model_name = 'base'\n",
    "random_init = False\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.bfloat16\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cuda:0'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "n_f_terms = None # Will be calculated\n",
    "\n",
    "# Training parameters!\n",
    "output_dir = model_folder\n",
    "num_train_epochs = 1\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "save_strategy = \"epoch\"\n",
    "logging_strategy = \"steps\"\n",
    "evaluation_strategy = \"epoch\"\n",
    "logging_steps = 10\n",
    "evaluation_steps = 1\n",
    "save_steps = 1\n",
    "gradient_accumulation_steps = 120\n",
    "logging_first_step = True\n",
    "logging_nan_inf_filter = False\n",
    "\n",
    "learning_rate = 2e-4 \n",
    "\n",
    "weight_decay = 0.0  # Parameter from first model run\n",
    "seed = 42\n",
    "resume_from_checkpoint = False\n",
    "\n",
    "# This that could improve performance\n",
    "dataloader_num_workers = 2\n",
    "# sytem varables that must be set for the tokenizer\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "torch_compile = False\n",
    "# V-Ram reduction only if default_dtype= float32\n",
    "fp16=False\n",
    "if default_dtype == torch.float16:\n",
    "    fp16=False\n",
    "bf16=True\n",
    "tf32=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f20e8",
   "metadata": {},
   "source": [
    "# Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b85e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating new tokenizer\n",
      "There are 195617 different F-Terms in the whole Dataset!\n"
     ]
    }
   ],
   "source": [
    "if calculation_profile == choices[0]:\n",
    "    i = 'y'\n",
    "elif calculation_profile == choices[1]:  \n",
    "    i = input(\"This creates a new tokenizer instance and saves it, if you want to proceed write y: \")\n",
    "else:\n",
    "    i = 'n'\n",
    "\n",
    "if i != 'y' and os.path.isfile(f'{tokenizer_folder}/tokenizer.json'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    print('Loadede Tokenizer from serialized instance!')    \n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "    \n",
    "else:\n",
    "    print('generating new tokenizer')\n",
    "    # Loads a pretrained Tokenizer for the galactica model and adds an additional token for each F-Term\n",
    "    tokenizer = get_tokenizer(dump_dir)\n",
    "    \n",
    "    # The Tokenizer contained initially 50000 Tokens which are stored as the vocab-size.\n",
    "    # The vocab_size attribute is not updated when the additional tokens are added to the tokenizer\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    tokenizer.save_pretrained(tokenizer_folder)\n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16f301",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3b2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapPatDataset(Dataset):\n",
    "    \"\"\"Dataset containing Japanese patents and their F-Term classification\"\"\"\n",
    "    def __init__(self, data_folder, tokenizer):\n",
    "        \"\"\"\n",
    "        data_folder: path to folder containing the text samples\n",
    "        tokenizer: tokenizer instance with added additional Tokens for F-Terms\n",
    "        \"\"\"\n",
    "        super(Dataset).__init__()\n",
    "        self.data_folder = data_folder\n",
    "        # This has to be manually set to the ammount of files in the 'dataset_samples' folder. Calculating the number of files in this folder would take forever.\n",
    "        # A to low number would lead to samples missing from the dataset.\n",
    "        # A to high number would raise a FileNotFound error.\n",
    "        self.l = len(os.listdir(data_folder))\n",
    "        #self.l = 10000\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with open(f'{self.data_folder}/{idx}.txt', 'r', encoding='utf-8') as f:\n",
    "                item = f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError\n",
    "        \n",
    "        # Tokenizing the item \n",
    "        #####################################\n",
    "        # The following code is just for the augmentation variant, comment it out if you don't want to use the augmentation during training\n",
    "\n",
    "        abstract, f_terms = item.split('<START F-TERMS>')\n",
    "        abstract += '<START F-TERMS>'\n",
    "        f_terms = f_terms.split(',')[:-1]\n",
    "        random.shuffle(f_terms)\n",
    "        for f_term in f_terms:\n",
    "            abstract += f_term\n",
    "            abstract += ','\n",
    "        item = abstract\n",
    "\n",
    "        #####################################\n",
    "        \n",
    "        output = self.tokenizer(item)  \n",
    "        output.pop('token_type_ids')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e4e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JapPatDataset(f'{dataset_folder}/train', tokenizer)\n",
    "validation_dataset = JapPatDataset(f'{dataset_folder}/validation', tokenizer)\n",
    "\n",
    "#### Debugging remove later: \n",
    "#validation_dataset.l = 100\n",
    "#train_dataset.l =5000\n",
    "#validation_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f3972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Memory {0: 24967643136, 'cpu': 196545060864}\n",
      "cuda:0\n",
      "New classification head has 195617 out_features\n",
      "The model interprets token-index 0 as the beginning of a sequence and token-index 2 as the end\n"
     ]
    }
   ],
   "source": [
    "# The pretrained model is loaded from Huggingface.\n",
    "# The token-embedding is expanded for all f-terms and the output embeddings is compleatly replaced by a F-Term classification head.\n",
    "model = load_and_modify_model(base_model_name, default_dtype, tensor_parallel, num_gpus, n_f_terms, default_device)\n",
    "print(f'The model interprets token-index {model.config.bos_token_id} as the beginning of a sequence and token-index {model.config.eos_token_id} as the end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab022d59-a78a-4ad0-93c7-fa6fb09c6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If intended, randomly initializating the model\n",
    "\n",
    "def randomize_model(model):\n",
    "    print('Random initialising the model')\n",
    "    for module_ in model.named_modules(): \n",
    "        if isinstance(module_[1],(torch.nn.Linear, torch.nn.Embedding)):\n",
    "            module_[1].weight.data.normal_(mean=0.0, std=model.config.init_std)\n",
    "        elif isinstance(module_[1], torch.nn.LayerNorm):\n",
    "            module_[1].bias.data.zero_()\n",
    "            module_[1].weight.data.fill_(1.0)\n",
    "        if isinstance(module_[1], torch.nn.Linear) and module_[1].bias is not None:\n",
    "            module_[1].bias.data.zero_()\n",
    "    return model\n",
    "\n",
    "\n",
    "if random_init:\n",
    "    model = randomize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f6047f-3e18-42c3-9081-a31fc0480b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Tokenizer: tensor([[    0, 34848, 16810, 14782, 50001]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<START F-TERMS>4K032/CC02,2H134/KH16,5H603/CB19,2G075/FD01,2G047/AA01,4E004/TA01,4C084/BA02,5D086/KK17,5F849/BB09,3G023/AA19,5J055/GX04,5J067/CA73,2H195/AB15,5J084/CA19,5C062/AE01,3K062/EB21,4B001/EC10,4G046/MA17,5H113/DD10,2C028/BA01,3H078/BB14,5C084/EE07,5K036/BB14,4E087/HB11,5B176/DE09,5J083/BA11,5F035/AD28,5D042/HA17,2H148/CA21,'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Text\n",
    "text = 'Good morning Mr'\n",
    "# Convert text to tokens\n",
    "tokens  = tokenizer(text, return_tensors='pt').input_ids\n",
    "print(f'Output of Tokenizer: {tokens}')\n",
    "# Model generating the predicted output tokens\n",
    "out = model.generate(tokens.to(default_device), max_length=30)\n",
    "# Decoding the tokens\n",
    "\n",
    "out = tokenizer.decode(out[0]+50000)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83feb227-160a-4ab1-b3d7-e4ec5d468b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model has 195617 output-features, the tokenizer has 245617 tokens'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(tokens)\n",
    "f'The model has {out[\"logits\"].shape[-1]} output-features, the tokenizer has {len(tokenizer)} tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3553e0",
   "metadata": {},
   "source": [
    "# Creating the Trainer Class by Subclassing from Huggingface-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e64f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Subclassing the Huggingface Trainer class to use custome code to calculate the loss\n",
    "The labels used for the loss are generated and the labels for the text tokens are set to -100 to ignore their loss,\n",
    "because the modified model can't predict text-tokens\n",
    "Also changing the log method to save the logs in a tensorboard format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_log_function():\n",
    "    \"\"\"\n",
    "    This function returns a logging-function that can be used as a method for the CustomTrainer class\n",
    "\n",
    "    :log_dir:  path to folder in which the logs will be saved\n",
    "    \"\"\"\n",
    "    writer = torch.utils.tensorboard.SummaryWriter()\n",
    "\n",
    "    def log(self, logs) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        # logging is printed after each - logging step but no update on the screen\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.state.log_history.append(output)\n",
    "        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
    "        for key, value in output.items():\n",
    "            writer.add_scalar(key, value)\n",
    "        writer.flush()\n",
    "\n",
    "    return log\n",
    "\n",
    "log_function = generate_log_function()\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool=False):\n",
    "        \"\"\"\n",
    "        model: model which should be trained.\n",
    "        inputs: A padded batch of samples from the dataset.\n",
    "        return_outputs: Indicates if the whole output of the model is returned or not.\n",
    "        \"\"\"\n",
    "        # Removing the token_type_ids because we don't need them\n",
    "        try:\n",
    "            inputs.pop('token_type_ids')\n",
    "        except KeyError:\n",
    "            pass\n",
    "        labels = inputs['input_ids']\n",
    "        # Generating the labels, because the model can only predict F-Terms but also can interpret Text-Tokens as input, \n",
    "        # The maximum token idx is 50000 higher than the maximum output_idx\n",
    "        labels = labels - 50000\n",
    "        # All text tokens have token_idx below 50000 after substracting 50000 they are negative and \n",
    "        # are now set to -100 to ignore them when the loss is computed\n",
    "        labels[labels<0] = -100\n",
    "        # generating the output of the model\n",
    "        # It is a dict of 'loss', 'logits' and 'past_key_values'\n",
    "        outputs = model(**inputs, output_attentions=False, output_hidden_states=False, return_dict=True, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "\n",
    "        message = f'loss: {loss.item()}'\n",
    "        sys.stdout.write('\\r'+ message)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        inputs: dict,\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: list = None,\n",
    "        ) -> tuple:\n",
    "\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            with self.compute_loss_context_manager():\n",
    "                loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        return loss, None, None\n",
    "\n",
    "    def log(self, logs) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        log_function(self, logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd386c2",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7c85d-d7fa-49b7-a479-9376ec17144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.348790168762207"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21475' max='86515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21475/86515 34:45:06 < 105:15:38, 0.17 it/s, Epoch 0.25/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.0437793731689452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.93319702148437557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.51562833786010754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.32031726837158244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.94792842864990232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.52083444595336964"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.59153056144714364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.11135029792785648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.71070241928100656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.87725925445556645"
     ]
    }
   ],
   "source": [
    "# The TrainingArguments class is a class which stores multiple parameters for the Custom-trainer of the model.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,              \n",
    "    num_train_epochs=num_train_epochs,             \n",
    "    per_device_train_batch_size=per_device_train_batch_size,    # batch size per device during training\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    save_strategy=save_strategy,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=evaluation_steps,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    logging_first_step=logging_first_step,\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_nan_inf_filter=logging_nan_inf_filter,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    seed=seed,\n",
    "    dataloader_num_workers=dataloader_num_workers, \n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    tf32=tf32\n",
    "    #,\n",
    "    #torch_compile=torch_compile\n",
    "    #,\n",
    "    #adam_beta1=adam_beta1,\n",
    "    #adam_beta2=adam_beta2,\n",
    "    #warmup_steps=warmup_steps\n",
    "\n",
    ")\n",
    "# Allow the training of the input embeddings\n",
    "model.enable_input_require_grads()\n",
    "trainer = CustomTrainer(model=model,\n",
    "                        args=training_args, \n",
    "                        train_dataset=train_dataset, \n",
    "                        eval_dataset=validation_dataset,\n",
    "                        data_collator=DataCollatorWithPadding(tokenizer,\n",
    "                                                              return_tensors='pt'))\n",
    "\n",
    "trainer.save_model(f'{output_dir}/checkpoint-0')\n",
    "train_results = trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8297e1-9ccb-4903-9cd2-a1ba5b9e0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811b218-1bb4-44e9-a4d1-7a02127d38de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
