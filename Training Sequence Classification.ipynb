{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c66194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3.10.0 (default, Jul 12 2023, 08:49:30) [GCC 12.2.0]',\n",
       " '/home/worker/.pyenv/versions/3.10.0/bin/python')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils.model_utils_seq_class import load_and_modify_model, get_tokenizer\n",
    "\n",
    "# Site-Packages\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, OPTForSequenceClassification\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "sys.version, sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e5b4422-53ef-47fc-b32e-70125677471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just calculate needed'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices = ['calculate all', 'ask for userinput', 'just calculate needed']\n",
    "calculation_profile =  choices[2]\n",
    "calculation_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80fbf8",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12c0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Paths to important folders have to be changed for your system.\n",
    "\"\"\"\n",
    "\n",
    "# Name of this experiment\n",
    "model_name = 'gal_125_seq_3'\n",
    "\n",
    "# This folder will be created and filled with txt.files for each sample after you run the Pytorch Dataset Notebook\n",
    "dataset_folder = f'data/dataset_samples'\n",
    "\n",
    "# The folder at which the model will be saved. This folder has to be created for your system \n",
    "model_folder = f'data/models/{model_name}'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Folder in which the tokenizer will be saved\n",
    "tokenizer_folder = f'data/tokenizers/{model_name}'\n",
    "os.makedirs(tokenizer_folder, exist_ok=True)\n",
    "\n",
    "# Folder at which all pickle files are stored. This folder is fixed for this project and should not be changed\n",
    "dump_dir = r'PK_DUMP'\n",
    "\n",
    "# Model parameters \n",
    "'''\n",
    "mini\t125 M\n",
    "base\t1.3 B\n",
    "standard\t6.7 B\n",
    "large\t30 B\n",
    "huge\t120 B'''\n",
    "base_model_name = 'mini'\n",
    "\n",
    "# All new Torch-objects will be by default in this dtype\n",
    "# if default_type = float16 fp16 must be False\n",
    "default_dtype = torch.bfloat16\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Default device on which the model will be loaded\n",
    "default_device = 'cuda:0'\n",
    "\n",
    "# Number of GPUs the model will be parallelised to \n",
    "num_gpus = 1\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero.\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "\n",
    "tensor_parallel = False\n",
    "n_f_terms = None # Will be calculated\n",
    "\n",
    "# Training parameters!\n",
    "output_dir = model_folder\n",
    "num_train_epochs = 4\n",
    "per_device_train_batch_size = 25\n",
    "per_device_eval_batch_size = 25\n",
    "gradient_accumulation_steps = 10\n",
    "save_strategy = \"steps\"\n",
    "logging_strategy = \"steps\"\n",
    "evaluation_strategy = \"steps\"\n",
    "logging_steps = 10\n",
    "evaluation_steps = 10000\n",
    "save_steps = 8000\n",
    "logging_first_step = True\n",
    "logging_nan_inf_filter = False\n",
    "\n",
    "\n",
    "learning_rate = 2e-4 \n",
    "weight_decay = 0.0  # Parameter from first model run\n",
    "seed = 42\n",
    "resume_from_checkpoint = False\n",
    "\n",
    "# This that could improve performance\n",
    "dataloader_num_workers = 8\n",
    "# sytem varables that must be set for the tokenizer\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "torch_compile = False\n",
    "# V-Ram reduction only if default_dtype= float32\n",
    "fp16=False\n",
    "if default_dtype == torch.float16:\n",
    "    fp16=False\n",
    "bf16=False\n",
    "tf32=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f20e8",
   "metadata": {},
   "source": [
    "# Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b85e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadede Tokenizer from serialized instance!\n",
      "There are 378166 different F-Terms in the whole Dataset!\n"
     ]
    }
   ],
   "source": [
    "if calculation_profile == choices[0]:\n",
    "    i = 'y'\n",
    "elif calculation_profile == choices[1]:  \n",
    "    i = input(\"This creates a new tokenizer instance and saves it, if you want to proceed write y: \")\n",
    "else:\n",
    "    i = 'n'\n",
    "\n",
    "if i != 'y' and os.path.isfile(f'{tokenizer_folder}/tokenizer.json'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_folder)\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    print('Loadede Tokenizer from serialized instance!')    \n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "    tokenizer.padding_side = 'left'\n",
    "    \n",
    "else:\n",
    "    print('generating new tokenizer')\n",
    "    # Loads a pretrained Tokenizer for the galactica model and adds an additional token for each F-Term\n",
    "    tokenizer = get_tokenizer(dump_dir)\n",
    "    \n",
    "    # The Tokenizer contained initially 50000 Tokens which are stored as the vocab-size.\n",
    "    # The vocab_size attribute is not updated when the additional tokens are added to the tokenizer\n",
    "    n_f_terms = len(tokenizer) - tokenizer.vocab_size\n",
    "    tokenizer.save_pretrained(tokenizer_folder)\n",
    "    print(f'There are {n_f_terms} different F-Terms in the whole Dataset!')\n",
    "\n",
    "\n",
    "#!!!! Important\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16f301",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c3b2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapPatDataset(Dataset):\n",
    "    \"\"\"Dataset containing Japanese patents and their F-Term classification.\n",
    "    This variant is adapted for sequence classification and returns the f_terms as a list of labels\"\"\"\n",
    "    def __init__(self, data_folder, tokenizer):\n",
    "        \"\"\"\n",
    "        data_folder: path to folder containing the text samples\n",
    "        tokenizer: tokenizer instance with added additional Tokens for F-Terms\n",
    "        \"\"\"\n",
    "        super(Dataset).__init__()\n",
    "        self.data_folder = data_folder\n",
    "        # This has to be manually set to the ammount of files in the 'dataset_samples' folder. Calculating the number of files in this folder would take forever.\n",
    "        # A to low number would lead to samples missing from the dataset.\n",
    "        # A to high number would raise a FileNotFound error.\n",
    "        self.l = len(os.listdir(data_folder))\n",
    "        self.start_f_term_token = '<START F-TERMS>'\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with open(f'{self.data_folder}/{idx}.txt', 'r', encoding='utf-8') as f:\n",
    "                item = f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError\n",
    "\n",
    "        #tokenizing the whole sample which will be later split into tokens and labels\n",
    "        tokenized = self.tokenizer(item)\n",
    "        tokenized.pop('token_type_ids')\n",
    "        attention_mask = tokenized.pop('attention_mask')\n",
    "        tokens = tokenized.pop('input_ids')\n",
    "        \n",
    "        tokens = torch.tensor(tokens)\n",
    "        # separating the abstract text tokens from the f_terms\n",
    "        input_ids = tokens[tokens < 50000].tolist()\n",
    "        f_term_ids = tokens[tokens >= 50002] - 50000\n",
    "        # rescaling the attention_mask to the shorter sequence\n",
    "        attention_mask = attention_mask[:len(input_ids)]\n",
    "\n",
    "        # creating a multi hot vector as the label \n",
    "        n_f_terms = len(self.tokenizer) - self.tokenizer.vocab_size\n",
    "        labels = torch.zeros([n_f_terms])\n",
    "        labels[f_term_ids] = 1\n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,  \n",
    "                'labels':labels.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9e4e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JapPatDataset(f'{dataset_folder}/train', tokenizer)\n",
    "validation_dataset = JapPatDataset(f'{dataset_folder}/validation', tokenizer)\n",
    "\n",
    "##### Debugging remove later\n",
    "#validation_dataset.l = 1500\n",
    "#train_dataset.l = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f3972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/galactica-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model interprets token-index 0 as the beginning of a sequence and 2 as the end\n"
     ]
    }
   ],
   "source": [
    "# The pretrained model is loaded from Huggingface.\n",
    "# The token-embedding is expanded for all f-terms and the output embeddings is compleatly replaced by a F-Term classification head.\n",
    "model = load_and_modify_model(base_model_name, default_dtype, tensor_parallel, num_gpus, n_f_terms, default_device)\n",
    "print(f'The model interprets token-index {model.config.bos_token_id} as the beginning of a sequence and {model.config.eos_token_id} as the end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33136075-57e2-4402-aad1-260b48f2baec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Loading the Model\n",
    "###########################\n",
    "device_map=None\n",
    "max_memory = {}\n",
    "if num_gpus > 0:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "        _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "model = OPTForSequenceClassification.from_pretrained(f'{model_folder}/checkpoint-3', torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                               device_map=device_map, max_memory=max_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a523053-d207-40f5-ab16-eb009c6c02fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92f6047f-3e18-42c3-9081-a31fc0480b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Tokenizer: tensor([[    0, 34848, 16810, 14782,     2]])\n",
      "torch.Size([756332]) torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Input Text\n",
    "text = 'Good morning Mr'\n",
    "# Convert text to tokens\n",
    "tokens  = tokenizer(text, return_tensors='pt').input_ids\n",
    "print(f'Output of Tokenizer: {tokens}')\n",
    "\n",
    "# creating one forward pass\n",
    "tokens = tokens.to(default_device)\n",
    "tokens = tokens[:,:-1]\n",
    "\n",
    "labels = torch.zeros([2, 378166])\n",
    "labels[:, 100] = 1\n",
    "labels[:, 1233] = 1\n",
    "print(labels.view(-1).shape, tokens.shape)\n",
    "model_output = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83feb227-160a-4ab1-b3d7-e4ec5d468b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model has 378166 output-features, the tokenizer has 428166 tokens'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'The model has {model_output[\"logits\"].shape[-1]} output-features, the tokenizer has {len(tokenizer)} tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3553e0",
   "metadata": {},
   "source": [
    "# Creating the Trainer Class by Subclassing from Huggingface-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3e64f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Subclassing the Huggingface Trainer class to use custome code to calculate the loss\n",
    "The labels used for the loss are generated and the labels for the text tokens are set to -100 to ignore their loss,\n",
    "because the modified model can't predict text-tokens\n",
    "Also changing the log method to save the logs in a tensorboard format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_log_function():\n",
    "    \"\"\"\n",
    "    This function returns a logging-function that can be used as a method for the CustomTrainer class\n",
    "\n",
    "    :log_dir:  path to folder in which the logs will be saved\n",
    "    \"\"\"\n",
    "    writer = torch.utils.tensorboard.SummaryWriter()\n",
    "\n",
    "    def log(self, logs) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        # logging is printed after each - logging step but no update on the screen\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.state.log_history.append(output)\n",
    "        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
    "        for key, value in output.items():\n",
    "            writer.add_scalar(key, value)\n",
    "        writer.flush()\n",
    "    return log\n",
    "\n",
    "\n",
    "log_function = generate_log_function()\n",
    "# Just beeing save and checking the right padding position of the tokenizer\n",
    "tokenizer.padding_side = 'left'\n",
    "cel = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool=False, loss_fc=torch.nn.CrossEntropyLoss()):\n",
    "        \"\"\"\n",
    "        model: model which should be trained.\n",
    "        inputs: A padded batch of samples from the dataset.\n",
    "        return_outputs: Indicates if the whole output of the model is returned or not.\n",
    "        loss_fc: Instance of a loss function which should be used for the loss calculation\n",
    "        \"\"\"\n",
    "        def cross_entropy_loss(logits, target):\n",
    "            \"\"\"\n",
    "            This function applies softmax and than cross entropy loss to a logits, target pair\n",
    "        \n",
    "            A custom CrossEntropy-Loss function that can be applied to multi Label Problems\n",
    "            \"\"\"\n",
    "            # Durch 1.0 Teilen erhält den Gradienten\n",
    "            predicted_distribution = torch.nn.functional.softmax(logits, -1)/1.0\n",
    "            # Scaling the predicted distribution to match the target distribution\n",
    "                    \n",
    "            n_targ = target.sum(-1).unsqueeze(-1)\n",
    "            predicted_distribution *= n_targ\n",
    "                    \n",
    "            epsilon = 1e-10\n",
    "            cross_entropy = target * torch.log(predicted_distribution + epsilon)\n",
    "            cross_entropy = cross_entropy.sum(-1)\n",
    "            return -torch.mean(cross_entropy)\n",
    "            \n",
    "        # Removing the token_type_ids because we don't need them\n",
    "        try:\n",
    "            inputs.pop('token_type_ids')\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        # extracting and normalizing the labels\n",
    "        labels = inputs.pop('labels')\n",
    "        labels = torch.nn.functional.normalize(labels, p=1 , dim = -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        model.train()\n",
    "        outputs = model(**inputs, output_attentions=False, output_hidden_states=False, return_dict=True)\n",
    "        logits = outputs['logits']\n",
    "\n",
    "        # calculating the loss\n",
    "\n",
    "\n",
    "        # uncomment if you want to use customeloss\n",
    "        #loss = cross_entropy_loss(logits, labels)\n",
    "        # comment out if you want to use custome crossentorpy loss\n",
    "        loss = cel(logits, labels)\n",
    "        \n",
    "\n",
    "        #message = f'loss: {loss.item()}'\n",
    "        message = f'loss: {loss.item():.5f} max logit: {torch.max(logits, dim=-1).values[0]:.5f} min logit: {torch.min(logits, dim =-1).values[0]:.5f}, max_label: {torch.max(labels, dim=-1).values[0]:.5f}, min_label: {torch.max(labels, dim=-1).values[0]:.5f}' \n",
    "        sys.stdout.write('\\r'+ message)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        inputs: dict,\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: list = None,\n",
    "        ) -> tuple:\n",
    "        #torch.cuda.empty_cache()\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            with self.compute_loss_context_manager():\n",
    "                loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        return loss, None, None\n",
    "\n",
    "    def log(self, logs) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        log_function(self, logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd386c2",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40a7c85d-d7fa-49b7-a479-9376ec17144f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_159576/4206974271.py\", line 45, in __getitem__\n    return {'input_ids': input_ids.to(default_device),\nAttributeError: 'list' object has no attribute 'to'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 42\u001b[0m\n\u001b[1;32m     32\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m                         args\u001b[38;5;241m=\u001b[39mtraining_args, \n\u001b[1;32m     34\u001b[0m                         train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, \n\u001b[1;32m     35\u001b[0m                         eval_dataset\u001b[38;5;241m=\u001b[39mvalidation_dataset,\n\u001b[1;32m     36\u001b[0m                         data_collator\u001b[38;5;241m=\u001b[39mDataCollatorWithPadding(tokenizer,\n\u001b[1;32m     37\u001b[0m                                                               return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#trainer.save_model(f'{output_dir}/checkpoint-0')\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#train_results = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/trainer.py:2932\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2929\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2931\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2932\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2933\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2936\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2942\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/trainer.py:3103\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3101\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 3103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   3104\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   3105\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   3106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_159576/4206974271.py\", line 45, in __getitem__\n    return {'input_ids': input_ids.to(default_device),\nAttributeError: 'list' object has no attribute 'to'\n"
     ]
    }
   ],
   "source": [
    "# The TrainingArguments class is a class which stores multiple parameters for the Custom-trainer of the model.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,              \n",
    "    num_train_epochs=num_train_epochs,             \n",
    "    per_device_train_batch_size=per_device_train_batch_size,    # batch size per device during training\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    save_strategy=save_strategy,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=evaluation_steps,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    logging_first_step=logging_first_step,\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_nan_inf_filter=logging_nan_inf_filter,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    seed=seed,\n",
    "    dataloader_num_workers=dataloader_num_workers, \n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    tf32=tf32,\n",
    "    torch_compile=torch_compile\n",
    "    #,\n",
    "    #adam_beta1=adam_beta1,\n",
    "    #adam_beta2=adam_beta2,\n",
    "    #warmup_steps=warmup_steps\n",
    "\n",
    ")\n",
    "# Allow the training of the input embeddings\n",
    "model.enable_input_require_grads()\n",
    "trainer = CustomTrainer(model=model,\n",
    "                        args=training_args, \n",
    "                        train_dataset=train_dataset, \n",
    "                        eval_dataset=validation_dataset,\n",
    "                        data_collator=DataCollatorWithPadding(tokenizer,\n",
    "                                                              return_tensors='pt'))\n",
    "\n",
    "#trainer.save_model(f'{output_dir}/checkpoint-0')\n",
    "#train_results = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "trainer.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9350b-2a35-4d6a-a15c-a4041f915db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, target):\n",
    "            \"\"\"\n",
    "            This function applies softmax and than cross entropy loss to a logits, target pair\n",
    "        \n",
    "            A custom CrossEntropy-Loss function that can be applied to multi Label Problems\n",
    "            \"\"\"\n",
    "            # Durch 1.0 Teilen erhält den Gradienten\n",
    "            predicted_distribution = torch.nn.functional.softmax(logits, -1)/1.0\n",
    "            print('bevore', predicted_distribution, target)\n",
    "            predicted_distribution = torch.nn.functional.normalize(predicted_distribution, p=1, dim=-1)\n",
    "            # Scaling the predicted distribution to match the target distribution\n",
    "            print('after', predicted_distribution)\n",
    "            n_targ = target.sum(-1).unsqueeze(-1)\n",
    "            predicted_distribution *= n_targ\n",
    "                    \n",
    "            epsilon = 1e-10\n",
    "            cross_entropy = target * torch.log(predicted_distribution + epsilon)\n",
    "            cross_entropy = cross_entropy.sum(-1)\n",
    "            return -torch.mean(cross_entropy)\n",
    "\n",
    "\n",
    "l = torch.tensor([[-1000, 10, -1000, 10]])/1.0\n",
    "t = torch.tensor([[0, 1, 0, 1]])/1.0\n",
    "t = torch.nn.functional.normalize(t, p=1, dim=-1)\n",
    "cross_entropy_loss(l, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144684f5-93e3-4d76-be61-a68e8f8a13b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3c2e6-2b18-4930-aad0-451f2ef0167a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
