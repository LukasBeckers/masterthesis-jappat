{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a02b967-9bab-4ec3-a858-5b50d2e96a67",
   "metadata": {},
   "source": [
    "In this notebook, the raw dataset is cleaned form all samples that contain F-Terms that are not defined (by the previously extracted f-term definitions) or corrupted (missing viewpoint or number). \n",
    "\n",
    "The Cleaned dataset is than saved sample by sample in .data/dataset_samples \n",
    "Each sample is a txt file containing the abstract and the f_terms separated by a special token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b428ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0da02de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 16:19:52.338873: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-24 16:19:52.369463: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-24 16:19:53.167572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Own Packages\n",
    "from Masterarbeit_utils import dataset_utils, model_utils\n",
    "from Masterarbeit_utils.dataset_utils import load_parquet_to_dask, LabelEmbedding\n",
    "\n",
    "# Site-Packages\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pickle as pk\n",
    "import cProfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3541180-ab9c-4ae6-8e3a-32df5c713862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just calculate needed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices = ['calculate all', 'ask for userinput', 'just calculate needed']\n",
    "calculation_profile =  choices[2]\n",
    "calculation_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7763b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344d8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquett-file of the raw dataset\n",
    "raw_dataset_path = 'data/JPO_patents_abstracts_fterms'\n",
    "\n",
    "# Path to the directory in which pickle-files are dumped\n",
    "dump_dir = r'PK_DUMP'\n",
    "if not os.path.isdir(dump_dir):\n",
    "    os.mkdirs(dump_dir)\n",
    "\n",
    "# Path where the cleaned data is saved. \n",
    "output_path_drop_undefined = r'data/drop_undefined'\n",
    "# Path where the cleaned and defined samples will be stored as individual files.\n",
    "files_path = \"data/dataset_samples\"\n",
    "\n",
    "# Start f-term Token (special token which shows the model that now the prediction of f-terms begins)\n",
    "start_f_term_token = '<START F-TERMS>'\n",
    "\n",
    "# Number of occurrences of F-Term the F-Term will be at least upsampled to\n",
    "upsample_to = 5\n",
    "# Valdiation Split Percentage\n",
    "train_val_split = 0.01\n",
    "# If a patent has less F-terms than drop_on_single F-Terms and also contains a F-Term that \n",
    "# occurrs just once in the dataset, it is dropped.\n",
    "drop_on_single = 5\n",
    "# maximum number of tokens allowd per sample\n",
    "max_total_tokens = 370\n",
    "# maximum allowed F-Terms per patent\n",
    "max_f_terms = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f4186",
   "metadata": {},
   "source": [
    "# Loading the Raw Dataset and Dropping Lines with Corrupted F-Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25226a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PURPOSE:A process for producing moisture absor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PURPOSE:To improve transmission characteristic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PURPOSE:To enhance a breakwater function resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PURPOSE:To enhance the cooling efficiency and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PURPOSE:To prevent the leakage of fluid by a m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sample\n",
       "0  PURPOSE:A process for producing moisture absor...\n",
       "1  PURPOSE:To improve transmission characteristic...\n",
       "2  PURPOSE:To enhance a breakwater function resul...\n",
       "3  PURPOSE:To enhance the cooling efficiency and ...\n",
       "4  PURPOSE:To prevent the leakage of fluid by a m..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the Dataset.\n",
    "data = dataset_utils.clean_df(raw_dataset_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a6837",
   "metadata": {},
   "source": [
    "# Extracting the unique f-terms\n",
    "\n",
    "### This is done to extract a list of F-Terms which should be added as tokens to the tokenizer\n",
    "### To recalculate the unique f-terms the f_terms_in_ds_dir.pk file must be first deleted from the PK_DUMP folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1df503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only on your first run of this notebook, it takes really long. \n",
    "# All outputs will be saved and can be loaded from disk.\n",
    "\n",
    "#In this dict all unique f-terms will be combined with a counting number\n",
    "\n",
    "if not os.path.isfile(f'{dump_dir}/f_terms_in_ds_dir.pk'):\n",
    "    f_term_dict = {}\n",
    "    def extract_f_terms(line):\n",
    "        return line.split(start_f_term_token)[1]\n",
    "    \n",
    "    raw_f_terms = data['Sample'].apply(extract_f_terms, meta=('extract_f_terms', 'str'))\n",
    "    for i, row in enumerate(raw_f_terms):\n",
    "        \n",
    "        if len(row) <9:\n",
    "            continue\n",
    "        f_terms = row.split(',')[:-1]\n",
    "    \n",
    "        for f_term in f_terms:\n",
    "            try: \n",
    "                _ = f_term_dict[f_term]\n",
    "            except KeyError:\n",
    "                f_term_dict[f_term] = i\n",
    "        \n",
    "        if i%10000 == 0: \n",
    "            print(f'Processed {i} Samples, found {len(f_term_dict.keys())} uniqe F-terms', end='\\r')\n",
    "            \n",
    "    \n",
    "    # Saving the dict\n",
    "    with open(f'{dump_dir}/f_terms_in_ds_dir.pk', 'wb') as f:\n",
    "        pk.dump(f_term_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba46a0-7736-4d2a-9d37-5390c2011eb5",
   "metadata": {},
   "source": [
    "There are about 450000 F-Terms in the dataset, 360000 F-Terms are expected. This is because there are many corrupted F-Terms in the dataset, which miss some parts like the viewpoint or the number.\n",
    "There are also a lot of F-Terms which are not used anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a9354",
   "metadata": {},
   "source": [
    "# Loading the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbfdc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dict containing all unique f-terms in the dataset\n",
    "\n",
    "with open(f'{dump_dir}/f_terms_in_ds_dir.pk', 'rb') as f:\n",
    "    f_term_dict = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613d51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a dict, which contains all uniqe f-terms with crawled definitions\n",
    "# This dict will be created when the Dataset Analysis is run\n",
    "\n",
    "with open(f'{dump_dir}/f_term_dict.pk', 'rb') as f:\n",
    "    definitions = pk.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b11d2a",
   "metadata": {},
   "source": [
    "## Creating a List With All F-Terms That Are Not Defined by the Definitions Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30484e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74205 F-Terms in the dataset, which have no definition in the definitions dict!\n"
     ]
    }
   ],
   "source": [
    "# Checking for which f-term form the dataset a f-term definition is present\n",
    "exceptions = {}\n",
    "exceptions_l = 0\n",
    "for i, key in enumerate(f_term_dict.keys()):\n",
    "    try: \n",
    "        _ = definitions[key]\n",
    "        exceptions[key] = 0\n",
    "    except KeyError:\n",
    "        exceptions[key] = 1\n",
    "        exceptions_l += 1\n",
    "print(f'There are {exceptions_l} F-Terms in the dataset, which have no definition in the definitions dict!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0129e1",
   "metadata": {},
   "source": [
    "# Dropping all Patents From the Dataset, Which Contain Undefined F-Terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78af03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_f_terms(line, Stoken : str='<START F-TERMS>'):\n",
    "    \"\"\"\n",
    "    This function should be applied to a dask dataframe with already cleaned data from the dataset_utils.clean_and_save_df function.\n",
    "    \n",
    "    This function extracts all f_terms from a\n",
    "    \"\"\"\n",
    "    line_data = line\n",
    "    f_term_string = line_data.split(Stoken)[-1]\n",
    "    f_terms = f_term_string.split(',')[:-1]\n",
    "    return f_terms\n",
    "    \n",
    "\n",
    "def drop_undefined(clean_data: dask.dataframe, exceptions_dict: dict):\n",
    "    \"\"\"\n",
    "    This function should be applied to a dask dataframe which was cleaned by the clean_and_save_df function and drops each row, which contains undefined_f_terms\n",
    "    \"\"\"\n",
    "    def test_f_terms(line, exceptions_dict: dict=exceptions_dict):\n",
    "        \"\"\"\n",
    "        This function should be applied to the extracted f-term column of a df.\n",
    "        This function will return True if all F-Terms are defined and will return False if any F-Term in a row is undefined\n",
    "        \"\"\"\n",
    "        res = [exceptions_dict[l] for l in line]\n",
    "        res_sum = sum(res)\n",
    "        return not bool(res_sum)\n",
    "    \n",
    "    clean_data['F_Terms'] = clean_data['Sample'].apply(extract_f_terms, meta=('F_Terms', 'str'))\n",
    "    clean_data['F_Terms'] = clean_data['F_Terms'].apply(test_f_terms, meta=('F_Terms', 'bool'))\n",
    "    defined_data = clean_data[['Sample']].loc[clean_data['F_Terms']]\n",
    "    return defined_data\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "562318fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PURPOSE:A process for producing moisture absor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PURPOSE:To improve transmission characteristic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PURPOSE:To enhance a breakwater function resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PURPOSE:To enhance the cooling efficiency and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PURPOSE:To improve high speed running characte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sample\n",
       "0  PURPOSE:A process for producing moisture absor...\n",
       "1  PURPOSE:To improve transmission characteristic...\n",
       "2  PURPOSE:To enhance a breakwater function resul...\n",
       "3  PURPOSE:To enhance the cooling efficiency and ...\n",
       "6  PURPOSE:To improve high speed running characte..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defined_data = drop_undefined(data, exceptions)\n",
    "defined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2dfb8-4497-4122-8c0e-2619d7b3c114",
   "metadata": {},
   "source": [
    "# Dropping Samples that Contain to Many Tokens or F-Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c53d3da3-4ea1-486b-973a-dc02207a4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model_utils.get_tokenizer(dump_dir)\n",
    "\n",
    "def get_n_tokens(sample):\n",
    "    \"\"\"\n",
    "    This function retruns True if the sample has less than the maximum allowed number of tokens and \n",
    "    False if it has more.\n",
    "    \"\"\"\n",
    "    l =  len(tokenizer(sample)['input_ids'])\n",
    "    return l < max_total_tokens\n",
    "\n",
    "def get_n_f_terms(sample):\n",
    "    \"\"\"\n",
    "    This function returns True if the sample has less than the maximum allowed number of tokens and False if it has more\n",
    "    \"\"\"\n",
    "    l = len(extract_f_terms(sample))\n",
    "    return l < max_f_terms\n",
    "\n",
    "defined_data['n_tokens'] = defined_data['Sample'].map(get_n_tokens, meta=('n_tokens', 'bool'))\n",
    "defined_data['n_f_terms'] = defined_data['Sample'].map(get_n_f_terms, meta=('n_f_terms', 'bool'))\n",
    "defined_data['keep'] = defined_data['n_tokens'] * defined_data['n_f_terms']\n",
    "defined_data = defined_data[['Sample']].loc[defined_data['keep']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a574f67-6f37-4603-ac88-ccb2be223401",
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_data = load_parquet_to_dask('data/tmp/defined_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5886d25-4b07-479c-8e34-3f47c186225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7478671"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(defined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bca05-fb4b-4824-a20b-807d680a3e3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Dropping all Samples with Low Occurrence\n",
    "\n",
    "All Samples which contain F-Terms which only occurr once in the dataset are dropped, because these samples would be either in the train, or in the validation dataset, both scenarios are not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aefd6a-8c20-4927-8a21-ed89cdb7d10a",
   "metadata": {},
   "source": [
    "# Counting the Occurrence of each F-Term in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf73c03c-1050-49ca-bbd9-160238308254",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning this cell takes a long time,\n",
      "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
      "be recalculated. \n",
      "If you wan't to proceed wirte 'y' n\n"
     ]
    }
   ],
   "source": [
    "# Run this cell just once it takes really long and all the outputs are also saved on disk\n",
    "# after the first run.\n",
    "i = input(\n",
    "\"\"\"Warning this cell takes a long time,\n",
    "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
    "be recalculated. \n",
    "If you wan't to proceed wirte 'y'\"\"\")\n",
    "if i == 'y':\n",
    "\n",
    "    # LabelEmbedding Instance (see Masterarbeit_utils.dataset_utils)\n",
    "    label_embedding = LabelEmbedding()\n",
    "    for i, sample in enumerate(defined_data['Sample']):\n",
    "        f_terms = extract_f_terms(sample)\n",
    "        [label_embedding(f_term) for f_term in f_terms]\n",
    "        if i%1000 == 0:\n",
    "            print(f'Processed {i} samples!', end='\\r')\n",
    "    \n",
    "    # Saving the Label Embeddings\n",
    "    with open(f'{dump_dir}/label_embedding_defined_data.pk', 'wb') as f:\n",
    "        pk.dump(label_embedding, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21e2dee5-8b5b-43be-a766-b284e78ee91c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_single_fterms(data: dask.dataframe, label_embedding: LabelEmbedding = None):\n",
    "    \"\"\"\n",
    "    This function should be applied to the dask.Dataframe dataset after the patents, which contain \n",
    "    undefined F-Terms were dropped.\n",
    "    This function extracts the F-Terms from the dataset and drops each patent less than 5 F-Terms and with a F-Term \n",
    "    which occurrs just once in the whole dataset.\n",
    "    For each patent that has more than 5 F-Terms and also contains a F-Term with just one \n",
    "    occurrence, the F-Term is removed, because it will not have a high influence on the \n",
    "    meaning of the patent.\n",
    "\n",
    "    :data: dataframe cleaned by drop_undefined function\n",
    "    :label_embedding: LabelEmbedding instance used on the defined-data dataframe\n",
    "                          It is used to check the occurrence of a patent.\n",
    "    \"\"\"\n",
    "    if label_embedding is None:\n",
    "        # loading the label embeddings\n",
    "        with open(f'{dump_dir}/label_embedding_defined_data.pk', 'rb') as f:\n",
    "            label_embedding = pk.load(f)\n",
    "    # Setting all label occurrences larger then one to zero\n",
    "    label_embedding.occurrence = np.array(label_embedding.occurrence)\n",
    "    label_embedding.occurrence[label_embedding.occurrence > 1] = 0\n",
    "\n",
    "    def check_single(f_terms: str, label_embedding: LabelEmbedding=label_embedding):\n",
    "        \"\"\"\n",
    "        This function checks if a sample contains a F-Term that occurrs just once in the\n",
    "        whole dataset.\n",
    "        It returns True if such an F-Term is found and returns False if all F-Terms occurr \n",
    "        more often than that.\n",
    "        :f_terms:  f_terms extracted by the previously defined extract_f_terms function. \n",
    "        :label_embedding: LabelEmbedding instance used on the defined-data dataframe\n",
    "                          It is used to check the occurrence of a patent.\n",
    "        \"\"\"\n",
    "        f_term_ind = [label_embedding.dict[f_term] for f_term in f_terms]\n",
    "        f_term_occ = [label_embedding.occurrence[ind] for ind in f_term_ind]\n",
    "        # iff all F-Terms occurr more than once the sum is 0 if any F-term occurrs just once\n",
    "        # the sum is larger than zero and thus True\n",
    "        res = sum(f_term_occ)\n",
    "        return bool(res)\n",
    "\n",
    "    def check_len(f_terms: str):\n",
    "        \"\"\"\n",
    "        This function checks if a patent contains more or less F-Terms than the threshold below\n",
    "        which the patent is removed if it contains patent with an occurrence of 1 in the dataset.\n",
    "\n",
    "        :f_terms: f_terms extracted by the previously defined extract_f_terms function.\n",
    "        \"\"\"\n",
    "        l = len(f_terms)\n",
    "        if l > drop_on_single:\n",
    "            # Patent will not be dropped\n",
    "            return False\n",
    "        else:\n",
    "            # Patent may be dropped if also a low occurring F-Term is found\n",
    "            return True\n",
    "\n",
    "    def should_drop(single_f_terms: bool, short_len:bool):\n",
    "        \"\"\"\n",
    "        Returns False if a patent contains a F-Term tha occurrs just once in the dataset and \n",
    "        also only a small ammount of F-Terms are present in the sample.\n",
    "        False = Sample will be dropped\n",
    "        True = Sample will be kept\n",
    "        \"\"\"\n",
    "        # Patent will be dropped if both conditions are True\n",
    "        return not all([single_f_terms, short_len])\n",
    "\n",
    "    def remove_f_terms(sample: str,\n",
    "                       \n",
    "                       label_embedding: LabelEmbedding = label_embedding):\n",
    "        \"\"\"\n",
    "        This function removes all f-terms that occurr just once in the dataset from a patent\n",
    "        if the patent hase more f-terms than \"drop_on_single\"\n",
    "        \n",
    "        :sample: sample from the dataset with abstact, start_f_term_token and f_terms\n",
    "       \n",
    "        :label_embedding: LabelEmbedding instance used on the defined-data dataframe\n",
    "                          It is used to check the occurrence of a patent.\n",
    "        \n",
    "        \"\"\"\n",
    "        f_terms = extract_f_terms(sample)\n",
    "        # Embedding the F-Terms to labels\n",
    "        f_terms_nums = [label_embedding.dict[f_term] for f_term in f_terms]\n",
    "        # Dropping all high occurring f_terms\n",
    "        f_terms_nums_low_occ = [n for n in f_terms_nums if label_embedding.occurrence[n] == 1]\n",
    "        # Converting the embedded F-Terms back to strings\n",
    "        f_terms_low_occ = [label_embedding.r_dict[n] for n in f_terms_nums_low_occ]\n",
    "        # removing the f_terms_from the sample\n",
    "        for f_term in f_terms_low_occ:\n",
    "            sample = sample.replace(f'{f_term},', '')\n",
    "        return sample        \n",
    "\n",
    "    data['F_Terms'] = data['Sample'].apply(extract_f_terms, meta=('F_Terms', 'str'))\n",
    "    # checking which patent contains low occurring F-Terms\n",
    "    data['Single'] = data['F_Terms'].apply(check_single, meta=('Single', 'bool'))\n",
    "    # removing low occurring F-Terms\n",
    "    data['Sample'] = data['Sample'].apply(remove_f_terms, meta=('Sample', 'str'))\n",
    "    # Re extract the F-Terms to check if drop below the threshold number of F-Terms\n",
    "    data['F_Terms'] = data['Sample'].apply(extract_f_terms, meta=('F_Terms', 'str'))\n",
    "    data['Len'] = data['F_Terms'].apply(check_len, meta=('Len', 'bool'))\n",
    "    # Creating indices for all patent which should be dropped\n",
    "    data['Drop'] = ~(data['Single'] * data['Len'])   # logical \"or\" operator\n",
    "    no_single_data = data[['Sample']].loc[data['Drop']]\n",
    "    return no_single_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a5192e0-9263-4c42-b972-26703c41e2a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iteratively_drop_samples(data: dask.dataframe):\n",
    "    \"\"\"\n",
    "    This function iteratively applies the drop_single_fterms function to the dataset until\n",
    "    no single f-terms remain in the dataset\n",
    "    \"\"\"\n",
    "    # placeholders\n",
    "    label_embedding = None\n",
    "    n_single_fterms = 1\n",
    "    t = 1\n",
    "    while n_single_fterms !=0:\n",
    "        \n",
    "        data = drop_single_fterms(data, label_embedding)\n",
    "        # dumping the data in a tmp dir \n",
    "        os.makedirs('data/tmp', exist_ok=True)\n",
    "        data.to_parquet('data/tmp/dataset')\n",
    "        data = load_parquet_to_dask('data/tmp/dataset')\n",
    "        # LabelEmbedding Instance (see Masterarbeit_utils.dataset_utils)\n",
    "        label_embedding = LabelEmbedding()\n",
    "        for i, sample in enumerate(data['Sample']):\n",
    "            f_terms = extract_f_terms(sample)\n",
    "            [label_embedding(f_term) for f_term in f_terms]\n",
    "            if i%1000 == 0:\n",
    "                print(f'Processed {i} samples!', end='\\r')\n",
    "        n_single_fterms = len([i for i in label_embedding.occurrence if i ==1])\n",
    "        print(f'Iteration: {t}, remaining single F-Terms: {n_single_fterms}')\n",
    "        t += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dda875aa-df58-41f4-84ad-70b59266dde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning this cell takes a long time,\n",
      "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
      "be recalculated. \n",
      "If you wan't to proceed wirte 'y' y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, remaining single F-Terms: 0\n"
     ]
    }
   ],
   "source": [
    "i = input(\n",
    "\"\"\"Warning this cell takes a long time,\n",
    "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
    "be recalculated. \n",
    "If you wan't to proceed wirte 'y'\"\"\")\n",
    "if i == 'y':\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    no_single_data = iteratively_drop_samples(defined_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a095135-6650-439a-8f11-48713b1ae628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved dataset\n",
    "no_single_data = load_parquet_to_dask('data/tmp/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda9abd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Saving the Dask dataframe as idividual text files\n",
    "### Todo:\n",
    "- Upsampeling low occurring F-Terms? Maybe later!\n",
    "- Making sure, that each F-Term occurrs in the tain and validation set. -- Making sure each F-Term occurs in the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "879b8ebb-32a7-4870-97ad-3f2a253ae766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning this cell takes a long time,\n",
      "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
      "be recalculated. \n",
      "If you wan't to proceed wirte 'y' y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7478000 samples!\r"
     ]
    }
   ],
   "source": [
    "i = input(\n",
    "\"\"\"Warning this cell takes a long time,\n",
    "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
    "be recalculated. \n",
    "If you wan't to proceed wirte 'y'\"\"\")\n",
    "\n",
    "if i == 'y':\n",
    "    label_embedding = LabelEmbedding()\n",
    "    for i, sample in enumerate(no_single_data['Sample']):\n",
    "        f_terms = extract_f_terms(sample)\n",
    "        [label_embedding(f_term) for f_term in f_terms]\n",
    "        if i%1000 == 0:\n",
    "            print(f'Processed {i} samples!', end='\\r')\n",
    "            \n",
    "    # Saving the Label Embeddings\n",
    "    with open(f'{dump_dir}/label_embedding_no_single.pk', 'wb') as f:\n",
    "        pk.dump(label_embedding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "670a2222-2aae-4489-997c-50a28673fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the samples to a list to shuffle them \n",
    "no_single_data = [row for row in no_single_data['Sample']]\n",
    "# Dask does not offer the functionality to shuffle the rows of a dataframe so \n",
    "# i convert the dataset to a list in memory, this works only on machines with high ram\n",
    "random.shuffle(no_single_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d574da08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning this cell takes a long time,\n",
      "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
      "be recalculated. \n",
      "If you wan't to proceed wirte 'y' y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/worker/.pyenv/versions/3.10.0/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Samples: 7403221 Val-Samples: 74780 set-split: 0.01 current-split: 0.0100 label-split: 0.0168 labels not in train: 0 samples: 7478000                 \r"
     ]
    }
   ],
   "source": [
    "# This cell takes really long (90 min)\n",
    "\n",
    "i = input(\n",
    "\"\"\"Warning this cell takes a long time,\n",
    "after the first run of this cell, all outputs are already saved on disk and don't need to\n",
    "be recalculated. \n",
    "If you wan't to proceed wirte 'y'\"\"\")\n",
    "\n",
    "if i == 'y':\n",
    "    # creating the train and validation folders\n",
    "    os.makedirs(f'{files_path}/train', exist_ok=True)\n",
    "    os.makedirs(f'{files_path}/validation', exist_ok=True)\n",
    "    \n",
    "    # creating label embeddings for the dataset without single F-Terms\n",
    "    # These will be needed to calculate the sampling factors, \n",
    "    # each must at least occurr 5 time in the dataset to do this some samples will be \n",
    "    # added multiple times to the dataset\n",
    "    with open(f'{dump_dir}/label_embedding_no_single.pk', 'rb') as f:\n",
    "        label_embedding = pk.load(f)\n",
    "    # During Separation of the dataset, the occurrences of the F-Terms are observed in the \n",
    "    # train- and validation-dataset and the decision in which dataset the sample is put \n",
    "    # is made according to the occurrences and the val-split parameter\n",
    "    train_occurrences = {key: 0 for key in label_embedding.dict.keys()}\n",
    "    validation_occurrences = {key: 0 for key in label_embedding.dict.keys()}\n",
    "    # metrics to investigate the progresss\n",
    "    n_train = 0\n",
    "    n_validation = 0\n",
    "    # target_occurrences \n",
    "    targ_train_occ = {key: value*(1-train_val_split) for key, value in label_embedding.dict.items()}\n",
    "    targ_val_occ = {key: value*(train_val_split) for key, value in label_embedding.dict.items()}\n",
    "    \n",
    "    \n",
    "    for i, sample in enumerate(no_single_data):\n",
    "        f_terms = extract_f_terms(sample)\n",
    "        train_occ = np.array([train_occurrences[f_term] for f_term in f_terms])\n",
    "        #val_occ = np.array([validation_occurrences[f_term] for f_term in f_terms])\n",
    "\n",
    "        # Each F-Term has to be at least once in the training dataset.\n",
    "        if 0 in train_occ:\n",
    "            with open(f'{files_path}/train/{n_train}.txt', 'w', encoding=\"utf-8\") as f:\n",
    "                f.write(sample)\n",
    "\n",
    "            for f_term in f_terms:\n",
    "                train_occurrences[f_term] += 1\n",
    "            n_train += 1\n",
    "\n",
    "        else:\n",
    "            score = n_validation / (n_train+n_validation)\n",
    "    \n",
    "            # Validation dataset\n",
    "            if score < train_val_split:\n",
    "                # There are more samples in the train-dataset than wanted, \n",
    "                # so the sample will be put into the val-dataset\n",
    "                with open(f'{files_path}/validation/{n_validation}.txt', 'w', encoding=\"utf-8\") as f:\n",
    "                        f.write(sample)\n",
    "        \n",
    "                for f_term in f_terms:\n",
    "                    validation_occurrences[f_term] += 1\n",
    "                n_validation += 1\n",
    "    \n",
    "            # Training dataset\n",
    "            if score >= train_val_split:\n",
    "                # There are more samples in the validation dataset tahn wanted, \n",
    "                # so the sample will be put into the train-dataset!\n",
    "                with open(f'{files_path}/train/{n_train}.txt', 'w', encoding=\"utf-8\") as f:\n",
    "                        f.write(sample)\n",
    "    \n",
    "                for f_term in f_terms:\n",
    "                    train_occurrences[f_term] += 1\n",
    "                n_train += 1\n",
    "                  \n",
    "        # Plotting progress\n",
    "        if i%1000 == 0:\n",
    "            train_occ_all = np.array([v[1] for v in train_occurrences.items()])\n",
    "            val_occ_all = np.array([v[1] for v in validation_occurrences.items()])\n",
    "            labels_not_in_train = len(train_occ_all[train_occ_all==0])\n",
    "            train_occ_all = train_occ_all[val_occ_all !=0]\n",
    "            val_occ_all = val_occ_all[val_occ_all !=0]\n",
    "            label_split = np.mean(val_occ_all/(val_occ_all + train_occ_all))\n",
    "            print(f'''Train-Samples: {n_train:.0f} Val-Samples: {n_validation:.0f} set-split: {train_val_split} current-split: {n_validation/(n_train+n_validation):.4f} label-split: {label_split:.4f} labels not in train: {labels_not_in_train} samples: {i}             ''', end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "863df706-3ad0-48b3-a119-9e593f772928",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of F-Terms with occurrence 1 in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "with open(f'{dump_dir}/label_embedding_no_single.pk', 'rb') as f:\n",
    "        label_embedding = pk.load(f)\n",
    "occ = np.array(label_embedding.occurrence)\n",
    "occ[occ>1] = 0\n",
    "print(f'Number of F-Terms with occurrence 1 in dataset: {sum(occ)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3cfcdef-2eb7-446b-a08e-d3010911699c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7478671"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "x = os.listdir('data/dataset_samples/train')\n",
    "x_2 = os.listdir('data/dataset_samples/validation')\n",
    "len(x + x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79bd17-194b-473c-9bc6-f04f6dc7b7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
