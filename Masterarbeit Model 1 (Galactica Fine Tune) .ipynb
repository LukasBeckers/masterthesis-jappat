{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06f7d17-99b2-42f3-a893-89bfb407dfc6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this notebook the galactica model 'mini' is altred for F-Term prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c3215",
   "metadata": {},
   "source": [
    "# Notitzen\n",
    "\n",
    "- man kann dem Model direkt embeddede tokens übergeben. Als key-word argument im foreward pass.\n",
    "- - somit vielleich embedding der label möglich\n",
    "    \n",
    "- es gibt eine classe von meta in die man (vermutlich) Galactica reinladen kann, die speziell für sequenz klassifizierung gedacht ist.\n",
    "\n",
    "- Die unter Klasse von OPTFor... ist PreTrainedmodel hier kann man die input embeddings definieren.\n",
    "\n",
    "- Man kann model in 8 bit laden key-word: load_in_8bit\n",
    "\n",
    "- Man kann mit tokenizer.add_tokens([token1,..]) neue tokens zum vocab des tokenizers hinzufügen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272e44f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781922b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from Masterarbeit_utils import model_utils, dataset_utils\n",
    "import psutil\n",
    "import torch\n",
    "import inspect\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, OPTForSequenceClassification\n",
    "\n",
    "default_dtype = torch.float16\n",
    "# If you change 'default_device' to 'cpu', make sure to set num_gpus to zero in the model configuration\n",
    "#default_device = 'cuda:0'\n",
    "default_device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac715f",
   "metadata": {},
   "source": [
    "# Downloading the Naked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f91f2f6-11d8-4aaf-8166-9efbec23fa16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█████| 787/787 [00:00<00:00, 7.88MB/s]\n",
      "Downloading model.safetensors: 100%|█████████| 250M/250M [00:22<00:00, 11.2MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████| 137/137 [00:00<00:00, 1.92MB/s]\n"
     ]
    }
   ],
   "source": [
    "# A dict to map the correct model urls\n",
    "HF_MAPPING = {\n",
    "    \"mini\": (\"facebook/galactica-125m\", torch.float32),\n",
    "    \"base\": (\"facebook/galactica-1.3b\", torch.float32),\n",
    "    \"standard\": (\"facebook/galactica-6.7b\", torch.float32),\n",
    "    \"large\": (\"facebook/galactica-30b\", torch.float32),\n",
    "    \"huge\": (\"facebook/galactica-120b\", torch.float16)\n",
    "}\n",
    "\n",
    "# Configuration of the model\n",
    "model_name = 'mini'\n",
    "dtype = default_dtype\n",
    "tensor_parallel = False\n",
    "device_map = None\n",
    "# Set to zero if you use the cpu as default device\n",
    "num_gpus = 1\n",
    "if default_device == 'cpu':\n",
    "    num_gpus = 0\n",
    "    default_dtype = torch.float32\n",
    "    dtype = default_dtype\n",
    "\n",
    "# All new torch objects will have this dtype\n",
    "torch.set_default_dtype(default_dtype)\n",
    "# Analyzing the system (code by huggingface)\n",
    "max_memory = {}\n",
    "if num_gpus > 0 and not tensor_parallel:\n",
    "    # based on https://github.com/huggingface/accelerate/blob/5315290b55ea9babd95a281a27c51d87b89d7c85/src/accelerate/utils/modeling.py#L274\n",
    "    for i in range(num_gpus):\n",
    "         _ = torch.tensor([0], device=i)\n",
    "    for i in range(num_gpus):\n",
    "        max_memory[i] = torch.cuda.mem_get_info(i)[0]\n",
    "    device_map = \"auto\"\n",
    "max_memory[\"cpu\"] = psutil.virtual_memory().available\n",
    "\n",
    "# Loading the model form web / from cache\n",
    "model = OPTForCausalLM.from_pretrained(HF_MAPPING[model_name][0], torch_dtype=dtype, low_cpu_mem_usage=True, device_map=device_map, max_memory=max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355f38b",
   "metadata": {},
   "source": [
    "# Loading the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32739e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|█| 2.14M/2.14M [00:00<00:00, 5.71MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_MAPPING[model_name][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ab49f",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2af3edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Tokenizer: tensor([[34848, 16810, 14782,    36]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Good morning Mr. H. S. (1920), \"The Greatest Man in the World\", The New York'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Text\n",
    "text = 'Good morning Mr.'\n",
    "# Convert text to tokens\n",
    "tokens  = tokenizer(text, return_tensors='pt').input_ids\n",
    "print(f'Output of Tokenizer: {tokens}')\n",
    "# Model generating the predicted output tokens\n",
    "out = model.generate(tokens.to(default_device), max_length=30)\n",
    "# Decoding the tokens\n",
    "\n",
    "out = tokenizer.decode(out[0])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6128e7",
   "metadata": {},
   "source": [
    "# Extract Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e673cdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model uses a nn.Embeddings instance as token embedding. \n",
      "\n",
      "It has a dict-size of <50000>.\n",
      "It has a embedding dimension of <768>\n",
      "and a padding index of <1>.\n",
      "\n",
      "The weights have a dtype of <torch.float32>\n",
      "and are on device <cpu>\n"
     ]
    }
   ],
   "source": [
    "token_embedding = model.get_input_embeddings()\n",
    "\n",
    "print(f'''The model uses a nn.Embeddings instance as token embedding. \n",
    "\n",
    "It has a dict-size of <{token_embedding.num_embeddings}>.\n",
    "It has a embedding dimension of <{token_embedding.embedding_dim}>\n",
    "and a padding index of <{token_embedding.padding_idx}>.\n",
    "\n",
    "The weights have a dtype of <{token_embedding.weight.dtype}>\n",
    "and are on device <{token_embedding.weight.device}>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463fe5b",
   "metadata": {},
   "source": [
    "# Creating a Custom Token and F-Term Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de907a",
   "metadata": {},
   "source": [
    "### Using the weights from the original embeddig and replacing the weigths of a larger embedding instance partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d405a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(original_embedding: torch.nn.Embedding, n_f_terms: int, dtype: torch.dtype=default_dtype) -> torch.nn.Embedding:\n",
    "    \"\"\"\n",
    "    This function takes the original_embedding instance of an OPT model, \n",
    "        (nn.Embedding instance).\n",
    "    and the number of f-terms it should embedd (n_f_terms) and creates a new embedding which has \n",
    "    new weights for all f_terms stacked ontop of the old weigths used for the original tokens\n",
    "    \n",
    "    returns: torch.nn.Embedding\n",
    "    \"\"\"\n",
    "    # calculating parameters for the new embedding instance\n",
    "    embedding_dim = original_embedding.embedding_dim\n",
    "    num_embeddings = original_embedding.num_embeddings + n_f_terms\n",
    "    padding_idx = original_embedding.padding_idx\n",
    "    \n",
    "    # creating new embedding (compleately untrained)\n",
    "    embedding = torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx)\n",
    "    # extracting the weigths of the original pretrained embeddign\n",
    "    old_weights = original_embedding.weight\n",
    "    new_weights = embedding.weight\n",
    "    \n",
    "    # replacing a chunk of the new parameters with te old parameters to retain the ability to encode natrual language tokens\n",
    "    embedding.weight = torch.nn.Parameter(\n",
    "                        torch.cat([old_weights.clone().to(default_device),\n",
    "                                   new_weights[original_embedding.num_embeddings:].clone().to(default_device)],\n",
    "                                  0))\n",
    "    return embedding\n",
    "    \n",
    "new_embeddings = create_embedding(token_embedding, 360000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d00381",
   "metadata": {},
   "source": [
    "### Adding the new embedding to the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780d6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Good morning Mr.\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Output:Good morning Mr. H. S. (1920), \"The Greatest Man in the World\", The New York \n",
      "\n",
      "\n",
      "======================================================================================================\n",
      "Prompt \"translated\" by model: evaluationSCI\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Output: NP queen Amer [START_REF] A new method for the first-order statistical model for the estimation of the critical value of the critical value of the critical value of the critical value of the critical value of the critical value of the critical value of the critical value of the critical\n"
     ]
    }
   ],
   "source": [
    "# Replacing the old embedding instance with the new embedding instance in the model instance\n",
    "model.set_input_embeddings(new_embeddings)\n",
    "\n",
    "# testing using known (natural language) tokens\n",
    "out = model.generate(tokens, max_length=30)\n",
    "out = tokenizer.decode(out[0])\n",
    "print(f'Prompt: {text}\\n\\n----------------------------------------------------\\n\\nOutput:{out}', '\\n\\n')\n",
    "print('======================================================================================================')\n",
    "\n",
    "# Testing using new unknown tokens (including tokens reserved for f-terms) \n",
    "random_tokens = torch.randint(410000, [1, 50])\n",
    "random_out = model.generate(random_tokens, max_length=100)\n",
    "random_input_out = tokenizer.decode(random_out[0][:30])\n",
    "random_generated_out = tokenizer.decode(random_out[0][30:])\n",
    "print(f'Prompt \"translated\" by model: {random_input_out}\\n\\n----------------------------------------------------\\n\\nOutput: {random_generated_out}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e202dd9",
   "metadata": {},
   "source": [
    "# Creating a Custom Classification-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5659911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The old classification head is an instance of <class 'torch.nn.modules.linear.Linear'>,\n",
      "it has <768> input features \n",
      "and <50000> output features.\n",
      "The weights have a dtype of <torch.float32>\n",
      "and are on device <cpu>\n"
     ]
    }
   ],
   "source": [
    "# extracting the old classification head from the model\n",
    "old_classification_head = model.get_output_embeddings()\n",
    "# analyzing the old classification head\n",
    "print(f'''\n",
    "The old classification head is an instance of {type(old_classification_head)},\n",
    "it has <{old_classification_head.in_features}> input features \n",
    "and <{old_classification_head.out_features}> output features.\n",
    "The weights have a dtype of <{old_classification_head.weight.dtype}>\n",
    "and are on device <{old_classification_head.weight.device}>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "206e2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_classification_head(n_f_terms: int, model_dim:int) -> torch.nn.Linear:\n",
    "    \"\"\"\n",
    "    Creates a new classification head for the model\n",
    "    \n",
    "    This classification head will be a new linear layer with 'model_dim' input features and 'n_f_terms' output features\n",
    "    \"\"\"\n",
    "    return torch.nn.Linear(in_features=model_dim, out_features=n_f_terms, bias=False).to(default_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbde525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The new classification head has <768> input features\n",
      "and <360000> output features.\n",
      "\n",
      "Its weights are in dtype <torch.float32>\n",
      "and on device <cpu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the new classification head\n",
    "new_classification_head = create_new_classification_head(360000, 768)\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "The new classification head has <{new_classification_head.in_features}> input features\n",
    "and <{new_classification_head.out_features}> output features.\n",
    "\n",
    "Its weights are in dtype <{new_classification_head.weight.dtype}>\n",
    "and on device <{new_classification_head.weight.device}>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3361c",
   "metadata": {},
   "source": [
    "### Adding the new classification head to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d55a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classification_head(_model: OPTForCausalLM, classification_head: torch.nn.Linear) -> OPTForCausalLM:\n",
    "    \"\"\"\n",
    "    This function implements the new classification head to the pretrained model.\n",
    "    \n",
    "    _model: Instanciated OPTForCausalLM model\n",
    "    classificaiton_head: New classification head for the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # changing the configuration of the model\n",
    "    vocab_size = classification_head.out_features\n",
    "    _model.config.vocab_size = vocab_size\n",
    "    _model.model.decoder.vocab_size = vocab_size\n",
    "    _model.num_labels = vocab_size\n",
    "    _model.config.num_labels = vocab_size\n",
    "    \n",
    "    # adding the classification head to the model\n",
    "    _model.set_output_embeddings(classification_head)\n",
    "    return _model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab6dba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input tokens shape: <torch.Size([1, 50])>\n",
      "Output predictions shape: <torch.Size([1, 50, 360000])>\n"
     ]
    }
   ],
   "source": [
    "# Replacing the old with the new classification head\n",
    "model = add_classification_head(model, new_classification_head)\n",
    "\n",
    "# Testing the model\n",
    "random_tokens = random_tokens.clone()\n",
    "x = model.get_input_embeddings()\n",
    "\n",
    "# cloning the tokens (could lead to cuda error otherwise)\n",
    "ip = random_tokens.clone()\n",
    "\n",
    "# generating the output\n",
    "random_out = model(ip, return_dict=1)\n",
    "\n",
    "print(f'''\n",
    "Input tokens shape: <{random_tokens.shape}>\n",
    "Output predictions shape: <{random_out['logits'].shape}>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8127f-5e22-4597-9956-ae9f5a0e7924",
   "metadata": {},
   "source": [
    "# End of file rest are just small experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfdba1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 360000]) torch.Size([1, 50])\n",
      "torch.Size([1, 49, 360000])\n",
      "torch.Size([1, 49, 360000]) torch.Size([1, 49])\n",
      "tensor([[ 2.8501e-01,  4.1437e-01,  1.3241e+00,  ..., -8.7459e-01,\n",
      "          7.9609e-01,  2.0880e-01],\n",
      "        [ 1.6861e-01,  1.9658e-01,  8.9533e-01,  ..., -6.9590e-01,\n",
      "          9.6748e-01,  3.0396e-01],\n",
      "        [-7.0320e-03,  1.6316e-01,  9.8075e-01,  ..., -8.6247e-01,\n",
      "          1.0221e+00,  4.0162e-01],\n",
      "        ...,\n",
      "        [ 2.8422e-01,  4.9986e-04,  5.8435e-01,  ..., -1.0092e+00,\n",
      "          9.7533e-01,  5.2518e-02],\n",
      "        [-1.7185e-02,  4.8661e-01,  1.1776e+00,  ..., -4.2681e-01,\n",
      "          4.5091e-01,  2.6646e-01],\n",
      "        [-2.9576e-01,  7.0435e-01,  8.4945e-01,  ..., -3.8210e-01,\n",
      "          4.0003e-01,  4.8613e-01]], grad_fn=<ViewBackward0>) tensor([ 32245, 308784, 205593, 320644, 190773, 197299, 349046,  89440, 189537,\n",
      "        303807, 119482, 238300, 308918, 336238,  72096, 171501, 269457, 114848,\n",
      "         43504, 310244, 122330, 112113, 281888, 299725, 166340,  34230,  96219,\n",
      "        255109, 273846, 124720, 269717,  94234, 326456, 199437,  93824,  97545,\n",
      "         11167,  24415, 284066, 108555, 299729,  67890,  77118, 150469, 323556,\n",
      "        223321,   6497, 195668,  37904])\n",
      "torch.Size([49, 360000]) torch.Size([49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(12.9740, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreating the loss function from the transformers module\n",
    "# Shift so that tokens < n predict n\n",
    "logits = random_out['logits']\n",
    "labels = torch.randint(360000, [1, 50])\n",
    "\n",
    "print(logits.shape, labels.shape)\n",
    "\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "print(shift_logits.shape)\n",
    "\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "print(shift_logits.shape, shift_labels.shape)\n",
    "\n",
    "# Flatten the tokens\n",
    "\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "print(shift_logits.view(-1, 360000), shift_labels.view(-1))\n",
    "print(shift_logits.view(-1, 360000).shape, shift_labels.view(-1).shape)\n",
    "loss = loss_fct(shift_logits.view(-1, 360000).type(torch.float32), shift_labels.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e46d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=30)\n",
    "\n",
    "for p in model.model.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddf32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.model.parameters():\n",
    "    #print(torch.mean(p).item())\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ccfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db906c41",
   "metadata": {},
   "source": [
    "# Probleme und Fragen\n",
    "\n",
    "- wird der Loss im Model berechnet, oder lasse ich mir die hidden states ausgeben und berechne den loss separat?\n",
    "    - vermutlich besser separat\n",
    "- definiere ich das Token embedding einfach neu (im Model oder gebe ich bereits embeddete Tokens in das Model?\n",
    "\n",
    "- Wie blockiere ich den Loss für meine Input sequenz?\n",
    "\n",
    "- Welche Kombination an start_sentence, stop_sentence, padding_tokens soll ich verwenden?\n",
    "    - Links oder rechts Padding?\n",
    "         - Vermutlich links\n",
    "         \n",
    "\n",
    "- Speichere ich das gesammte Datenset in Token-Form?\n",
    "    - gepadded oder nicht?\n",
    "        - padding während der batch-Erstellung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b611bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
